{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10015986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9c5a44",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Transformers4Rec demo: \n",
    "<h2><center> Train a Session-based recommender with Yoochoose e-commerce dataset using  XLNET </center></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220be71e",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Define Data Input and Output Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa3c2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import cudf\n",
    "import cupy\n",
    "import nvtabular as nvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e22c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/workspace/data/\"\n",
    "FILENAME_PATTERN = 'yoochoose-clicks.dat'\n",
    "DATA_PATH = os.path.join(DATA_FOLDER, FILENAME_PATTERN)\n",
    "\n",
    "OUTPUT_FOLDER = \"./yoochoose_transformed\"\n",
    "OVERWRITE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525abc40",
   "metadata": {},
   "source": [
    "### 1.1. Load and clean raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb92521",
   "metadata": {},
   "source": [
    "In this notebook we are using the `YOOCHOOSE dataset` which contains a collection of sessions from a retailer, where each session is encapsulating the click events that the user performed in that session.\n",
    "\n",
    "The dataset is available on [Kaggle](https://www.kaggle.com/chadgostopp/recsys-challenge-2015). Note that we are using the file `yoochoose-clicks.dat`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c53c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = cudf.read_csv(DATA_PATH, sep=',', \n",
    "                                names=['session_id','timestamp', 'item_id', 'category'], \n",
    "                                dtype=['int', 'datetime64[s]', 'int', 'int'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b15c011",
   "metadata": {},
   "source": [
    "    1- Remove repeated interactions within the same session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d67e9b06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count with in-session repeated interactions: 33003944\n",
      "Count after removed in-session repeated interactions: 28971543\n"
     ]
    }
   ],
   "source": [
    "print(\"Count with in-session repeated interactions: {}\".format(len(interactions_df)))\n",
    "# Sorts the dataframe by session and timestamp, to remove consecutive repetitions\n",
    "interactions_df.timestamp = interactions_df.timestamp.astype(int)\n",
    "interactions_df = interactions_df.sort_values(['session_id', 'timestamp'])\n",
    "past_ids = interactions_df['item_id'].shift(1).fillna()\n",
    "session_past_ids = interactions_df['session_id'].shift(1).fillna()\n",
    "# Keeping only no consectutive repeated in session interactions\n",
    "interactions_df = interactions_df[~((interactions_df['session_id'] == session_past_ids) & (interactions_df['item_id'] == past_ids))]\n",
    "print(\"Count after removed in-session repeated interactions: {}\".format(len(interactions_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1eb2cd",
   "metadata": {},
   "source": [
    "    2- Create date when item was seen for the first-time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dfa9d26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category</th>\n",
       "      <th>itemid_ts_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549</td>\n",
       "      <td>1396774534</td>\n",
       "      <td>214714927</td>\n",
       "      <td>0</td>\n",
       "      <td>1396334996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549</td>\n",
       "      <td>1396774556</td>\n",
       "      <td>214517450</td>\n",
       "      <td>0</td>\n",
       "      <td>1396329825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>549</td>\n",
       "      <td>1396774617</td>\n",
       "      <td>214714929</td>\n",
       "      <td>0</td>\n",
       "      <td>1396341783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>549</td>\n",
       "      <td>1396774647</td>\n",
       "      <td>214518555</td>\n",
       "      <td>0</td>\n",
       "      <td>1396327272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>549</td>\n",
       "      <td>1396774664</td>\n",
       "      <td>214639297</td>\n",
       "      <td>0</td>\n",
       "      <td>1396353119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id   timestamp    item_id  category  itemid_ts_first\n",
       "0         549  1396774534  214714927         0       1396334996\n",
       "1         549  1396774556  214517450         0       1396329825\n",
       "2         549  1396774617  214714929         0       1396341783\n",
       "3         549  1396774647  214518555         0       1396327272\n",
       "4         549  1396774664  214639297         0       1396353119"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_first_ts_df = interactions_df.groupby('item_id').agg({'timestamp': 'min'}).reset_index().rename(columns={'timestamp': 'itemid_ts_first'})\n",
    "interactions_merged_df = interactions_df.merge(items_first_ts_df, on=['item_id'], how='left')\n",
    "interactions_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a86951",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# free gpu memory\n",
    "import gc\n",
    "del interactions_df, session_past_ids, items_first_ts_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be0ec7",
   "metadata": {},
   "source": [
    "## 2. Define NVTabular Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84e959",
   "metadata": {},
   "source": [
    "NVTabular is a feature engineering and preprocessing library for tabular data designed to quickly and easily manipulate terabyte scale datasets used to train deep learning based recommender systems. It provides a high level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS cuDF library.\n",
    "\n",
    "NVTabular supports different feature engineering transformations required by Deep Learning models such as Categroical encoding and numerical feature normalization. It also supports session-based feature engineering such as the creation of lists via grouping by operations and list slicing. \n",
    "\n",
    "More information about the supported features can be found <a href=https://nvidia.github.io/NVTabular/main/index.html> here. </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c0bcd",
   "metadata": {},
   "source": [
    "### 2.1 Feature engineering: Create and Transform items features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaeb4c6",
   "metadata": {},
   "source": [
    "In this cell, we are defining three transformations ops: \n",
    "\n",
    "    1- Encoding categorical variables using Categorify() op.\n",
    "    2- Deriving temporal features from timestamp and computing their cyclical representation using a custom lambda function. \n",
    "    3- Computing the item recency in days using a custom Op. We note that item recency is defined as the difference between the first occurence of the item in dataset and the actual date of item interaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2efe23db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"1137pt\" height=\"404pt\"\n",
       " viewBox=\"0.00 0.00 1136.71 404.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 400)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-400 1132.71,-400 1132.71,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"300.27\" cy=\"-90\" rx=\"252.66\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"300.27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">(lambda col: get_cycled_feature_value_sin(col+1, 7))</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"300.27\" cy=\"-18\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"300.27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Rename</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;10 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>0&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300.27,-71.7C300.27,-63.98 300.27,-54.71 300.27,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"303.77,-46.1 300.27,-36.1 296.77,-46.1 303.77,-46.1\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"300.27\" cy=\"-162\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"300.27\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">Rename</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>5&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300.27,-143.7C300.27,-135.98 300.27,-126.71 300.27,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"303.77,-118.1 300.27,-108.1 296.77,-118.1 303.77,-118.1\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"300.27\" cy=\"-234\" rx=\"230.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"300.27\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">nvt.ops.LambdaOp(lambda col: col.dt.weekday)</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300.27,-215.7C300.27,-207.98 300.27,-198.71 300.27,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"303.77,-190.1 300.27,-180.1 296.77,-190.1 303.77,-190.1\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>9</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"300.27\" cy=\"-306\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"300.27\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">Rename</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>9&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300.27,-287.7C300.27,-279.98 300.27,-270.71 300.27,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"303.77,-262.1 300.27,-252.1 296.77,-262.1 303.77,-262.1\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"689.27\" cy=\"-234\" rx=\"58.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"689.27\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">Normalize</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>7</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"689.27\" cy=\"-162\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"689.27\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">Rename</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;7 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M689.27,-215.7C689.27,-207.98 689.27,-198.71 689.27,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"692.77,-190.1 689.27,-180.1 685.77,-190.1 692.77,-190.1\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"689.27\" cy=\"-306\" rx=\"40.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"689.27\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">LogOp</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>6&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M689.27,-287.7C689.27,-279.98 689.27,-270.71 689.27,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"692.77,-262.1 689.27,-252.1 685.77,-262.1 692.77,-262.1\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"689.27\" cy=\"-378\" rx=\"70.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"689.27\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\">ItemRecency</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;6 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>3&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M689.27,-359.7C689.27,-351.98 689.27,-342.71 689.27,-334.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"692.77,-334.1 689.27,-324.1 685.77,-334.1 692.77,-334.1\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"300.27\" cy=\"-378\" rx=\"300.05\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"300.27\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\">nvt.ops.LambdaOp(lambda col: cudf.to_datetime(col, unit=&#39;s&#39;))</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;9 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>4&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300.27,-359.7C300.27,-351.98 300.27,-342.71 300.27,-334.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"303.77,-334.1 300.27,-324.1 296.77,-334.1 303.77,-334.1\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"938.27\" cy=\"-378\" rx=\"59.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"938.27\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\">Categorify</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"938.27\" cy=\"-306\" rx=\"190.37\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"938.27\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">nvt.ops.LambdaOp(lambda col: col +1)</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;12 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>8&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M938.27,-359.7C938.27,-351.98 938.27,-342.71 938.27,-334.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"941.77,-334.1 938.27,-324.1 934.77,-334.1 941.77,-334.1\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"938.27\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"938.27\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"938.27\" cy=\"-162\" rx=\"77.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"938.27\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">output cols=[]</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;13 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>11&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M938.27,-215.7C938.27,-207.98 938.27,-198.71 938.27,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"941.77,-190.1 938.27,-180.1 934.77,-190.1 941.77,-190.1\"/>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;11 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>12&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M938.27,-287.7C938.27,-279.98 938.27,-270.71 938.27,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"941.77,-262.1 938.27,-252.1 934.77,-262.1 941.77,-262.1\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fd7dffc3a60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categorify features \n",
    "cat_feats = nvt.ColumnSelector(['session_id', 'category', 'item_id']) >> nvt.ops.Categorify() >> nvt.ops.LambdaOp(lambda col: col +1)\n",
    "\n",
    "# create time features\n",
    "sessionTs = nvt.ColumnSelector(['timestamp'])\n",
    "sessionTime = (\n",
    "    sessionTs >> \n",
    "    nvt.ops.LambdaOp(lambda col: cudf.to_datetime(col, unit='s')) >> \n",
    "    nvt.ops.Rename(name = 'event_time_dt')\n",
    ")\n",
    "sessionTime_weekday = (\n",
    "    sessionTime >> \n",
    "    nvt.ops.LambdaOp(lambda col: col.dt.weekday) >> \n",
    "    nvt.ops.Rename(name ='et_dayofweek')\n",
    ")\n",
    "\n",
    "# Derive cyclical features : Define custom lambda function \n",
    "def get_cycled_feature_value_sin(col, max_value):\n",
    "    value_scaled = (col + 0.000001) / max_value\n",
    "    value_sin = np.sin(2*np.pi*value_scaled)\n",
    "    return value_sin\n",
    "\n",
    "weekday_sin = sessionTime_weekday >> (lambda col: get_cycled_feature_value_sin(col+1, 7)) >> nvt.ops.Rename(name = 'et_dayofweek_sin')\n",
    "\n",
    "# Compute Item recency : Define custom operator \n",
    "class ItemRecency(nvt.ops.Operator):\n",
    "    def transform(self, columns, gdf):\n",
    "        for column in columns.names:\n",
    "            col = gdf[column]\n",
    "            item_first_timestamp = gdf['itemid_ts_first']\n",
    "            delta_days = (col - item_first_timestamp) / (60*60*24)\n",
    "            gdf[column + \"_age_days\"] = delta_days * (delta_days >=0)\n",
    "        return gdf\n",
    "           \n",
    "    def output_column_names(self, columns):\n",
    "        return nvt.ColumnSelector([column + \"_age_days\" for column in columns.names])\n",
    "\n",
    "    def dependencies(self):\n",
    "        return [\"itemid_ts_first\"]\n",
    "    \n",
    "\n",
    "recency_features = sessionTs >> ItemRecency() \n",
    "recency_features_norm = recency_features >> nvt.ops.LogOp() >> nvt.ops.Normalize() >> nvt.ops.Rename(name='product_recency_days_log_norm')\n",
    "\n",
    "\n",
    "time_features = (\n",
    "    sessionTime +\n",
    "    sessionTime_weekday +\n",
    "    weekday_sin + \n",
    "    recency_features_norm\n",
    ")\n",
    "\n",
    "features = nvt.ColumnSelector(['timestamp', 'session_id']) + cat_feats + time_features \n",
    "features.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077e51d",
   "metadata": {},
   "source": [
    "### 2.2 Generate session-based features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea8e930",
   "metadata": {},
   "source": [
    "Once the item features are generated, the objective of this cell is grouping them together at the session level, sorting the interactions by time. We additionnaly truncate all sessions to first 20 interactions and filtered out sessions with less than 2 interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7a632b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Groupby Operator\n",
    "groupby_features = features >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"session_id\"], \n",
    "    sort_cols=[\"timestamp\"],\n",
    "    aggs={\n",
    "        'item_id': [\"list\", \"count\"],\n",
    "        'category': [\"list\"],  \n",
    "        'timestamp': [\"first\"],\n",
    "        'event_time_dt': [\"first\"],\n",
    "        'et_dayofweek_sin': [\"list\"],\n",
    "        'product_recency_days_log_norm': [\"list\"]\n",
    "        },\n",
    "    name_sep=\"-\")\n",
    "\n",
    "\n",
    "# Truncate sequence features to first interacted 20 items \n",
    "SESSIONS_MAX_LENGTH = 20 \n",
    "\n",
    "groupby_features_list = groupby_features['item_id-list', 'category-list', 'et_dayofweek_sin-list', 'product_recency_days_log_norm-list']\n",
    "groupby_features_trim = groupby_features_list >> nvt.ops.ListSlice(0, SESSIONS_MAX_LENGTH) >> nvt.ops.Rename(postfix = '_seq')\n",
    "\n",
    "# calculate session day index based on 'event_time_dt-first' column\n",
    "day_index = ((groupby_features['event_time_dt-first'])  >> \n",
    "    nvt.ops.LambdaOp(lambda col: (col - col.min()).dt.days +1) >> \n",
    "    nvt.ops.Rename(f = lambda col: \"day_index\")\n",
    ")\n",
    "\n",
    "# Select features for training \n",
    "selected_features = groupby_features['session_id', 'item_id-count'] + groupby_features_trim + day_index\n",
    "\n",
    "# Filter out sessions with less than 2 interactions \n",
    "MINIMUM_SESSION_LENGTH = 2\n",
    "filtered_sessions = selected_features >> nvt.ops.Filter(f=lambda df: df[\"item_id-count\"] >= MINIMUM_SESSION_LENGTH) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949fe023",
   "metadata": {},
   "source": [
    "### 2.3 Execute NVTabular workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ac133",
   "metadata": {},
   "source": [
    "Once we have defined the general workflow `filtered_sessions`, we provide our cudf dataset to nvt.Dataset class which is optimized to split data into chunks that can fit in device memory and to handle the calculation of complex global statistics. Then, we execute the pipeline that fits and transforms data to get the desired output features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f2fb50f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numba/cuda/compiler.py:865: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (160) will likely result in GPU under utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "dataset = nvt.Dataset(interactions_merged_df)\n",
    "workflow = nvt.Workflow(filtered_sessions)\n",
    "workflow.fit(dataset)\n",
    "sessions_gdf = workflow.transform(dataset).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c410c88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>item_id-count</th>\n",
       "      <th>item_id-list_seq</th>\n",
       "      <th>category-list_seq</th>\n",
       "      <th>et_dayofweek_sin-list_seq</th>\n",
       "      <th>product_recency_days_log_norm-list_seq</th>\n",
       "      <th>day_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>[2223, 2125, 1800, 123, 3030, 1861, 1076, 1285...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1.1285199e-06, 1.1285199e-06, 1.1285199e-06, ...</td>\n",
       "      <td>[-1.1126341, -0.9665389, -0.1350116, -0.127809...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>[26562, 35137, 19260, 46449, 29027, 39096, 272...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0.43388295, 0.43388295, 0.43388295, 0.4338829...</td>\n",
       "      <td>[0.40848607, 0.39331725, 0.5418466, -3.0278225...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>[23212, 30448, 16468, 2052, 22490, 31097, 6243...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0.9749277, 0.9749277, 0.9749277, 0.9749277, 0...</td>\n",
       "      <td>[0.6801631, 0.7174695, 0.7185285, 0.7204116, 0...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>[230, 451, 732, 1268, 2014, 567, 497, 439, 338...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, ...</td>\n",
       "      <td>[0.43388295, 0.43388295, 0.43388295, 0.4338829...</td>\n",
       "      <td>[1.3680888, -0.6530481, -0.69314253, -0.590593...</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>[23, 70, 160, 70, 90, 742, 851, 359, 734, 878,...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0.43388295, 0.43388295, 0.43388295, 0.4338829...</td>\n",
       "      <td>[1.3714824, 1.3715883, 1.3715737, 1.3715955, 1...</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id  item_id-count  \\\n",
       "0           2            200   \n",
       "1           3            200   \n",
       "2           4            200   \n",
       "3           5            200   \n",
       "4           6            200   \n",
       "\n",
       "                                    item_id-list_seq  \\\n",
       "0  [2223, 2125, 1800, 123, 3030, 1861, 1076, 1285...   \n",
       "1  [26562, 35137, 19260, 46449, 29027, 39096, 272...   \n",
       "2  [23212, 30448, 16468, 2052, 22490, 31097, 6243...   \n",
       "3  [230, 451, 732, 1268, 2014, 567, 497, 439, 338...   \n",
       "4  [23, 70, 160, 70, 90, 742, 851, 359, 734, 878,...   \n",
       "\n",
       "                                   category-list_seq  \\\n",
       "0  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "1  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "2  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, ...   \n",
       "4  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                           et_dayofweek_sin-list_seq  \\\n",
       "0  [1.1285199e-06, 1.1285199e-06, 1.1285199e-06, ...   \n",
       "1  [0.43388295, 0.43388295, 0.43388295, 0.4338829...   \n",
       "2  [0.9749277, 0.9749277, 0.9749277, 0.9749277, 0...   \n",
       "3  [0.43388295, 0.43388295, 0.43388295, 0.4338829...   \n",
       "4  [0.43388295, 0.43388295, 0.43388295, 0.4338829...   \n",
       "\n",
       "              product_recency_days_log_norm-list_seq  day_index  \n",
       "0  [-1.1126341, -0.9665389, -0.1350116, -0.127809...         27  \n",
       "1  [0.40848607, 0.39331725, 0.5418466, -3.0278225...         58  \n",
       "2  [0.6801631, 0.7174695, 0.7185285, 0.7204116, 0...         71  \n",
       "3  [1.3680888, -0.6530481, -0.69314253, -0.590593...        149  \n",
       "4  [1.3714824, 1.3715883, 1.3715737, 1.3715955, 1...        149  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5698b80c",
   "metadata": {},
   "source": [
    "**Save the workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c34c7e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.save('workflow_etl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1d2108",
   "metadata": {},
   "source": [
    "### 2.4 Export pre-processed data by day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2fc9c7",
   "metadata": {},
   "source": [
    "  - For daily training we need to partition data by day, create the Train/Validation/Test splits and save them to disk using day-index column. \n",
    "  \n",
    "  \n",
    "  - It is worthwhile to note that the dataset have a single categorical feature (category), but it is inconsistent over time in the dataset. All interactions before day 84 (2014-06-23) have the same value for that feature, whereas many other categories are introduced afterwards. \n",
    "  \n",
    "  \n",
    "  - For the purpose of the demo, we only save the last five days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1054510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_gdf = sessions_gdf[sessions_gdf.day_index>=178]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e356554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating time-based splits: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers4rec.utils.gpu_preprocessing import save_time_based_splits\n",
    "save_time_based_splits(data=nvt.Dataset(sessions_gdf),\n",
    "                       output_dir= \"./preproc_sessions_by_day\",\n",
    "                       partition_col='day_index',\n",
    "                       timestamp_col='session_id', \n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38e65690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preproc_sessions_by_day_ts/\n",
      "    180/\n",
      "        test.parquet\n",
      "        valid.parquet\n",
      "        train.parquet\n",
      "    181/\n",
      "        test.parquet\n",
      "        valid.parquet\n",
      "        train.parquet\n",
      "    179/\n",
      "        test.parquet\n",
      "        valid.parquet\n",
      "        train.parquet\n",
      "    182/\n",
      "        test.parquet\n",
      "        valid.parquet\n",
      "        train.parquet\n",
      "    178/\n",
      "        test.parquet\n",
      "        valid.parquet\n",
      "        train.parquet\n"
     ]
    }
   ],
   "source": [
    "from demo_utils import list_files\n",
    "list_files('./preproc_sessions_by_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4cb6886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# free gpu memory\n",
    "del  sessions_gdf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e9dee",
   "metadata": {},
   "source": [
    "## 3. Model definition using Transformers4Rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0b4aa",
   "metadata": {},
   "source": [
    "### 3.1 Get the schema "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50543a",
   "metadata": {},
   "source": [
    "The library uses a schema format to configure the input features and automatically creates the necessary layers. This proto text file contains the description of each input feature by defining : The name, the type, the number of elements of a list column,  the cardinality of a categorical feature and the min and max values of a continuous varable. In addition, the annotation field contains the tags to allow custom annotation such as specifying the `target` column or the `item` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb438b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"item_id-list_seq\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 185\n",
      "  }\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"item_id/list\"\n",
      "    min: 1\n",
      "    max: 52742\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"item_id\"\n",
      "    tag: \"list\"\n",
      "    tag: \"categorical\"\n",
      "    tag: \"item\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from merlin_standard_lib import Schema\n",
    "SCHEMA_PATH = \"schema_demo.pb\"\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "!head -20 $SCHEMA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbfc8c",
   "metadata": {},
   "source": [
    "We can select the subset of features we want to use for training the model by their tags or their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "993dca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema.select_by_tag('item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f06fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema.select_by_name(\n",
    "   ['item_id-list_seq', 'category-list_seq', 'product_recency_days_log_norm-list_seq', 'et_dayofweek_sin-list_seq']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248678e",
   "metadata": {},
   "source": [
    "### 3.2 Define the end-to-end Session-based Transformer-based recommendation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f50afa",
   "metadata": {
    "tags": []
   },
   "source": [
    "For session-based recommendation model definition, the end-to-end model definition requires four steps:\n",
    "\n",
    "1. Instantiate `TabularSequenceFeatures` input-module from schema to prepare the embedding tables of categorical variables and project continuous features, if specified. In addition the module provides different aggregation methods to merge input features and generate the sequence of interactions embeddings. The module also supports language modeling tasks to prepare masked labels for training and evaluation (e.g: 'causal' for causal language modeling) \n",
    "\n",
    "2. Next, we need to define one or multiple prediction tasks. For this demo, we are going to use `NextItemPredictionTask`: during training the next item or randomly selected items are predicted depending on the masking scheme. For inference it is meant to always predict the next item to be interacted with.\n",
    "\n",
    "3. Then we construct a `transformer_config` based on the architectures provided by [Hugging Face Transformers](https://github.com/huggingface/transformers) framework. </a>\n",
    "\n",
    "4. Finally we link the transformer-body to the inputs and the prediction tasks to get the final pytorch `Model` class.\n",
    "    \n",
    "For more details about the features supported by each sub-module, please check the `Core Features` <a href=https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/docs/source/core_features.md> documentation page </a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa746e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projecting inputs of NextItemPredictionTask to'64' As weight tying requires the input dimension '320' to be equal to the item-id embedding dimension '64'\n"
     ]
    }
   ],
   "source": [
    "from transformers4rec import torch as tr\n",
    "\n",
    "max_sequence_length, d_model = 20, 320\n",
    "# Define input module to process tabular input-features and to prepare masked inputs\n",
    "input_module = tr.TabularSequenceFeatures.from_schema(\n",
    "    schema,\n",
    "    max_sequence_length=max_sequence_length,\n",
    "    continuous_projection=64,\n",
    "    aggregation=\"concat\",\n",
    "    d_output=d_model,\n",
    "    masking=\"mlm\",\n",
    ")\n",
    "\n",
    "# Define Next item prediction-task \n",
    "prediction_task = tr.NextItemPredictionTask(hf_format=True, weight_tying=True)\n",
    "\n",
    "# Define the config of the XLNet Transformer architecture\n",
    "transformer_config = tr.XLNetConfig.build(\n",
    "    d_model=d_model, n_head=8, n_layer=2, total_seq_length=max_sequence_length\n",
    ")\n",
    "\n",
    "#Get the end-to-end model \n",
    "model = transformer_config.to_torch_model(input_module, prediction_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d00bae3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (heads): ModuleList(\n",
       "    (0): Head(\n",
       "      (body): SequentialBlock(\n",
       "        (0): TabularSequenceFeatures(\n",
       "          (_aggregation): ConcatFeatures()\n",
       "          (to_merge): ModuleDict(\n",
       "            (continuous_module): SequentialBlock(\n",
       "              (0): ContinuousFeatures(\n",
       "                (filter_features): FilterFeatures()\n",
       "                (_aggregation): ConcatFeatures()\n",
       "              )\n",
       "              (1): SequentialBlock(\n",
       "                (0): DenseBlock(\n",
       "                  (0): Linear(in_features=1, out_features=64, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (2): AsTabular()\n",
       "            )\n",
       "            (categorical_module): SequenceEmbeddingFeatures(\n",
       "              (filter_features): FilterFeatures()\n",
       "              (embedding_tables): ModuleDict(\n",
       "                (item_id-list_seq): Embedding(52743, 64, padding_idx=0)\n",
       "                (category-list_seq): Embedding(338, 64, padding_idx=0)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (projection_module): SequentialBlock(\n",
       "            (0): DenseBlock(\n",
       "              (0): Linear(in_features=192, out_features=320, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (_masking): MaskedLanguageModeling()\n",
       "        )\n",
       "        (1): TansformerBlock(\n",
       "          (transformer): XLNetModel(\n",
       "            (word_embedding): Embedding(1, 320)\n",
       "            (layer): ModuleList(\n",
       "              (0): XLNetLayer(\n",
       "                (rel_attn): XLNetRelativeAttention(\n",
       "                  (layer_norm): LayerNorm((320,), eps=0.03, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (ff): XLNetFeedForward(\n",
       "                  (layer_norm): LayerNorm((320,), eps=0.03, elementwise_affine=True)\n",
       "                  (layer_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                  (layer_2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "              (1): XLNetLayer(\n",
       "                (rel_attn): XLNetRelativeAttention(\n",
       "                  (layer_norm): LayerNorm((320,), eps=0.03, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (ff): XLNetFeedForward(\n",
       "                  (layer_norm): LayerNorm((320,), eps=0.03, elementwise_affine=True)\n",
       "                  (layer_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                  (layer_2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (masking): MaskedLanguageModeling()\n",
       "        )\n",
       "      )\n",
       "      (prediction_tasks): ModuleDict(\n",
       "        (next-item): NextItemPredictionTask(\n",
       "          (sequence_summary): SequenceSummary(\n",
       "            (summary): Identity()\n",
       "            (activation): Identity()\n",
       "            (first_dropout): Identity()\n",
       "            (last_dropout): Identity()\n",
       "          )\n",
       "          (metrics): ModuleList(\n",
       "            (0): NDCGAt()\n",
       "            (1): AvgPrecisionAt()\n",
       "            (2): RecallAt()\n",
       "          )\n",
       "          (loss): NLLLoss()\n",
       "          (embeddings): SequenceEmbeddingFeatures(\n",
       "            (filter_features): FilterFeatures()\n",
       "            (embedding_tables): ModuleDict(\n",
       "              (item_id-list_seq): Embedding(52743, 64, padding_idx=0)\n",
       "              (category-list_seq): Embedding(338, 64, padding_idx=0)\n",
       "            )\n",
       "          )\n",
       "          (item_embedding_table): Embedding(52743, 64, padding_idx=0)\n",
       "          (masking): MaskedLanguageModeling()\n",
       "          (task_block): SequentialBlock(\n",
       "            (0): DenseBlock(\n",
       "              (0): Linear(in_features=320, out_features=64, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (pre): Block(\n",
       "            (module): NextItemPredictionTask(\n",
       "              (item_embedding_table): Embedding(52743, 64, padding_idx=0)\n",
       "              (log_softmax): LogSoftmax(dim=-1)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e9c1d",
   "metadata": {},
   "source": [
    "### 3.3. Daily Fine-Tuning: Training over a time window¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da49f9",
   "metadata": {},
   "source": [
    "Now that the model is defined, we are going to launch training. For that, Transfromers4rec extends HF Transformers Trainer class to adapt the evaluation loop for session-based recommendation task and the calculation of ranking metrics. The original train() method is not modified meaning that we leverage the efficient training implementation from that library, which manages for example half-precision (FP16) training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea6d806",
   "metadata": {},
   "source": [
    "- **Set Training arguments**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2830b21",
   "metadata": {},
   "source": [
    "An additional argument `data_loader_engine` is defined to automatically load the features needed for training using the schema. The default value is `nvtabular` for optimized GPU-based data-loading.  Optionally the PyarrowDataLoader (`pyarrow`) can also be used as a basic option, but it is slower and works only for small datasets, as the full data is loaded to CPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f25796fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = tr.trainer.T4RecTrainingArguments(\n",
    "            output_dir=\"./tmp\",\n",
    "            max_sequence_length=20,\n",
    "            data_loader_engine='nvtabular',\n",
    "            num_train_epochs=10, \n",
    "            dataloader_drop_last=False,\n",
    "            per_device_train_batch_size = 384,\n",
    "            per_device_eval_batch_size = 512,\n",
    "            learning_rate=0.0005,\n",
    "            fp16=True,\n",
    "            report_to = [],\n",
    "            logging_steps=200\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edad135",
   "metadata": {},
   "source": [
    "* **Instantiate the trainer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "423576e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "recsys_trainer = tr.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    schema=schema,\n",
    "    compute_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbaca1",
   "metadata": {},
   "source": [
    "* **Launch daily Training and Evaluation:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e4b59",
   "metadata": {},
   "source": [
    "In this demo, we will use the `fit_and_evaluate` method that allows us to conduct a time-based finetuning by iteratively training and evaluating using a sliding time window: At each iteration, we use training data of a specific time index $t$ to train the model then we evaluate on the validation data of next index $t + 1$. Particularly, the start time is set to 178 and end time to 180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d932f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(trainer, start_time_index, end_time_index, input_dir='./preproc_sessions_by_day'):\n",
    "    \"\"\"\n",
    "    Util function for time-window based fine-tuning using the T4rec Trainer class. \n",
    "    Iteratively train using data of a given index and evaluate on the validation data\n",
    "    of the following index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_time_index: int\n",
    "        the start index for training, it should match the partitions of the data directory \n",
    "    end_time_index: int\n",
    "        the end index for training, it should match the partitions of the  data directory \n",
    "    input_dir: str\n",
    "        The input directory where the parquet files were saved based on partition column\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    aot_metrics: dict\n",
    "        The average over time of ranking metrics.\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    import os\n",
    "    aot_metrics = {}\n",
    "    for time_index in range(start_time_index, end_time_index+1):\n",
    "        # 1. Set data \n",
    "        time_index_train = time_index\n",
    "        time_index_eval = time_index + 1\n",
    "        train_paths = glob.glob(os.path.join(input_dir, f\"{time_index_train}/train.parquet\"))\n",
    "        eval_paths = glob.glob(os.path.join(input_dir, f\"{time_index_eval}/valid.parquet\")) \n",
    "\n",
    "        # 2. Train on day related to time_index \n",
    "        print(\"\\n***** Launch training for day %s: *****\" %time_index)\n",
    "        trainer.train_dataset_or_path = train_paths\n",
    "        trainer.reset_lr_scheduler()\n",
    "        trainer.train()\n",
    "        trainer.state.global_step +=1\n",
    "\n",
    "        # 3. Evaluate on the following day\n",
    "        trainer.eval_dataset_or_path = eval_paths\n",
    "        eval_metrics = trainer.evaluate(metric_key_prefix='eval')\n",
    "        print(\"\\n***** Evaluation results for day %s:*****\\n\" %time_index_eval)\n",
    "        for key in sorted(eval_metrics.keys()):\n",
    "            if 'at_' in key: \n",
    "                print(\" %s = %s\" % (key.replace('at_', '@'), str(eval_metrics[key]))) \n",
    "                if 'AOT_'+key.replace('at_', '@') in aot_metrics: \n",
    "                    aot_metrics['AOT_'+key.replace('_at_', '@')] += [eval_metrics[key]]\n",
    "                else: \n",
    "                    aot_metrics['AOT_'+key.replace('_at_', '@')] = [eval_metrics[key]]\n",
    "\n",
    "        # free GPU for next day training \n",
    "        trainer.wipe_memory()\n",
    "\n",
    "    return aot_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f6cc0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 28800\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 384\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 384\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Launch training for day 178: *****\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 00:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.491200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>5.187700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./tmp/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Evaluation results for day 179:*****\n",
      "\n",
      " eval/next-item/avg_precision_@10 = 0.14822982251644135\n",
      " eval/next-item/avg_precision_@20 = 0.15453767776489258\n",
      " eval/next-item/ndcg_@10 = 0.190110981464386\n",
      " eval/next-item/ndcg_@20 = 0.21371710300445557\n",
      " eval/next-item/recall_@10 = 0.3240847885608673\n",
      " eval/next-item/recall_@20 = 0.4161849617958069\n"
     ]
    }
   ],
   "source": [
    "aot_results = fit_and_evaluate(recsys_trainer, start_time_index=178, end_time_index=178, input_dir='./preproc_sessions_by_day')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f33772",
   "metadata": {
    "tags": []
   },
   "source": [
    "* **Visualize the average over time metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d42e4a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AOT_eval/next-item/avg_precision@10 = 0.13518714904785156\n",
      " AOT_eval/next-item/avg_precision@20 = 0.1435002237558365\n",
      " AOT_eval/next-item/ndcg@10 = 0.17796631157398224\n",
      " AOT_eval/next-item/ndcg@20 = 0.20810143649578094\n",
      " AOT_eval/next-item/recall@10 = 0.3126159608364105\n",
      " AOT_eval/next-item/recall@20 = 0.4304267168045044\n"
     ]
    }
   ],
   "source": [
    "mean_results = {k: np.mean(v) for k,v in aot_results.items()}\n",
    "for key in sorted(mean_results.keys()): \n",
    "    print(\" %s = %s\" % (key, str(mean_results[key]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3fad10",
   "metadata": {},
   "source": [
    "* **Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26cfd6b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./tmp/checkpoint-751\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"
     ]
    }
   ],
   "source": [
    "recsys_trainer._save_model_and_checkpoint(save_model_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b393c88",
   "metadata": {},
   "source": [
    "- **Export the worflow and model in the format required by Triton server:** \n",
    "\n",
    "NVTabular’s `export_pytorch_ensemble` function enables us to create model files and config files to be served to Triton Inference Server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "831be532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular.inference.triton import export_pytorch_ensemble\n",
    "export_pytorch_ensemble(\n",
    "    model,\n",
    "    workflow,\n",
    "    sparse_max=recsys_trainer.get_train_dataloader().dataset.sparse_max,\n",
    "    name= \"t4r_pytorch\",\n",
    "    model_path= \"/workspace/models/\",\n",
    "    label_columns =[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eeae8c",
   "metadata": {},
   "source": [
    "## 4. Serving Ensemble Model to the Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9881575",
   "metadata": {},
   "source": [
    "At this point, before connecing to the Triton Server, we launch the inference docker container and then load the ensemble `t4r_pytorch` to the inference server. This is done with the scripts below:\n",
    "\n",
    "**launch the docker container:**<br>\n",
    "```\n",
    "docker run -it --gpus device=0 -p 8000:8000 -p 8001:8001 -p 8002:8002 -v <path_to_saved_models>:/root/models/ nvcr.io/nvidia/merlin/merlin-inference:0.6\n",
    "```\n",
    "This script will mount your local model-repository folder that includes your saved models from the previous cell to `/root/models` directory in the merlin-inference docker container.\n",
    "\n",
    "**start triton server:**<br>\n",
    "After you started the merlin-inference container, you can start triton server with the command below. You need to provide correct path of the models folder.\n",
    "\n",
    "\n",
    "```\n",
    "tritonserver --model-repository=<path_to_models> --model-control-mode=explicit\n",
    "```\n",
    "Note: The model-repository path for our example is `/root/models`. The models haven't been loaded, yet. Below, we will request the Triton server to load the saved ensemble model below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdac1e3",
   "metadata": {},
   "source": [
    "- Connect to the Triton Inference Server and check if the server is alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b745029a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n",
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tritonhttpclient/__init__.py:31: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tritonhttpclient\n",
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))\n",
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056b4116",
   "metadata": {},
   "source": [
    "- Load raw data for inference: We select the last 50 interactions and filter out sessions with less than 2 interactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe2df868",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_merged_df=interactions_merged_df.sort_values('timestamp')\n",
    "batch = interactions_merged_df[-50:]\n",
    "sessions_to_use = batch.session_id.value_counts()[batch.session_id.value_counts() > 1].index.values\n",
    "filtered_batch = batch[batch.session_id.isin(sessions_to_use)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb7a61a",
   "metadata": {},
   "source": [
    "- Send the request to triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "223ed4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '77'}>\n",
      "bytearray(b'[{\"name\":\"t4r_pytorch\"},{\"name\":\"t4r_pytorch_nvt\"},{\"name\":\"t4r_pytorch_pt\"}]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 't4r_pytorch'},\n",
       " {'name': 't4r_pytorch_nvt'},\n",
       " {'name': 't4r_pytorch_pt'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9ba37",
   "metadata": {},
   "source": [
    "- Load the ensemble model to triton: If all models are loaded succesfully, you should be seeing `successfully loaded` status next to each model name on your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26576989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/t4r_pytorch/load, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 't4r_pytorch'\n"
     ]
    }
   ],
   "source": [
    "triton_client.load_model(model_name=\"t4r_pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2e6c9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output :\n",
      " [[-24.15944   -27.404991   -4.94611   ... -22.49609   -24.682003\n",
      "  -25.877012 ]\n",
      " [-15.90125   -15.065917   -6.661085  ... -15.156942  -15.099242\n",
      "  -15.492717 ]\n",
      " [-17.890999  -18.058104   -6.7184315 ... -17.763319  -17.483799\n",
      "  -18.03847  ]\n",
      " ...\n",
      " [-21.916159  -24.027723   -5.6256742 ... -22.193144  -23.233112\n",
      "  -24.193878 ]\n",
      " [-21.491413  -24.481197   -5.6657963 ... -22.603016  -22.793688\n",
      "  -24.614845 ]\n",
      " [-21.478308  -23.406733   -5.571887  ... -21.498636  -22.689898\n",
      "  -23.32546  ]]\n"
     ]
    }
   ],
   "source": [
    "import nvtabular.inference.triton as nvt_triton\n",
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "inputs = nvt_triton.convert_df_to_triton_input(filtered_batch.columns, filtered_batch, grpcclient.InferInput)\n",
    "\n",
    "output_names = [\"output\"]\n",
    "\n",
    "outputs = []\n",
    "for col in output_names:\n",
    "    outputs.append(grpcclient.InferRequestedOutput(col))\n",
    "    \n",
    "MODEL_NAME_NVT = \"t4r_pytorch\"\n",
    "\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(MODEL_NAME_NVT, inputs)\n",
    "    print(col, ':\\n', response.as_numpy(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524259ce",
   "metadata": {},
   "source": [
    "- Visualise top-k predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb981624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Top-5 predictions for session `11257991`: 88 || 302 || 93 || 289 || 166\n",
      "\n",
      "- Top-5 predictions for session `11270119`: 2774 || 81 || 157 || 224 || 2814\n",
      "\n",
      "- Top-5 predictions for session `11311424`: 2788 || 2214 || 157 || 620 || 633\n",
      "\n",
      "- Top-5 predictions for session `11336059`: 475 || 1987 || 1085 || 597 || 633\n",
      "\n",
      "- Top-5 predictions for session `11394056`: 224 || 2814 || 2789 || 53 || 157\n",
      "\n",
      "- Top-5 predictions for session `11399751`: 1453 || 1219 || 620 || 157 || 633\n",
      "\n",
      "- Top-5 predictions for session `11401481`: 224 || 633 || 157 || 620 || 2788\n",
      "\n",
      "- Top-5 predictions for session `11421333`: 836 || 1020 || 612 || 1169 || 51\n",
      "\n",
      "- Top-5 predictions for session `11425751`: 633 || 2214 || 2556 || 224 || 157\n",
      "\n",
      "- Top-5 predictions for session `11445777`: 206 || 804 || 442 || 184 || 33\n",
      "\n",
      "- Top-5 predictions for session `11457123`: 2 || 804 || 442 || 184 || 33\n",
      "\n",
      "- Top-5 predictions for session `11467406`: 620 || 2651 || 224 || 157 || 2788\n",
      "\n",
      "- Top-5 predictions for session `11493827`: 1756 || 206 || 33 || 61 || 423\n",
      "\n",
      "- Top-5 predictions for session `11528554`: 206 || 423 || 597 || 1085 || 475\n",
      "\n",
      "- Top-5 predictions for session `11561822`: 442 || 184 || 61 || 33 || 206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from demo_utils import visualize_response\n",
    "visualize_response(filtered_batch, response, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d923b01c",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ca2d4",
   "metadata": {},
   "source": [
    "- Merlin Transformers4rec: https://github.com/NVIDIA-Merlin/Transformers4Rec\n",
    "\n",
    "- Merlin NVTabular: https://github.com/NVIDIA/NVTabular/tree/main/nvtabular\n",
    "\n",
    "- Triton inference server: https://github.com/triton-inference-server"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b543a88d374ac88bf8df97911b380f671b13649694a5b49eb21e60fd27eb479"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
