{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0bb2de",
   "metadata": {},
   "source": [
    "# Triton for Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac113fc6",
   "metadata": {},
   "source": [
    "The Triton Inference Server allows us to deploy our model to the web regardless of cloud provider, and it supports a number of different machine learning frameworks such as TensorFlow and PyTorch.\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "Learn how to deploy a model to Triton\n",
    "1. Deploy saved NVTabular and PyTorch models to Triton Inference Server\n",
    "2. Sent requests for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a28d73",
   "metadata": {},
   "source": [
    "**Pull and start Inference docker container**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b4874",
   "metadata": {},
   "source": [
    "At this point, before connecing to the Triton Server, we launch the inference docker container and then load the ensemble t4r_pytorch to the inference server. This is done with the scripts below:\n",
    "\n",
    "launch the docker container:\n",
    "```\n",
    "docker run -it --gpus device=0 -p 8000:8000 -p 8001:8001 -p 8002:8002 -v <path_to_saved_models>:/root/models/ nvcr.io/nvidia/merlin/merlin-inference:0.6\n",
    "```\n",
    "\n",
    "This script will mount your local model-repository folder that includes your saved models from the previous cell to /root/models directory in the merlin-inference docker container.\n",
    "\n",
    "start triton server:\n",
    "After you started the merlin-inference container, you can start triton server with the command below. You need to provide correct path of the models folder.\n",
    "```\n",
    "tritonserver --model-repository=<path_to_models> --model-control-mode=explicit\n",
    "```\n",
    "Note: The model-repository path for our example is /root/models. The models haven't been loaded, yet. Below, we will request the Triton server to load the saved ensemble model below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b09b60",
   "metadata": {},
   "source": [
    "## 1. Deploy PyTorch and NVTabular Model to Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e01aae",
   "metadata": {},
   "source": [
    "Our Triton server has already been launched with to the web and is ready to make requests. We already, exportex the saved PyTorch model in the previous notebook, and generated the config files for Triton Inference Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dbe3c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Import dependencies\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import cudf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f99d8e",
   "metadata": {},
   "source": [
    "## 1.2 Review exported files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ddc9b",
   "metadata": {},
   "source": [
    "Triton expects a specific directory structure for our models as the following format:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e35a09",
   "metadata": {},
   "source": [
    "```\n",
    "<model-name>/\n",
    "[config.pbtxt]\n",
    "<version-name>/\n",
    "  [model.savedmodel]/\n",
    "    <pytorch_saved_model_files>/\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dcc2a5",
   "metadata": {},
   "source": [
    "Let's check out our model repository layout. You can install tree library with apt-get install tree, and then run `!tree /workspace/models/` to print out the model repository layout as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70113bb7",
   "metadata": {},
   "source": [
    "Triton needs a [config file](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/model_configuration.html) to understand how to interpret the model. Let's look at the generated config file. It defines the input columns with datatype and dimensions and the output layer. Manually creating this config file can be complicated and NVTabular provides an easy function with `export_pytorch_ensemble` to deploy PyTorch model to Triton.\n",
    "\n",
    "\n",
    "\n",
    "The config file needs the following information:\n",
    "* [name](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig4nameE): The name of our model. Must be the same name as the parent folder.\n",
    "* [platform](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig8platformE): The type of framework serving the model.\n",
    "* [input](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig5inputE): The input our model expects.\n",
    "  * `name`: Should correspond with the model input name.\n",
    "  * `data_type`: Should correspond to the input's data type.\n",
    "  * `dims`: The dimensions of the *request* for the input, as in the dimensions of the data the user passes to us.\n",
    "* [output](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig6outputE): The output parameters of our model.\n",
    "  * `name`: Should correspond with the model output name.\n",
    "  * `data_type`: Should correspond to the output's data type.\n",
    "  * `dims`: The dimensions of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5855c",
   "metadata": {},
   "source": [
    "## 1.3. Loading Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b3786",
   "metadata": {},
   "source": [
    "Next, let's build a client to connect to our server. This InferenceServerClient object is what we'll be using to talk to Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c9ada92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n",
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tritonhttpclient/__init__.py:31: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tritonhttpclient\n",
    "\n",
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"10.110.20.127:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))\n",
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75606387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '77'}>\n",
      "bytearray(b'[{\"name\":\"t4r_pytorch\"},{\"name\":\"t4r_pytorch_nvt\"},{\"name\":\"t4r_pytorch_pt\"}]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 't4r_pytorch'},\n",
       " {'name': 't4r_pytorch_nvt'},\n",
       " {'name': 't4r_pytorch_pt'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f400218f",
   "metadata": {},
   "source": [
    "- We load the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2652130e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/t4r_pytorch/load, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=400 headers={'content-type': 'application/json', 'content-length': '65'}>\n"
     ]
    },
    {
     "ename": "InferenceServerException",
     "evalue": "failed to load 't4r_pytorch', no version is available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17925/1682100151.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"t4r_pytorch\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtriton_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tritonclient/http/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_name, headers, query_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m                               \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                               query_params=query_params)\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0m_raise_if_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded model '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tritonclient/http/__init__.py\u001b[0m in \u001b[0;36m_raise_if_error\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInferenceServerException\u001b[0m: failed to load 't4r_pytorch', no version is available"
     ]
    }
   ],
   "source": [
    "model_name = \"t4r_pytorch\"\n",
    "triton_client.load_model(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1150c65",
   "metadata": {},
   "source": [
    "If all models are loaded succesfully, you should be seeing successfully loaded status next to each model name on your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d3d67",
   "metadata": {},
   "source": [
    "## 2. Sent Requests for Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef70b3",
   "metadata": {},
   "source": [
    "- Load raw data for inference: We select the last 20 interactions and filter out sessions with less than 2 interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "batch = cudf.read_parquet('Oct-2019.parquet').iloc[20:40,:]\n",
    "batch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98498c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_to_use = batch.user_session.value_counts()[batch.user_session.value_counts() > 1].index.values\n",
    "filtered_batch = batch[batch.user_session.isin(sessions_to_use)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvtabular.inference.triton as nvt_triton\n",
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "inputs = nvt_triton.convert_df_to_triton_input(filtered_batch.columns, filtered_batch, grpcclient.InferInput)\n",
    "\n",
    "output_names = [\"output\"]\n",
    "\n",
    "outputs = []\n",
    "for col in output_names:\n",
    "    outputs.append(grpcclient.InferRequestedOutput(col))\n",
    "    \n",
    "MODEL_NAME_NVT = \"t4r_pytorch\"\n",
    "\n",
    "with grpcclient.InferenceServerClient(\"10.110.20.127:8001\") as client:\n",
    "    response = client.infer(MODEL_NAME_NVT, inputs)\n",
    "    print(col, ':\\n', response.as_numpy(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.as_numpy('output').shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
