{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "constitutional-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import numpy\n",
    "import pandas as pd \n",
    "import cudf\n",
    "import cupy\n",
    "import nvtabular as nvt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-solomon",
   "metadata": {},
   "source": [
    "### Create random input data similar to pre-processed Yoochoose dataset structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liberal-decimal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>day</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category</th>\n",
       "      <th>timestamp/age_days</th>\n",
       "      <th>timestamp/weekday/sin</th>\n",
       "      <th>purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78657</td>\n",
       "      <td>3</td>\n",
       "      <td>14220</td>\n",
       "      <td>200</td>\n",
       "      <td>0.042894</td>\n",
       "      <td>0.980474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74690</td>\n",
       "      <td>5</td>\n",
       "      <td>45798</td>\n",
       "      <td>111</td>\n",
       "      <td>0.105155</td>\n",
       "      <td>0.009693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74941</td>\n",
       "      <td>1</td>\n",
       "      <td>27810</td>\n",
       "      <td>176</td>\n",
       "      <td>0.402443</td>\n",
       "      <td>0.858032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72915</td>\n",
       "      <td>8</td>\n",
       "      <td>15592</td>\n",
       "      <td>197</td>\n",
       "      <td>0.791257</td>\n",
       "      <td>0.507413</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79719</td>\n",
       "      <td>4</td>\n",
       "      <td>12723</td>\n",
       "      <td>233</td>\n",
       "      <td>0.302400</td>\n",
       "      <td>0.547591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>74677</td>\n",
       "      <td>1</td>\n",
       "      <td>48906</td>\n",
       "      <td>245</td>\n",
       "      <td>0.801508</td>\n",
       "      <td>0.408195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>76836</td>\n",
       "      <td>2</td>\n",
       "      <td>45562</td>\n",
       "      <td>99</td>\n",
       "      <td>0.677022</td>\n",
       "      <td>0.951179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>78828</td>\n",
       "      <td>5</td>\n",
       "      <td>19379</td>\n",
       "      <td>325</td>\n",
       "      <td>0.226396</td>\n",
       "      <td>0.329395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>74510</td>\n",
       "      <td>5</td>\n",
       "      <td>12348</td>\n",
       "      <td>85</td>\n",
       "      <td>0.598994</td>\n",
       "      <td>0.394180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>71054</td>\n",
       "      <td>5</td>\n",
       "      <td>2523</td>\n",
       "      <td>228</td>\n",
       "      <td>0.598271</td>\n",
       "      <td>0.730656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        session_id  day  item_id  category  timestamp/age_days  \\\n",
       "0            78657    3    14220       200            0.042894   \n",
       "1            74690    5    45798       111            0.105155   \n",
       "2            74941    1    27810       176            0.402443   \n",
       "3            72915    8    15592       197            0.791257   \n",
       "4            79719    4    12723       233            0.302400   \n",
       "...            ...  ...      ...       ...                 ...   \n",
       "199995       74677    1    48906       245            0.801508   \n",
       "199996       76836    2    45562        99            0.677022   \n",
       "199997       78828    5    19379       325            0.226396   \n",
       "199998       74510    5    12348        85            0.598994   \n",
       "199999       71054    5     2523       228            0.598271   \n",
       "\n",
       "        timestamp/weekday/sin  purchase  \n",
       "0                    0.980474         0  \n",
       "1                    0.009693         0  \n",
       "2                    0.858032         0  \n",
       "3                    0.507413         0  \n",
       "4                    0.547591         1  \n",
       "...                       ...       ...  \n",
       "199995               0.408195         1  \n",
       "199996               0.951179         0  \n",
       "199997               0.329395         0  \n",
       "199998               0.394180         1  \n",
       "199999               0.730656         0  \n",
       "\n",
       "[200000 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_ROWS = 200000\n",
    "session_length = 20\n",
    "inputs = {\n",
    "    'session_id': numpy.random.randint(70000, 80000, NUM_ROWS),\n",
    "    'day': numpy.random.randint(1, 10, NUM_ROWS),\n",
    "    'item_id': numpy.random.randint(1, 51996, NUM_ROWS),\n",
    "    'category': numpy.random.randint(0, 332, NUM_ROWS),\n",
    "    'timestamp/age_days': numpy.random.uniform(0, 1, NUM_ROWS),\n",
    "    'timestamp/weekday/sin' : numpy.random.uniform(0, 1, NUM_ROWS),\n",
    "    'purchase': numpy.random.randint(0, 2, NUM_ROWS)\n",
    "    }\n",
    "random_data = cudf.DataFrame(inputs)\n",
    "random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-commitment",
   "metadata": {},
   "source": [
    "### NVTabular workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-expert",
   "metadata": {},
   "source": [
    "- #TODO : Change the workflow using tagging API once it is finalized  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "narrow-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Groupby Workflow\n",
    "groupby_features = list(inputs.keys()) >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"session_id\"], \n",
    "    aggs={\n",
    "        \"item_id\": [\"list\"],\n",
    "        \"category\": [\"list\"],     \n",
    "        \"day\": [\"first\"],\n",
    "        \"purchase\": [\"first\"],\n",
    "        \"timestamp/age_days\": [\"list\"],\n",
    "        'timestamp/weekday/sin': [\"list\"],\n",
    "        },\n",
    "    name_sep=\"-\")\n",
    "# Trim sessions to first 20 items \n",
    "groupby_features_list = [x for x in groupby_features.output_columns.names if '-list' in x]\n",
    "\n",
    "#groupby_features_nonlist:  need to fix a BUG related to adding two workflow nodes\n",
    "#groupby_features_trim = groupby_features_list >> nvt.ops.ListSlice(0,20) >> nvt.ops.Rename(postfix = '_trim')\n",
    "workflow = nvt.Workflow(groupby_features)\n",
    "dataset = nvt.Dataset(random_data, cpu=False)\n",
    "workflow.fit(dataset)\n",
    "sessions_gdf = workflow.transform(dataset).to_ddf().compute()\n",
    "\n",
    "# Re-compute tri: to be removed when the BUG of two workflow nodes is fixed : \n",
    "groupby_features_trim =  groupby_features_list >> nvt.ops.ListSlice(0,20) >> nvt.ops.Rename(postfix = '_trim')\n",
    "workflow = nvt.Workflow(list(sessions_gdf.columns)  + groupby_features_trim)\n",
    "dataset = nvt.Dataset(sessions_gdf, cpu=False)\n",
    "workflow.fit(dataset)\n",
    "sessions_gdf = workflow.transform(dataset).to_ddf().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "first-constant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp/age_days-list_trim</th>\n",
       "      <th>timestamp/weekday/sin-list_trim</th>\n",
       "      <th>item_id-list_trim</th>\n",
       "      <th>category-list_trim</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp/age_days-list</th>\n",
       "      <th>timestamp/weekday/sin-list</th>\n",
       "      <th>day-first</th>\n",
       "      <th>item_id-list</th>\n",
       "      <th>purchase-first</th>\n",
       "      <th>category-list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.2159907365849888, 0.38978614558809666, 0.41...</td>\n",
       "      <td>[0.34691455014776873, 0.6141594448562755, 0.19...</td>\n",
       "      <td>[3453, 29113, 40598, 18312, 17358, 16541, 2983...</td>\n",
       "      <td>[146, 88, 231, 289, 43, 228, 243, 259, 10, 278...</td>\n",
       "      <td>70000</td>\n",
       "      <td>[0.2159907365849888, 0.38978614558809666, 0.41...</td>\n",
       "      <td>[0.34691455014776873, 0.6141594448562755, 0.19...</td>\n",
       "      <td>6</td>\n",
       "      <td>[3453, 29113, 40598, 18312, 17358, 16541, 2983...</td>\n",
       "      <td>0</td>\n",
       "      <td>[146, 88, 231, 289, 43, 228, 243, 259, 10, 278...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.16954246605001888, 0.7339434412221009, 0.54...</td>\n",
       "      <td>[0.29087761083519337, 0.23826932571806037, 0.2...</td>\n",
       "      <td>[17305, 47030, 9157, 3953, 39788, 40392, 3221,...</td>\n",
       "      <td>[154, 2, 297, 168, 140, 38, 280, 174, 118, 249...</td>\n",
       "      <td>70001</td>\n",
       "      <td>[0.16954246605001888, 0.7339434412221009, 0.54...</td>\n",
       "      <td>[0.29087761083519337, 0.23826932571806037, 0.2...</td>\n",
       "      <td>7</td>\n",
       "      <td>[17305, 47030, 9157, 3953, 39788, 40392, 3221,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[154, 2, 297, 168, 140, 38, 280, 174, 118, 249...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.7209095602846058, 0.11445547315539895, 0.34...</td>\n",
       "      <td>[0.43690866483674196, 0.6574986542124382, 0.49...</td>\n",
       "      <td>[10705, 7155, 40486, 6641, 25219, 8900, 42472,...</td>\n",
       "      <td>[51, 330, 314, 76, 271, 274, 265, 231, 78, 325...</td>\n",
       "      <td>70002</td>\n",
       "      <td>[0.7209095602846058, 0.11445547315539895, 0.34...</td>\n",
       "      <td>[0.43690866483674196, 0.6574986542124382, 0.49...</td>\n",
       "      <td>9</td>\n",
       "      <td>[10705, 7155, 40486, 6641, 25219, 8900, 42472,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[51, 330, 314, 76, 271, 274, 265, 231, 78, 325...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        timestamp/age_days-list_trim  \\\n",
       "0  [0.2159907365849888, 0.38978614558809666, 0.41...   \n",
       "1  [0.16954246605001888, 0.7339434412221009, 0.54...   \n",
       "2  [0.7209095602846058, 0.11445547315539895, 0.34...   \n",
       "\n",
       "                     timestamp/weekday/sin-list_trim  \\\n",
       "0  [0.34691455014776873, 0.6141594448562755, 0.19...   \n",
       "1  [0.29087761083519337, 0.23826932571806037, 0.2...   \n",
       "2  [0.43690866483674196, 0.6574986542124382, 0.49...   \n",
       "\n",
       "                                   item_id-list_trim  \\\n",
       "0  [3453, 29113, 40598, 18312, 17358, 16541, 2983...   \n",
       "1  [17305, 47030, 9157, 3953, 39788, 40392, 3221,...   \n",
       "2  [10705, 7155, 40486, 6641, 25219, 8900, 42472,...   \n",
       "\n",
       "                                  category-list_trim  session_id  \\\n",
       "0  [146, 88, 231, 289, 43, 228, 243, 259, 10, 278...       70000   \n",
       "1  [154, 2, 297, 168, 140, 38, 280, 174, 118, 249...       70001   \n",
       "2  [51, 330, 314, 76, 271, 274, 265, 231, 78, 325...       70002   \n",
       "\n",
       "                             timestamp/age_days-list  \\\n",
       "0  [0.2159907365849888, 0.38978614558809666, 0.41...   \n",
       "1  [0.16954246605001888, 0.7339434412221009, 0.54...   \n",
       "2  [0.7209095602846058, 0.11445547315539895, 0.34...   \n",
       "\n",
       "                          timestamp/weekday/sin-list  day-first  \\\n",
       "0  [0.34691455014776873, 0.6141594448562755, 0.19...          6   \n",
       "1  [0.29087761083519337, 0.23826932571806037, 0.2...          7   \n",
       "2  [0.43690866483674196, 0.6574986542124382, 0.49...          9   \n",
       "\n",
       "                                        item_id-list  purchase-first  \\\n",
       "0  [3453, 29113, 40598, 18312, 17358, 16541, 2983...               0   \n",
       "1  [17305, 47030, 9157, 3953, 39788, 40392, 3221,...               0   \n",
       "2  [10705, 7155, 40486, 6641, 25219, 8900, 42472,...               0   \n",
       "\n",
       "                                       category-list  \n",
       "0  [146, 88, 231, 289, 43, 228, 243, 259, 10, 278...  \n",
       "1  [154, 2, 297, 168, 140, 38, 280, 174, 118, 249...  \n",
       "2  [51, 330, 314, 76, 271, 274, 265, 231, 78, 325...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions_gdf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-floating",
   "metadata": {},
   "source": [
    "- We can save the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stylish-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.save('workflow_inference_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-chick",
   "metadata": {},
   "source": [
    "### Export pre-processed data by day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nearby-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a Dataset and write out hive-partitioned data to disk\n",
    "nvt_output_path_tmp ='./output_nvt_tmp/'\n",
    "PARTITION_COL = 'day-first'\n",
    "nvt.Dataset(sessions_gdf).to_parquet(nvt_output_path_tmp, partition_on=[PARTITION_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incoming-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER = \"./preproc_sessions_by_day_ts/\"\n",
    "!mkdir -p $OUTPUT_FOLDER\n",
    "days_folders = [f for f in sorted(os.listdir(nvt_output_path_tmp)) if f.startswith(PARTITION_COL)]\n",
    "for day_folder in days_folders:\n",
    "    df = cudf.read_parquet(os.path.join(nvt_output_path_tmp, day_folder))\n",
    "    out_folder = os.path.join(OUTPUT_FOLDER, day_folder.replace('day-first=', ''))\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    df.to_parquet(os.path.join(out_folder, 'train.parquet'))\n",
    "    \n",
    "    random_values = cupy.random.rand(len(df))\n",
    "    \n",
    "    #Extracts 10% for valid and test set. Those sessions are also in the train set, but as evaluation\n",
    "    #happens only for the subsequent day of training, that is not an issue, and we can keep the train set larger.\n",
    "    valid_set = df[random_values <= 0.10]\n",
    "    valid_set.to_parquet(os.path.join(out_folder, 'valid.parquet'))\n",
    "    \n",
    "    test_set = df[random_values >= 0.90]\n",
    "    test_set.to_parquet(os.path.join(out_folder, 'test.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-minority",
   "metadata": {},
   "source": [
    "# Transformers4rec model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "swedish-cooking",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b80faaada6c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers4rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorch4rec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers4rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTabularSequenceFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMLPBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequentialBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers4rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetSchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/first_example/Transformers4Rec/transformers4rec/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtabular\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAsTabular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFilterFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMergeTabular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTabularModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m __all__ = [\n",
      "\u001b[0;32m/workspace/first_example/Transformers4Rec/transformers4rec/torch/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mT4RecTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyarrowDataLoaderBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNVTDataLoaderBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/first_example/Transformers4Rec/transformers4rec/config/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mT4RecTrainingArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainingArguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \"\"\"\n\u001b[1;32m      9\u001b[0m     \u001b[0mClass\u001b[0m \u001b[0mthat\u001b[0m \u001b[0minherits\u001b[0m \u001b[0mHF\u001b[0m \u001b[0mTrainingArguments\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0madd\u001b[0m \u001b[0mon\u001b[0m \u001b[0mtop\u001b[0m \u001b[0mof\u001b[0m \u001b[0mit\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mneeded\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/first_example/Transformers4Rec/transformers4rec/config/trainer.py\u001b[0m in \u001b[0;36mT4RecTrainingArguments\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     data_loader_engine: Optional[str] = field(\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nvtabular\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         metadata={\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Optional' is not defined"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import transformers4rec.torch as torch4rec\n",
    "from transformers4rec.torch import TabularSequenceFeatures, MLPBlock, SequentialBlock, Head, TransformerBlock\n",
    "\n",
    "from transformers4rec.utils.schema import DatasetSchema\n",
    "from transformers4rec.torch.head import NextItemPredictionTask\n",
    "from transformers4rec.config import transformer\n",
    "from transformers4rec.torch.ranking_metric import NDCGAt, AvgPrecisionAt, RecallAt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-stupid",
   "metadata": {},
   "source": [
    "- Manually set the schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema object to pass it to the SequentialTabularFeatures\n",
    "schema = DatasetSchema.from_schema(\"schema.pb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-cowboy",
   "metadata": {},
   "source": [
    "### Define the sequential input module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-submission",
   "metadata": {},
   "source": [
    "Below we define our `input` bloc using [`SequentialTabularFeatures` class](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/features/sequential.py). The `from_schema` module directly parse schema and accepts categorical and continuous sequential inputs and supports data augmentation, data aggregation, `sequential-concat` and `elementwise-sum` aggregations, the projection of the interaction embeddings and the masking tasks.\n",
    "\n",
    "`max_sequence_length` defines the maximum sequence length of our sequential input, and if `continuous_projection` argument is set,  all numerical features are concatenated and projected by a number of MLP layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = TabularSequenceFeatures.from_schema(\n",
    "        schema,\n",
    "        max_sequence_length=20,\n",
    "        continuous_projection=64,\n",
    "        d_output=100,\n",
    "        masking=\"causal\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-disposition",
   "metadata": {},
   "source": [
    "### End-to-end session-based Transformer-based model for item prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-transsexual",
   "metadata": {},
   "source": [
    "- LM task + HF Transformer architecture + Next item-prediction task. \n",
    "- We build a [T4RecConfig](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/config/transformer.py#L8) class to update the config class of the transformer architecture with the specified arguments, then load the related model. Here we use it to instantiate an XLNET model according to the  arguments (d_model, n_head, etc.), defining the model architecture.\n",
    "- [TransformerBlock](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/block/transformer.py#L37) class is created to support HF Transformers for session-based and sequential-based recommendation models.\n",
    "- [NextItemPredictionTask](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/head.py#L212) is the class to support next item prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define XLNetConfig class and set default parameters \n",
    "\n",
    "# Set HF config of XLNet \n",
    "transformer_config = transformer.XLNetConfig.build(\n",
    "    d_model=64, n_head=4, n_layer=2, total_seq_length=20\n",
    ")\n",
    "# Define the model block including: inputs, masking, projection and transformer block.\n",
    "body = torch4rec.SequentialBlock(\n",
    "    inputs, torch4rec.MLPBlock([64]), torch4rec.TransformerBlock(transformer=transformer_config, masking=inputs.masking)\n",
    ")\n",
    "\n",
    "# Define the head related to next item prediction task \n",
    "head = torch4rec.Head(\n",
    "    body,\n",
    "    torch4rec.NextItemPredictionTask(weight_tying=True, hf_format=True),\n",
    "    inputs=inputs,\n",
    "    \n",
    ")\n",
    "\n",
    "# Get the end-to-end Model class \n",
    "model = torch4rec.Model(head, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-raise",
   "metadata": {},
   "source": [
    "# Non-incremental training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-person",
   "metadata": {},
   "source": [
    "- **Set Training arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers4rec.config.trainer import T4RecTrainingArguments\n",
    "from transformers4rec.torch import Trainer\n",
    "#Set argumentd for training \n",
    "train_args = T4RecTrainingArguments(local_rank = -1, dataloader_drop_last = True, engine='nvtabular',\n",
    "                                  report_to = [], debug = [\"r\"], gradient_accumulation_steps = 32,\n",
    "                                  per_device_train_batch_size = 512, per_device_eval_batch_size = 32,\n",
    "                                  output_dir = \".\", use_legacy_prediction_loop = False,\n",
    "                                  max_sequence_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-aberdeen",
   "metadata": {},
   "source": [
    "- **Define paths to train and eval data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from transformers4rec.torch.utils.data_utils import PyarrowDataLoaderBuilder, NVTDataLoaderBuilder\n",
    "train_data_paths = glob.glob(\"./preproc_sessions_by_day_ts/*/train.parquet\")[:-1]\n",
    "eval_data_paths = glob.glob(\"./preproc_sessions_by_day_ts/*/valid.parquet\")[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-debate",
   "metadata": {},
   "source": [
    "- **Define Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the T4Rec Trainer, which manages training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    schema=schema,\n",
    "    compute_metrics=True,\n",
    "    train_dataset_or_path=train_data_paths,\n",
    "    eval_dataset_or_path=eval_data_paths,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-driving",
   "metadata": {},
   "source": [
    "- **Train the model**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.reset_lr_scheduler()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-metro",
   "metadata": {},
   "source": [
    "- **Compute evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = trainer.evaluate(eval_dataset=eval_data_paths, metric_key_prefix='eval')\n",
    "for key in sorted(eval_metrics.keys()):\n",
    "    print(\"  %s = %s\" % (key, str(eval_metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-target",
   "metadata": {},
   "source": [
    "- **Compute Train metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = trainer.evaluate(eval_dataset=train_data_paths, metric_key_prefix='train')\n",
    "for key in sorted(train_metrics.keys()):\n",
    "    print(\"  %s = %s\" % (key, str(train_metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-brook",
   "metadata": {},
   "source": [
    "* **Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._save_model_and_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-majority",
   "metadata": {},
   "source": [
    "* **Reload model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_model_trainer_states_from_checkpoint('./checkpoint-3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-agency",
   "metadata": {},
   "source": [
    "- **Re-compute eval metrics of train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = trainer.evaluate(eval_dataset=train_data_paths, metric_key_prefix='train')\n",
    "for key in sorted(train_metrics.keys()):\n",
    "    print(\"  %s = %s\" % (key, str(train_metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-member",
   "metadata": {},
   "source": [
    "* **Resume Training** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset lr scheduler to train on new day data\n",
    "trainer.reset_lr_scheduler()\n",
    "# set new data from last day\n",
    "trainer.train_dataset = train_data_paths[-1]\n",
    "trainer.train(resume_from_checkpoint='./checkpoint-3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-commerce",
   "metadata": {},
   "source": [
    "- **Evaluate on last day**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new data from last day\n",
    "eval_metrics = trainer.evaluate(eval_dataset=eval_data_paths[-1], metric_key_prefix='eval')\n",
    "for key in sorted(eval_metrics.keys()):\n",
    "    print(\"  %s = %s\" % (key, str(eval_metrics[key])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
