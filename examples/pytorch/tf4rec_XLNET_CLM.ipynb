{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3643614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import numpy\n",
    "import pandas as pd \n",
    "import cudf\n",
    "import cupy\n",
    "import nvtabular as nvt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41769047",
   "metadata": {},
   "source": [
    "### Create random input data similar to pre-processed Yoochoose dataset structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "551db053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>day</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category</th>\n",
       "      <th>timestamp/age_days</th>\n",
       "      <th>timestamp/weekday/sin</th>\n",
       "      <th>purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77758</td>\n",
       "      <td>5</td>\n",
       "      <td>27127</td>\n",
       "      <td>87</td>\n",
       "      <td>0.610196</td>\n",
       "      <td>0.372406</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75614</td>\n",
       "      <td>7</td>\n",
       "      <td>18540</td>\n",
       "      <td>5</td>\n",
       "      <td>0.832218</td>\n",
       "      <td>0.552607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78956</td>\n",
       "      <td>6</td>\n",
       "      <td>37940</td>\n",
       "      <td>74</td>\n",
       "      <td>0.771559</td>\n",
       "      <td>0.504475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70217</td>\n",
       "      <td>8</td>\n",
       "      <td>30688</td>\n",
       "      <td>73</td>\n",
       "      <td>0.913372</td>\n",
       "      <td>0.053951</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72516</td>\n",
       "      <td>6</td>\n",
       "      <td>48452</td>\n",
       "      <td>154</td>\n",
       "      <td>0.032286</td>\n",
       "      <td>0.261013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>73903</td>\n",
       "      <td>3</td>\n",
       "      <td>24334</td>\n",
       "      <td>163</td>\n",
       "      <td>0.682913</td>\n",
       "      <td>0.249499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>74271</td>\n",
       "      <td>5</td>\n",
       "      <td>11657</td>\n",
       "      <td>160</td>\n",
       "      <td>0.461387</td>\n",
       "      <td>0.412575</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>74480</td>\n",
       "      <td>2</td>\n",
       "      <td>49481</td>\n",
       "      <td>94</td>\n",
       "      <td>0.716491</td>\n",
       "      <td>0.203513</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>79750</td>\n",
       "      <td>9</td>\n",
       "      <td>22656</td>\n",
       "      <td>147</td>\n",
       "      <td>0.271502</td>\n",
       "      <td>0.581491</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>76758</td>\n",
       "      <td>7</td>\n",
       "      <td>4012</td>\n",
       "      <td>62</td>\n",
       "      <td>0.567699</td>\n",
       "      <td>0.951343</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_id  day  item_id  category  timestamp/age_days  \\\n",
       "0          77758    5    27127        87            0.610196   \n",
       "1          75614    7    18540         5            0.832218   \n",
       "2          78956    6    37940        74            0.771559   \n",
       "3          70217    8    30688        73            0.913372   \n",
       "4          72516    6    48452       154            0.032286   \n",
       "...          ...  ...      ...       ...                 ...   \n",
       "9995       73903    3    24334       163            0.682913   \n",
       "9996       74271    5    11657       160            0.461387   \n",
       "9997       74480    2    49481        94            0.716491   \n",
       "9998       79750    9    22656       147            0.271502   \n",
       "9999       76758    7     4012        62            0.567699   \n",
       "\n",
       "      timestamp/weekday/sin  purchase  \n",
       "0                  0.372406         0  \n",
       "1                  0.552607         0  \n",
       "2                  0.504475         1  \n",
       "3                  0.053951         0  \n",
       "4                  0.261013         0  \n",
       "...                     ...       ...  \n",
       "9995               0.249499         0  \n",
       "9996               0.412575         0  \n",
       "9997               0.203513         1  \n",
       "9998               0.581491         0  \n",
       "9999               0.951343         1  \n",
       "\n",
       "[10000 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_ROWS = 10000\n",
    "session_length = 20\n",
    "batch_size = 100\n",
    "inputs = {\n",
    "    'session_id': numpy.random.randint(70000, 80000, NUM_ROWS),\n",
    "    'day': numpy.random.randint(1, 10, NUM_ROWS),\n",
    "    'item_id': numpy.random.randint(1, 51996, NUM_ROWS),\n",
    "    'category': numpy.random.randint(0, 332, NUM_ROWS),\n",
    "    'timestamp/age_days': numpy.random.uniform(0, 1, NUM_ROWS),\n",
    "    'timestamp/weekday/sin' : numpy.random.uniform(0, 1, NUM_ROWS),\n",
    "    'purchase': numpy.random.randint(0, 2, NUM_ROWS)\n",
    "    }\n",
    "random_data = cudf.DataFrame(inputs)\n",
    "random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1827df",
   "metadata": {},
   "source": [
    "### NVTabular workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f83f8f",
   "metadata": {},
   "source": [
    "- #TODO : Change the workflow using tagging API once it is finalized  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed85e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronayak/ronaya/NVTabular/nvtabular/workflow/node.py:43: FutureWarning: The `[\"a\", \"b\", \"c\"] >> ops.Operator` syntax for creating a `ColumnGroup` has been deprecated in NVTabular 21.09 and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Groupby Workflow\n",
    "groupby_features = list(inputs.keys()) >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"session_id\"], \n",
    "    aggs={\n",
    "        \"item_id\": [\"list\"],\n",
    "        \"category\": [\"list\"],     \n",
    "        \"day\": [\"first\"],\n",
    "        \"purchase\": [\"first\"],\n",
    "        \"timestamp/age_days\": [\"list\"],\n",
    "        'timestamp/weekday/sin': [\"list\"],\n",
    "        },\n",
    "    name_sep=\"-\")\n",
    "# Trim sessions to first 20 items \n",
    "groupby_features_nonlist = [x for x in groupby_features.selector if '-list' not in x]\n",
    "groupby_features_nonlist\n",
    "groupby_features_trim = ((groupby_features - groupby_features_nonlist)) >> nvt.ops.ListSlice(0,20) >> nvt.ops.Rename(postfix = '_trim')\n",
    "\n",
    "workflow = nvt.Workflow(groupby_features + groupby_features_trim )\n",
    "dataset = nvt.Dataset(random_data, cpu=False)\n",
    "workflow.fit(dataset)\n",
    "sessions_gdf = workflow.transform(dataset).to_ddf().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb01c7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp/weekday/sin-list</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp/age_days-list</th>\n",
       "      <th>day-first</th>\n",
       "      <th>item_id-list</th>\n",
       "      <th>purchase-first</th>\n",
       "      <th>category-list</th>\n",
       "      <th>timestamp/weekday/sin-list_trim</th>\n",
       "      <th>timestamp/age_days-list_trim</th>\n",
       "      <th>item_id-list_trim</th>\n",
       "      <th>category-list_trim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3533943546820013]</td>\n",
       "      <td>70000</td>\n",
       "      <td>[0.03640205328638679]</td>\n",
       "      <td>7</td>\n",
       "      <td>[14574]</td>\n",
       "      <td>1</td>\n",
       "      <td>[135]</td>\n",
       "      <td>[0.3533943546820013]</td>\n",
       "      <td>[0.03640205328638679]</td>\n",
       "      <td>[14574]</td>\n",
       "      <td>[135]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.04203522126107073, 0.3275783399892225]</td>\n",
       "      <td>70002</td>\n",
       "      <td>[0.005719239287048983, 0.9500323724164531]</td>\n",
       "      <td>3</td>\n",
       "      <td>[41003, 30571]</td>\n",
       "      <td>0</td>\n",
       "      <td>[234, 119]</td>\n",
       "      <td>[0.04203522126107073, 0.3275783399892225]</td>\n",
       "      <td>[0.005719239287048983, 0.9500323724164531]</td>\n",
       "      <td>[41003, 30571]</td>\n",
       "      <td>[234, 119]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.6206760679502472]</td>\n",
       "      <td>70003</td>\n",
       "      <td>[0.8331312174661043]</td>\n",
       "      <td>5</td>\n",
       "      <td>[48445]</td>\n",
       "      <td>1</td>\n",
       "      <td>[166]</td>\n",
       "      <td>[0.6206760679502472]</td>\n",
       "      <td>[0.8331312174661043]</td>\n",
       "      <td>[48445]</td>\n",
       "      <td>[166]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp/weekday/sin-list  session_id  \\\n",
       "0                       [0.3533943546820013]       70000   \n",
       "1  [0.04203522126107073, 0.3275783399892225]       70002   \n",
       "2                       [0.6206760679502472]       70003   \n",
       "\n",
       "                      timestamp/age_days-list  day-first    item_id-list  \\\n",
       "0                       [0.03640205328638679]          7         [14574]   \n",
       "1  [0.005719239287048983, 0.9500323724164531]          3  [41003, 30571]   \n",
       "2                        [0.8331312174661043]          5         [48445]   \n",
       "\n",
       "   purchase-first category-list            timestamp/weekday/sin-list_trim  \\\n",
       "0               1         [135]                       [0.3533943546820013]   \n",
       "1               0    [234, 119]  [0.04203522126107073, 0.3275783399892225]   \n",
       "2               1         [166]                       [0.6206760679502472]   \n",
       "\n",
       "                 timestamp/age_days-list_trim item_id-list_trim  \\\n",
       "0                       [0.03640205328638679]           [14574]   \n",
       "1  [0.005719239287048983, 0.9500323724164531]    [41003, 30571]   \n",
       "2                        [0.8331312174661043]           [48445]   \n",
       "\n",
       "  category-list_trim  \n",
       "0              [135]  \n",
       "1         [234, 119]  \n",
       "2              [166]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions_gdf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e0755-dc55-4cd6-b89b-b133130a15a8",
   "metadata": {},
   "source": [
    "- We can save the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "909251f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.save('workflow_inference_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba34c9",
   "metadata": {},
   "source": [
    "### Export pre-processed data by day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7476bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a Dataset and write out hive-partitioned data to disk\n",
    "nvt_output_path_tmp ='./output_nvt_tmp/'\n",
    "PARTITION_COL = 'day-first'\n",
    "nvt.Dataset(sessions_gdf).to_parquet(nvt_output_path_tmp, partition_on=[PARTITION_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e329fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER = \"./preproc_sessions_by_day_ts/\"\n",
    "!mkdir -p $OUTPUT_FOLDER\n",
    "days_folders = [f for f in sorted(os.listdir(nvt_output_path_tmp)) if f.startswith(PARTITION_COL)]\n",
    "for day_folder in days_folders:\n",
    "    df = cudf.read_parquet(os.path.join(nvt_output_path_tmp, day_folder))\n",
    "    out_folder = os.path.join(OUTPUT_FOLDER, day_folder.replace('day-first=', ''))\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    df.to_parquet(os.path.join(out_folder, 'train.parquet'))\n",
    "    \n",
    "    random_values = cupy.random.rand(len(df))\n",
    "    \n",
    "    #Extracts 10% for valid and test set. Those sessions are also in the train set, but as evaluation\n",
    "    #happens only for the subsequent day of training, that is not an issue, and we can keep the train set larger.\n",
    "    valid_set = df[random_values <= 0.10]\n",
    "    valid_set.to_parquet(os.path.join(out_folder, 'valid.parquet'))\n",
    "    \n",
    "    test_set = df[random_values >= 0.90]\n",
    "    test_set.to_parquet(os.path.join(out_folder, 'test.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f76c4",
   "metadata": {},
   "source": [
    "# Transformers4rec model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f74589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import transformers4rec.torch as torch4rec\n",
    "from transformers4rec.torch import SequentialTabularFeatures, MLPBlock, SequentialBlock, Head, TransformerBlock\n",
    "\n",
    "from transformers4rec.utils.schema import DatasetSchema\n",
    "\n",
    "from transformers4rec.torch.head import NextItemPredictionTask\n",
    "\n",
    "from transformers4rec.config import transformer\n",
    "from transformers4rec.torch.ranking_metric import NDCGAt, AvgPrecisionAt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dd5b7c-6772-4a66-9799-32bcdb3a2cdf",
   "metadata": {},
   "source": [
    "- Manually set the schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2d484ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema object to pass it to the SequentialTabularFeatures\n",
    "schema = DatasetSchema.from_schema(\"schema.pb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93309161-2ec0-4d41-97d2-6666cbe9bf07",
   "metadata": {},
   "source": [
    "### Define the sequential input module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a6f4ce-0b6a-4328-a9e1-0fd9eb1e3374",
   "metadata": {},
   "source": [
    "Below we define our `input` bloc using [`SequentialTabularFeatures` class](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/features/sequential.py). The `from_schema` module directly parse schema and accepts categorical and continuous sequential inputs and supports data augmentation, data aggregation, `sequential-concat` and `elementwise-sum` aggregations, the projection of the interaction embeddings and the masking tasks.\n",
    "\n",
    "`max_sequence_length` defines the maximum sequence length of our sequential input, and if `continuous_projection` argument is set,  all numerical features are concatenated and projected by a number of MLP layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516942f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = SequentialTabularFeatures.from_schema(\n",
    "        schema,\n",
    "        max_sequence_length=20,\n",
    "        continuous_projection=64,\n",
    "        d_output=100,\n",
    "        masking=\"causal\",\n",
    "    )\n",
    "\n",
    "inputs.masking.device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce0cff",
   "metadata": {},
   "source": [
    "### End-to-end session-based Transformer-based model for item prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc699ca-5622-401e-b844-627ccaf3f925",
   "metadata": {},
   "source": [
    "- LM task + HF Transformer architecture + Next item-prediction task. \n",
    "- We build a [T4RecConfig](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/config/transformer.py#L8) class to update the config class of the transformer architecture with the specified arguments, then load the related model. Here we use it to instantiate an XLNET model according to the  arguments (d_model, n_head, etc.), defining the model architecture.\n",
    "- [TransformerBlock](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/block/transformer.py#L37) class is created to support HF Transformers for session-based and sequential-based recommendation models.\n",
    "- [NextItemPredictionTask](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/head.py#L212) is the class to support next item prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c1cd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case-1: Define XLNetConfig class and set default parameters \n",
    "\n",
    "transformer_config = transformer.XLNetConfig.build(\n",
    "    d_model=64, n_head=4, n_layer=2, total_seq_length=20\n",
    ")\n",
    "\n",
    "body = torch4rec.SequentialBlock(\n",
    "    inputs, torch4rec.MLPBlock([64]), torch4rec.TransformerBlock(transformer=transformer_config, masking=inputs.masking)\n",
    ")\n",
    "\n",
    "head = torch4rec.Head(\n",
    "    body,\n",
    "    torch4rec.NextItemPredictionTask(weight_tying=True, hf_format=True),\n",
    "    inputs=inputs,\n",
    ")\n",
    "model = torch4rec.Model(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252fc69",
   "metadata": {},
   "source": [
    "# Train the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf445e-7c01-4119-b4bf-3509e21e8546",
   "metadata": {},
   "source": [
    "- Load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c0a16c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers4rec.recsys_args import DataArguments, ModelArguments, TrainingArguments\n",
    "\n",
    "TrainingArguments.local_rank = -1\n",
    "TrainingArguments.world_size = 1\n",
    "TrainingArguments.dataloader_drop_last = True\n",
    "TrainingArguments.device = \"cuda\"\n",
    "TrainingArguments.report_to = []\n",
    "TrainingArguments.debug = [\"r\"]\n",
    "TrainingArguments.n_gpu = 1\n",
    "TrainingArguments.gradient_accumulation_steps = 32\n",
    "TrainingArguments.train_batch_size = 512\n",
    "TrainingArguments.per_device_train_batch_size = 512\n",
    "TrainingArguments.per_device_eval_batch_size = 512\n",
    "TrainingArguments.output_dir = \"\"\n",
    "TrainingArguments.world_size = 1\n",
    "\n",
    "\n",
    "DataArguments.data_path = \"./preproc_sessions_by_day_ts/\"\n",
    "DataArguments.data_loader_engine = \"nvtabular\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a2f360-c477-4dd8-8fc6-ae71d6468be8",
   "metadata": {},
   "source": [
    "- Load data using old NVTabular dataloader  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3b4a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# NVTabular dependencies\n",
    "from nvtabular import Dataset as NVTDataset\n",
    "from nvtabular.loader.torch import DLDataLoader\n",
    "from nvtabular.loader.torch import TorchAsyncItr as NVTDataLoader\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n",
    "\n",
    "SESSION_LENGTH_MAX = 20\n",
    "\n",
    "x_cat_names, x_cont_names = ['item_id-list_trim', 'category-list_trim'], ['timestamp/weekday/sin-list_trim','timestamp/age_days-list_trim']\n",
    "\n",
    "sparse_features_max = {\n",
    "    fname: SESSION_LENGTH_MAX\n",
    "    for fname in x_cat_names + x_cont_names\n",
    "}\n",
    "\n",
    "train_data_paths = glob.glob(\"./preproc_sessions_by_day_ts/*/train.parquet\")\n",
    "train_dataset = NVTDataset(\n",
    "    train_data_paths,\n",
    "    engine=\"parquet\",\n",
    ")\n",
    "\n",
    "def dataloader_collate_dict(inputs):\n",
    "    # Gets only the features dict\n",
    "    inputs = inputs[0][0]\n",
    "    return inputs\n",
    "\n",
    "class DLDataLoaderWrapper(DLDataLoader):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        if \"batch_size\" in kwargs:\n",
    "            # Setting the batch size directly to DLDataLoader makes it 3x slower. \n",
    "            # So we set as an alternative attribute and use it within RecSysTrainer during evaluation\n",
    "            self._batch_size = kwargs.pop(\"batch_size\")\n",
    "        super().__init__(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8ba308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = NVTDataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=TrainingArguments.train_batch_size,\n",
    "    shuffle=False,\n",
    "    cats=x_cat_names,\n",
    "    conts=x_cont_names,\n",
    "    device=0,\n",
    "    labels=[],\n",
    "    sparse_names=x_cat_names + x_cont_names,\n",
    "    sparse_max=sparse_features_max,\n",
    "    sparse_as_dense=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "dl_loader = DLDataLoaderWrapper(\n",
    "    loader, collate_fn=dataloader_collate_dict, batch_size=TrainingArguments.train_batch_size\n",
    "    )\n",
    "out = next(iter(dl_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637805b9-0671-4bfe-ad3f-88a84ddfb4b8",
   "metadata": {},
   "source": [
    "- Test the output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out['item_id-list_trim'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "068b2b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_id-list_trim': tensor([[24401,     0,     0,  ...,     0,     0,     0],\n",
       "         [ 1028,     0,     0,  ...,     0,     0,     0],\n",
       "         [ 3865,     0,     0,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [30488, 17763,     0,  ...,     0,     0,     0],\n",
       "         [33885, 37891,     0,  ...,     0,     0,     0],\n",
       "         [22565,  3352,     0,  ...,     0,     0,     0]], device='cuda:0'),\n",
       " 'category-list_trim': tensor([[138,   0,   0,  ...,   0,   0,   0],\n",
       "         [ 91,   0,   0,  ...,   0,   0,   0],\n",
       "         [100,   0,   0,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [295, 273,   0,  ...,   0,   0,   0],\n",
       "         [135, 223,   0,  ...,   0,   0,   0],\n",
       "         [107, 114,   0,  ...,   0,   0,   0]], device='cuda:0'),\n",
       " 'timestamp/weekday/sin-list_trim': tensor([[0.0753, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.8018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5628, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.7691, 0.6410, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5408, 0.9047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5669, 0.9635, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "        device='cuda:0'),\n",
       " 'timestamp/age_days-list_trim': tensor([[0.7847, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.6172, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.6521, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.9003, 0.5759, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0059, 0.4263, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0722, 0.6637, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18a69745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# NVTabular dependencies\n",
    "from nvtabular import Dataset as NVTDataset\n",
    "from nvtabular.loader.torch import DLDataLoader\n",
    "from nvtabular.loader.torch import TorchAsyncItr as NVTDataLoader\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n",
    "\n",
    "\n",
    "SESSION_LENGTH_MAX = 20\n",
    "\n",
    "x_cat_names, x_cont_names = ['item_id-list_trim', 'category-list_trim'], ['timestamp/weekday/sin-list_trim','timestamp/age_days-list_trim']\n",
    "\n",
    "sparse_features_max = {\n",
    "    fname: SESSION_LENGTH_MAX\n",
    "    for fname in x_cat_names + x_cont_names\n",
    "}\n",
    "\n",
    "train_data_paths = glob.glob(\"./preproc_sessions_by_day_ts/*/train.parquet\")\n",
    "train_dataset = NVTDataset(\n",
    "    train_data_paths,\n",
    "    engine=\"parquet\",\n",
    ")\n",
    "\n",
    "def dataloader_collate_dict(inputs):\n",
    "    # Gets only the features dict\n",
    "    inputs = inputs[0][0]\n",
    "    return inputs\n",
    "\n",
    "class DLDataLoaderWrapper(DLDataLoader):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        if \"batch_size\" in kwargs:\n",
    "            self._batch_size = kwargs.pop(\"batch_size\")\n",
    "        super().__init__(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f99f250-9890-4cc6-9a97-40388e0c91da",
   "metadata": {},
   "source": [
    "We define wrapper that is needed to prepare the modelâ€™s inputs in the format required by [HuggingFace Trainer](https://github.com/huggingface/transformers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a32cdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFWrapper(torch.nn.Module): \n",
    "    def __init__(self, model): \n",
    "        super().__init__()\n",
    "        self.model = model \n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        inputs = kwargs\n",
    "        return model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d4353db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wp = HFWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3bb295a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HFWrapper(\n",
       "  (model): Model(\n",
       "    (heads): ModuleList(\n",
       "      (0): Head(\n",
       "        (body): SequentialBlock(\n",
       "          (0): SequentialTabularFeatures(\n",
       "            (to_merge): ModuleDict(\n",
       "              (continuous_module): SequentialBlock(\n",
       "                (0): ContinuousFeatures(\n",
       "                  (filter_features): FilterFeatures()\n",
       "                  (_aggregation): SequentialConcatFeatures()\n",
       "                )\n",
       "                (1): SequentialBlock(\n",
       "                  (0): DenseBlock(\n",
       "                    (0): Linear(in_features=1, out_features=64, bias=True)\n",
       "                    (1): ReLU(inplace=True)\n",
       "                  )\n",
       "                )\n",
       "                (2): AsTabular()\n",
       "              )\n",
       "              (categorical_module): SequentialEmbeddingFeatures(\n",
       "                (filter_features): FilterFeatures()\n",
       "                (embedding_tables): ModuleDict(\n",
       "                  (category-list_trim): Embedding(332, 64, padding_idx=0)\n",
       "                  (item_id-list_trim): Embedding(51996, 64, padding_idx=0)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (_aggregation): SequentialConcatFeatures()\n",
       "            (projection_module): SequentialBlock(\n",
       "              (0): DenseBlock(\n",
       "                (0): Linear(in_features=192, out_features=100, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (masking): CausalLanguageModeling()\n",
       "          )\n",
       "          (1): SequentialBlock(\n",
       "            (0): DenseBlock(\n",
       "              (0): Linear(in_features=100, out_features=64, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): TansformerBlock(\n",
       "            (transformer): XLNetModel(\n",
       "              (word_embedding): Embedding(1, 64)\n",
       "              (layer): ModuleList(\n",
       "                (0): XLNetLayer(\n",
       "                  (rel_attn): XLNetRelativeAttention(\n",
       "                    (layer_norm): LayerNorm((64,), eps=0.03, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.3, inplace=False)\n",
       "                  )\n",
       "                  (ff): XLNetFeedForward(\n",
       "                    (layer_norm): LayerNorm((64,), eps=0.03, elementwise_affine=True)\n",
       "                    (layer_1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                    (layer_2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                    (dropout): Dropout(p=0.3, inplace=False)\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (1): XLNetLayer(\n",
       "                  (rel_attn): XLNetRelativeAttention(\n",
       "                    (layer_norm): LayerNorm((64,), eps=0.03, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.3, inplace=False)\n",
       "                  )\n",
       "                  (ff): XLNetFeedForward(\n",
       "                    (layer_norm): LayerNorm((64,), eps=0.03, elementwise_affine=True)\n",
       "                    (layer_1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                    (layer_2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                    (dropout): Dropout(p=0.3, inplace=False)\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (masking): CausalLanguageModeling()\n",
       "          )\n",
       "        )\n",
       "        (prediction_tasks): ModuleDict(\n",
       "          (0): NextItemPredictionTask(\n",
       "            (sequence_summary): SequenceSummary(\n",
       "              (summary): Identity()\n",
       "              (activation): Identity()\n",
       "              (first_dropout): Identity()\n",
       "              (last_dropout): Identity()\n",
       "            )\n",
       "            (metrics): ModuleList()\n",
       "            (loss): NLLLoss()\n",
       "            (item_embedding_table): Embedding(51996, 64, padding_idx=0)\n",
       "            (masking): CausalLanguageModeling()\n",
       "            (pre): Block(\n",
       "              (module): NextItemPredictionTask(\n",
       "                (item_embedding_table): Embedding(51996, 64, padding_idx=0)\n",
       "                (log_softmax): LogSoftmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcdfee6",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ef136",
   "metadata": {},
   "source": [
    "- Basic fit  and evaluate to test the model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2020e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the RecSysTrainer, which manages training and evaluation\n",
    "from transformers4rec.recsys_trainer import RecSysTrainer, DatasetType\n",
    "\n",
    "trainer = RecSysTrainer(\n",
    "    model=model_wp,\n",
    "    args=TrainingArguments,\n",
    "    model_args=ModelArguments,\n",
    "    data_args=DataArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a1ada5",
   "metadata": {},
   "source": [
    "- Fit the model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5215c65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=16.449358622233074, metrics={'train_runtime': 1.4791, 'train_samples_per_second': 2.028, 'total_flos': 0.0, 'epoch': 3.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_gpu_alloc_delta': 14147584, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 7831552, 'train_mem_gpu_alloc_delta': 41794048, 'train_mem_cpu_peaked_delta': 192512, 'train_mem_gpu_peaked_delta': 441261568})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.set_train_dataloader(dl_loader)\n",
    "trainer.reset_lr_scheduler()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2dd79-2cd4-4235-9e4d-a8ea048b922c",
   "metadata": {},
   "source": [
    "- Evaluate the model: here we evaluate the model using training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9415da28-13b8-44ec-9845-eda3a9efde7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all data has been set. Are you sure you passed all values?\n",
      "Not all data has been set. Are you sure you passed all values?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch = 3.0\n",
      "  eval_mem_cpu_alloc_delta = 4096\n",
      "  eval_mem_cpu_peaked_delta = 184320\n",
      "  eval_mem_gpu_alloc_delta = 0\n",
      "  eval_mem_gpu_peaked_delta = 289772544\n",
      "  train_avg_precision@10 = 0.0\n",
      "  train_avg_precision@1000 = 8.79675315865405e-05\n",
      "  train_avg_precision@20 = 2.249212971387001e-05\n",
      "  train_loss = 34.15816409771259\n",
      "  train_ndcg@10 = 0.0\n",
      "  train_ndcg@1000 = 0.002303272736473725\n",
      "  train_ndcg@20 = 7.293877514222494e-05\n",
      "  train_precision@10 = 0.0\n",
      "  train_precision@1000 = 1.9401000827201642e-05\n",
      "  train_precision@20 = 1.349527715669515e-05\n",
      "  train_recall@10 = 0.0\n",
      "  train_recall@1000 = 0.01940100109921052\n",
      "  train_recall@20 = 0.0002699055386563906\n",
      "  train_runtime = 3.9485\n",
      "  train_samples_per_second = 2593.362\n"
     ]
    }
   ],
   "source": [
    "trainer.set_eval_dataloader(dl_loader)\n",
    "train_metrics = trainer.evaluate(metric_key_prefix=DatasetType.train.value)\n",
    "for key in sorted(train_metrics.keys()):\n",
    "    print(\"  %s = %s\" % (key, str(train_metrics[key])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
