{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a098fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6337a",
   "metadata": {},
   "source": [
    "# Session-based Recommendation with XLNET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a013f",
   "metadata": {},
   "source": [
    "In this notebook, we build a session-based recommendation model with XLNET, and train and evaluate it with NVTabular Pytorch Dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "befd4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import numpy\n",
    "import pandas as pd \n",
    "import cudf\n",
    "import cupy\n",
    "import nvtabular as nvt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee9da4c",
   "metadata": {},
   "source": [
    "## Create a Synthetic Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a4b692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>day</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category</th>\n",
       "      <th>timestamp/age_days</th>\n",
       "      <th>timestamp/weekday/sin</th>\n",
       "      <th>purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70658</td>\n",
       "      <td>2</td>\n",
       "      <td>12541</td>\n",
       "      <td>27</td>\n",
       "      <td>0.738148</td>\n",
       "      <td>0.024413</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71982</td>\n",
       "      <td>2</td>\n",
       "      <td>27523</td>\n",
       "      <td>299</td>\n",
       "      <td>0.274989</td>\n",
       "      <td>0.435644</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75009</td>\n",
       "      <td>9</td>\n",
       "      <td>30666</td>\n",
       "      <td>46</td>\n",
       "      <td>0.339352</td>\n",
       "      <td>0.164251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70424</td>\n",
       "      <td>3</td>\n",
       "      <td>17118</td>\n",
       "      <td>219</td>\n",
       "      <td>0.778682</td>\n",
       "      <td>0.985127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78554</td>\n",
       "      <td>8</td>\n",
       "      <td>32957</td>\n",
       "      <td>25</td>\n",
       "      <td>0.753821</td>\n",
       "      <td>0.736395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id  day  item_id  category  timestamp/age_days  \\\n",
       "0       70658    2    12541        27            0.738148   \n",
       "1       71982    2    27523       299            0.274989   \n",
       "2       75009    9    30666        46            0.339352   \n",
       "3       70424    3    17118       219            0.778682   \n",
       "4       78554    8    32957        25            0.753821   \n",
       "\n",
       "   timestamp/weekday/sin  purchase  \n",
       "0               0.024413         0  \n",
       "1               0.435644         1  \n",
       "2               0.164251         1  \n",
       "3               0.985127         1  \n",
       "4               0.736395         0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_ROWS = 100000\n",
    "inputs = {\n",
    "    'session_id': numpy.random.randint(70000, 80000, NUM_ROWS),\n",
    "    'day': numpy.random.randint(1, 10, NUM_ROWS),\n",
    "    'item_id': numpy.random.randint(1, 51996, NUM_ROWS),\n",
    "    'category': numpy.random.randint(0, 332, NUM_ROWS),\n",
    "    'timestamp/age_days': numpy.random.uniform(0, 1, NUM_ROWS),\n",
    "    'timestamp/weekday/sin' : numpy.random.uniform(0, 1, NUM_ROWS),\n",
    "    'purchase': numpy.random.randint(0, 2, NUM_ROWS)\n",
    "    }\n",
    "random_data = cudf.DataFrame(inputs)\n",
    "random_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece43dd",
   "metadata": {},
   "source": [
    "## Feature Engineering with NVTabular Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36682d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvtabular/nvtabular/workflow/node.py:45: FutureWarning: The `[\"a\", \"b\", \"c\"] >> ops.Operator` syntax for creating a `ColumnGroup` has been deprecated in NVTabular 21.09 and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Groupby Workflow\n",
    "groupby_feats = ['session_id', 'day', 'item_id', 'category', 'timestamp/age_days', 'timestamp/weekday/sin', 'purchase']\n",
    "\n",
    "groupby_features = groupby_feats >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"session_id\"], \n",
    "    aggs={\n",
    "        \"item_id\": [\"list\", \"count\"],\n",
    "        \"category\": [\"list\"],     \n",
    "        \"day\": [\"first\"],\n",
    "        \"purchase\": [\"first\"],\n",
    "        \"timestamp/age_days\": [\"list\"],\n",
    "        'timestamp/weekday/sin': [\"list\"],\n",
    "        },\n",
    "    name_sep=\"-\")\n",
    "\n",
    "# Trim sessions to first 20 items \n",
    "groupby_features_nonlist = [x for x in groupby_features.output_columns.names if '-list' not in x]\n",
    "\n",
    "groupby_features_trim = ((groupby_features - groupby_features_nonlist)) >> nvt.ops.ListSlice(0,20) >> nvt.ops.Rename(postfix = '_trim')\n",
    "\n",
    "MINIMUM_SESSION_LENGTH = 2\n",
    "\n",
    "selected_features = groupby_features[groupby_features_nonlist] + groupby_features_trim\n",
    "\n",
    "filtered_sessions = (selected_features) >> nvt.ops.Filter(f=lambda df: df[\"item_id-count\"] >= MINIMUM_SESSION_LENGTH)\n",
    "\n",
    "workflow = nvt.Workflow(filtered_sessions)\n",
    "dataset = nvt.Dataset(random_data, cpu=False)\n",
    "workflow.fit(dataset)\n",
    "sessions_gdf = workflow.transform(dataset).to_ddf().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97828ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day-first</th>\n",
       "      <th>purchase-first</th>\n",
       "      <th>item_id-count</th>\n",
       "      <th>session_id</th>\n",
       "      <th>item_id-list_trim</th>\n",
       "      <th>timestamp/weekday/sin-list_trim</th>\n",
       "      <th>timestamp/age_days-list_trim</th>\n",
       "      <th>category-list_trim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>70000</td>\n",
       "      <td>[42968, 20258, 9094, 21342, 25064, 12432]</td>\n",
       "      <td>[0.8168488520400191, 0.6686634528158085, 0.326...</td>\n",
       "      <td>[0.4865734630505416, 0.7622569743055907, 0.029...</td>\n",
       "      <td>[292, 22, 252, 0, 202, 266]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>70001</td>\n",
       "      <td>[10364, 21399, 8586, 13012, 24951, 4886, 7856]</td>\n",
       "      <td>[0.20090628414166323, 0.06770251893493051, 0.6...</td>\n",
       "      <td>[0.6204740594272689, 0.7052139557453468, 0.903...</td>\n",
       "      <td>[42, 326, 251, 58, 185, 74, 150]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>70002</td>\n",
       "      <td>[11019, 1448, 42300, 10052, 24214, 34642, 2686...</td>\n",
       "      <td>[0.9203113024778837, 0.8309483352776478, 0.237...</td>\n",
       "      <td>[0.30113824260730027, 0.05405530256120705, 0.6...</td>\n",
       "      <td>[62, 41, 31, 247, 232, 271, 298, 219, 210, 205...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day-first  purchase-first  item_id-count  session_id  \\\n",
       "0          3               0              6       70000   \n",
       "1          1               0              7       70001   \n",
       "2          4               1             12       70002   \n",
       "\n",
       "                                   item_id-list_trim  \\\n",
       "0          [42968, 20258, 9094, 21342, 25064, 12432]   \n",
       "1     [10364, 21399, 8586, 13012, 24951, 4886, 7856]   \n",
       "2  [11019, 1448, 42300, 10052, 24214, 34642, 2686...   \n",
       "\n",
       "                     timestamp/weekday/sin-list_trim  \\\n",
       "0  [0.8168488520400191, 0.6686634528158085, 0.326...   \n",
       "1  [0.20090628414166323, 0.06770251893493051, 0.6...   \n",
       "2  [0.9203113024778837, 0.8309483352776478, 0.237...   \n",
       "\n",
       "                        timestamp/age_days-list_trim  \\\n",
       "0  [0.4865734630505416, 0.7622569743055907, 0.029...   \n",
       "1  [0.6204740594272689, 0.7052139557453468, 0.903...   \n",
       "2  [0.30113824260730027, 0.05405530256120705, 0.6...   \n",
       "\n",
       "                                  category-list_trim  \n",
       "0                        [292, 22, 252, 0, 202, 266]  \n",
       "1                   [42, 326, 251, 58, 185, 74, 150]  \n",
       "2  [62, 41, 31, 247, 232, 271, 298, 219, 210, 205...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions_gdf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84275c98",
   "metadata": {},
   "source": [
    "- We can save the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a75dc6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.save('workflow_etl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09dce11",
   "metadata": {},
   "source": [
    "### Export pre-processed data by day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51f99543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating time-based splits: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 10.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# requires cudf + cupy + nvtabular + dask_cudf\n",
    "from transformers4rec.utils.gpu_preprocessing import save_time_based_splits\n",
    "save_time_based_splits(data=nvt.Dataset(sessions_gdf),\n",
    "                       output_dir= \"./preproc_sessions_by_day\",\n",
    "                       partition_col='day-first',\n",
    "                       timestamp_col='session_id', \n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b1660",
   "metadata": {},
   "source": [
    "## Build a DL model with Transformers4Rec library  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a685e",
   "metadata": {},
   "source": [
    "- import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "832d97dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import transformers4rec.torch as torch4rec\n",
    "from transformers4rec.torch import TabularSequenceFeatures, MLPBlock, SequentialBlock, Head, TransformerBlock\n",
    "\n",
    "from transformers4rec.utils.schema import DatasetSchema\n",
    "from transformers4rec.torch.model.head import NextItemPredictionTask\n",
    "from transformers4rec.config import transformer\n",
    "from transformers4rec.torch.ranking_metric import NDCGAt, AvgPrecisionAt, RecallAt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60939e9",
   "metadata": {},
   "source": [
    "- Manually set the schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6d99d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema object to pass it to the SequentialTabularFeatures\n",
    "schema = DatasetSchema.from_proto(\"schema.pb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe00ed4",
   "metadata": {},
   "source": [
    "### Define the sequential input module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca6ca3",
   "metadata": {},
   "source": [
    "Below we define our `input` bloc using [`SequentialTabularFeatures` class](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/features/sequential.py). The `from_schema` module directly parse schema and accepts categorical and continuous sequential inputs and supports data augmentation, data aggregation, `sequential-concat` and `elementwise-sum` aggregations, the projection of the interaction embeddings and the masking tasks.\n",
    "\n",
    "`max_sequence_length` defines the maximum sequence length of our sequential input, and if `continuous_projection` argument is set,  all numerical features are concatenated and projected by a number of MLP layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4b09c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = TabularSequenceFeatures.from_schema(\n",
    "        schema,\n",
    "        max_sequence_length=20,\n",
    "        continuous_projection=64,\n",
    "        d_output=100,\n",
    "        masking=\"causal\",\n",
    "    )\n",
    "inputs.masking.device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c08526d",
   "metadata": {},
   "source": [
    "- LM task + HF Transformer architecture + Next item-prediction task. \n",
    "- We build a [T4RecConfig](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/config/transformer.py#L8) class to update the config class of the transformer architecture with the specified arguments, then load the related model. Here we use it to instantiate an XLNET model according to the  arguments (d_model, n_head, etc.), defining the model architecture.\n",
    "- [TransformerBlock](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/block/transformer.py#L37) class is created to support HF Transformers for session-based and sequential-based recommendation models.\n",
    "- [NextItemPredictionTask](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/head.py#L212) is the class to support next item prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a95ab791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XLNetConfig class and set default parameters \n",
    "\n",
    "# Set HF config of XLNet \n",
    "transformer_config = transformer.XLNetConfig.build(\n",
    "    d_model=64, n_head=4, n_layer=2, total_seq_length=20\n",
    ")\n",
    "# Define the model block including: inputs, masking, projection and transformer block.\n",
    "body = torch4rec.SequentialBlock(\n",
    "    inputs, torch4rec.MLPBlock([64]), torch4rec.TransformerBlock(transformer_config, masking=inputs.masking)\n",
    ")\n",
    "\n",
    "# Define the head related to next item prediction task \n",
    "head = torch4rec.Head(\n",
    "    body,\n",
    "    torch4rec.NextItemPredictionTask(weight_tying=True, hf_format=True),\n",
    "    inputs=inputs,\n",
    ")\n",
    "\n",
    "# Get the end-to-end Model class \n",
    "model = torch4rec.Model(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4564c",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2756286",
   "metadata": {},
   "source": [
    "- **Set Training arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0b62311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers4rec.config.trainer import T4RecTrainingArguments\n",
    "from transformers4rec.torch import Trainer\n",
    "#Set argumentd for training \n",
    "train_args = T4RecTrainingArguments(local_rank = -1, dataloader_drop_last = True, data_loader_engine='nvtabular',\n",
    "                                  report_to = [], debug = [\"r\"], gradient_accumulation_steps = 32,\n",
    "                                  per_device_train_batch_size = 256, per_device_eval_batch_size = 32,\n",
    "                                  output_dir = \".\", lr_scheduler_type='cosine', \n",
    "                                  learning_rate_num_cosine_cycles_by_epoch=1.5,\n",
    "                                  max_sequence_length=20, fp16=False, no_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6968b7c2",
   "metadata": {},
   "source": [
    "- **Define paths to train and eval data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9015eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "train_data_paths = glob.glob(\"./preproc_sessions_by_day/*/train.parquet\")[:-1]\n",
    "eval_data_paths = glob.glob(\"./preproc_sessions_by_day/*/valid.parquet\")[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472d9673",
   "metadata": {},
   "source": [
    "- **Define Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d7dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the T4Rec Trainer, which manages training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    schema=schema,\n",
    "    compute_metrics=True,\n",
    "    train_dataset_or_path=train_data_paths,\n",
    "    eval_dataset_or_path=eval_data_paths,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94b0d0",
   "metadata": {},
   "source": [
    "- **Train the model**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d67dee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8704\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32768\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:02, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=11.425968170166016, metrics={'train_runtime': 3.9482, 'train_samples_per_second': 6.079, 'train_steps_per_second': 0.76, 'total_flos': 0.0, 'train_loss': 11.425968170166016, 'epoch': 2.94})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.reset_lr_scheduler()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe8376",
   "metadata": {},
   "source": [
    "- **Compute evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fc66bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch = 2.94\n",
      "  eval_avgprecisionat_10 = 0.0\n",
      "  eval_avgprecisionat_20 = 6.760639735148288e-06\n",
      "  eval_loss = 10.928376197814941\n",
      "  eval_ndcgat_10 = 0.0\n",
      "  eval_ndcgat_20 = 2.9721029932261445e-05\n",
      "  eval_recallat_10 = 0.0\n",
      "  eval_recallat_20 = 0.00012845215678680688\n",
      "  eval_runtime = 1.391\n",
      "  eval_samples_per_second = 621.143\n",
      "  eval_steps_per_second = 5.032\n"
     ]
    }
   ],
   "source": [
    "eval_metrics = trainer.evaluate(eval_dataset=eval_data_paths, metric_key_prefix='eval')\n",
    "for key in sorted(eval_metrics.keys()):\n",
    "    print(\"  %s = %s\" % (key, str(eval_metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c62893",
   "metadata": {},
   "source": [
    "- **Compute Train metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78a5b8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch = 2.94\n",
      "  train_avgprecisionat_10 = 0.0\n",
      "  train_avgprecisionat_20 = 9.212599252350628e-06\n",
      "  train_loss = 10.931198120117188\n",
      "  train_ndcgat_10 = 0.0\n",
      "  train_ndcgat_20 = 4.050030111102387e-05\n",
      "  train_recallat_10 = 0.0\n",
      "  train_recallat_20 = 0.00017503938579466194\n",
      "  train_runtime = 1.0375\n",
      "  train_samples_per_second = 616.867\n",
      "  train_steps_per_second = 4.819\n"
     ]
    }
   ],
   "source": [
    "train_metrics = trainer.evaluate(eval_dataset=train_data_paths, metric_key_prefix='train')\n",
    "for key in sorted(train_metrics.keys()):\n",
    "    print(\"  %s = %s\" % (key, str(train_metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0707e0f",
   "metadata": {},
   "source": [
    "* **Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dddd27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-3\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"
     ]
    }
   ],
   "source": [
    "trainer._save_model_and_checkpoint(save_model_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a571a2d",
   "metadata": {},
   "source": [
    "* **Reload model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8db43b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_model_trainer_states_from_checkpoint('./checkpoint-%s'%trainer.state.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4434acb5",
   "metadata": {},
   "source": [
    "- **Re-compute eval metrics of train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6f32c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch = 2.94\n",
      "  train_avgprecisionat_10 = 0.0\n",
      "  train_avgprecisionat_20 = 9.212599252350628e-06\n",
      "  train_loss = 10.931198120117188\n",
      "  train_ndcgat_10 = 0.0\n",
      "  train_ndcgat_20 = 4.050030111102387e-05\n",
      "  train_recallat_10 = 0.0\n",
      "  train_recallat_20 = 0.00017503938579466194\n",
      "  train_runtime = 1.1741\n",
      "  train_samples_per_second = 545.121\n",
      "  train_steps_per_second = 4.259\n"
     ]
    }
   ],
   "source": [
    "train_metrics = trainer.evaluate(eval_dataset=train_data_paths, metric_key_prefix='train')\n",
    "for key in sorted(train_metrics.keys()):\n",
    "    print(\"  %s = %s\" % (key, str(train_metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94070e6a",
   "metadata": {},
   "source": [
    "* **Resume Training** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe223fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ./checkpoint-3).\n",
      "***** Running training *****\n",
      "  Num examples = 1024\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32768\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 3\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 3\n",
      "  Continuing training from global step 3\n",
      "  Will skip the first 3 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21daa37d933422cb0808b18b6366d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 : < :, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.0, metrics={'train_runtime': 0.183, 'train_samples_per_second': 671.957, 'train_steps_per_second': 16.389, 'total_flos': 0.0, 'train_loss': 0.0, 'epoch': 2.94})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset lr scheduler to train on new day data\n",
    "trainer.reset_lr_scheduler()\n",
    "# set new data from last day\n",
    "trainer.train_dataset = train_data_paths[-1]\n",
    "trainer.train(resume_from_checkpoint='./checkpoint-%s'%trainer.state.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98566ad",
   "metadata": {},
   "source": [
    "- **Evaluate on last day**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e02ef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch = 2.94\n",
      "  eval_avgprecisionat_10 = 0.0\n",
      "  eval_avgprecisionat_20 = 6.760639735148288e-06\n",
      "  eval_loss = 10.928376197814941\n",
      "  eval_ndcgat_10 = 0.0\n",
      "  eval_ndcgat_20 = 2.9721029932261445e-05\n",
      "  eval_recallat_10 = 0.0\n",
      "  eval_recallat_20 = 0.00012845215678680688\n",
      "  eval_runtime = 2.0212\n",
      "  eval_samples_per_second = 427.467\n",
      "  eval_steps_per_second = 3.463\n"
     ]
    }
   ],
   "source": [
    "# set new data from last day\n",
    "eval_metrics = trainer.evaluate(eval_dataset=eval_data_paths[-1], metric_key_prefix='eval')\n",
    "for key in sorted(eval_metrics.keys()):\n",
    "    print(\"  %s = %s\" % (key, str(eval_metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a65f4",
   "metadata": {},
   "source": [
    "# Daily Fine-Tuning: Training over a time window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46915dc7",
   "metadata": {},
   "source": [
    "Here we do daily fine-tuning meaning that we use the first day to train and second day to evaluate, then we use the second day data to train the model by resuming from the first step, and evaluate on the third day, so on so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73b3e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the T4Rec Trainer, which manages training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    schema=schema,\n",
    "    compute_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e77dd472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1024\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32768\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training for day 1 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1024\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32768\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 2 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " epoch = 3.0\n",
      " eval_avgprecisionat_10 = 0.0\n",
      " eval_avgprecisionat_20 = 0.0\n",
      " eval_loss = 10.92973804473877\n",
      " eval_ndcgat_10 = 0.0\n",
      " eval_ndcgat_20 = 0.0\n",
      " eval_recallat_10 = 0.0\n",
      " eval_recallat_20 = 0.0\n",
      " eval_runtime = 0.2179\n",
      " eval_samples_per_second = 440.569\n",
      " eval_steps_per_second = 4.589\n",
      "\n",
      "********************\n",
      "Launch training for day 2 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 1024\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32768\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 3 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " epoch = 3.0\n",
      " eval_avgprecisionat_10 = 0.0\n",
      " eval_avgprecisionat_20 = 0.0\n",
      " eval_loss = 10.908461570739746\n",
      " eval_ndcgat_10 = 0.0\n",
      " eval_ndcgat_20 = 0.0\n",
      " eval_recallat_10 = 0.0\n",
      " eval_recallat_20 = 0.0\n",
      " eval_runtime = 0.2072\n",
      " eval_samples_per_second = 463.364\n",
      " eval_steps_per_second = 4.827\n",
      "********************\n",
      "Launch training for day 3 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 1024\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32768\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 4 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " epoch = 3.0\n",
      " eval_avgprecisionat_10 = 0.0\n",
      " eval_avgprecisionat_20 = 0.0\n",
      " eval_loss = 10.920406341552734\n",
      " eval_ndcgat_10 = 0.0\n",
      " eval_ndcgat_20 = 0.0\n",
      " eval_recallat_10 = 0.0\n",
      " eval_recallat_20 = 0.0\n",
      " eval_runtime = 0.2059\n",
      " eval_samples_per_second = 466.183\n",
      " eval_steps_per_second = 4.856\n",
      "********************\n",
      "Launch training for day 4 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 1024\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32768\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 5 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " epoch = 3.0\n",
      " eval_avgprecisionat_10 = 0.0\n",
      " eval_avgprecisionat_20 = 0.0\n",
      " eval_loss = 10.95324420928955\n",
      " eval_ndcgat_10 = 0.0\n",
      " eval_ndcgat_20 = 0.0\n",
      " eval_recallat_10 = 0.0\n",
      " eval_recallat_20 = 0.0\n",
      " eval_runtime = 0.2007\n",
      " eval_samples_per_second = 478.245\n",
      " eval_steps_per_second = 4.982\n",
      "********************\n",
      "Launch training for day 5 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 1024\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32768\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 6 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " epoch = 3.0\n",
      " eval_avgprecisionat_10 = 0.00011185682524228469\n",
      " eval_avgprecisionat_20 = 0.00011185682524228469\n",
      " eval_loss = 10.917628288269043\n",
      " eval_ndcgat_10 = 0.00032333872513845563\n",
      " eval_ndcgat_20 = 0.00032333872513845563\n",
      " eval_recallat_10 = 0.0011185682378709316\n",
      " eval_recallat_20 = 0.0011185682378709316\n",
      " eval_runtime = 0.2146\n",
      " eval_samples_per_second = 447.411\n",
      " eval_steps_per_second = 4.661\n",
      "********************\n",
      "Launch training for day 6 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 7 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " epoch = 3.0\n",
      " eval_avgprecisionat_10 = 0.0\n",
      " eval_avgprecisionat_20 = 0.0\n",
      " eval_loss = 10.940314292907715\n",
      " eval_ndcgat_10 = 0.0\n",
      " eval_ndcgat_20 = 0.0\n",
      " eval_recallat_10 = 0.0\n",
      " eval_recallat_20 = 0.0\n",
      " eval_runtime = 0.2051\n",
      " eval_samples_per_second = 468.02\n",
      " eval_steps_per_second = 4.875\n"
     ]
    }
   ],
   "source": [
    "start_time_window_index = 1\n",
    "final_time_window_index = 7\n",
    "for time_index in range(start_time_window_index, final_time_window_index):\n",
    "    # Set data \n",
    "    time_index_train = time_index\n",
    "    time_index_eval = time_index + 1\n",
    "    train_paths = glob.glob(f\"./preproc_sessions_by_day/{time_index_train}/train.parquet\")\n",
    "    eval_paths = glob.glob(f\"./preproc_sessions_by_day/{time_index_eval}/valid.parquet\")  \n",
    "    \n",
    "    # Train on day related to time_index \n",
    "    print('*'*20)\n",
    "    print(\"Launch training for day %s are:\" %time_index)\n",
    "    print('*'*20 + '\\n')\n",
    "    trainer.train_dataset = train_paths\n",
    "    trainer.reset_lr_scheduler()\n",
    "    trainer.train()\n",
    "    trainer.state.global_step +=1\n",
    "    # Evaluate on the following day\n",
    "    trainer.eval_dataset = eval_paths\n",
    "    train_metrics = trainer.evaluate(metric_key_prefix='eval')\n",
    "    print('*'*20)\n",
    "    print(\"Eval results for day %s are:\\t\" %time_index_eval)\n",
    "    print('\\n' + '*'*20 + '\\n')\n",
    "    for key in sorted(train_metrics.keys()):\n",
    "        print(\" %s = %s\" % (key, str(train_metrics[key]))) \n",
    "    trainer.wipe_memory()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b543a88d374ac88bf8df97911b380f671b13649694a5b49eb21e60fd27eb479"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
