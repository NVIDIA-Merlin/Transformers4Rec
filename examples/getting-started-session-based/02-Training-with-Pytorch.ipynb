{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b845d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e521d",
   "metadata": {},
   "source": [
    "# Session-based Recommendation with XLNET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02640a2",
   "metadata": {},
   "source": [
    "In this notebook we use [Transformers4Rec](https://github.com/NVIDIA-Merlin/Transformers4Rec) which leverages the popular [HuggingFaceâ€™s Transformers](https://github.com/huggingface/transformers) and make it possible to experiment with cutting-edge implementation of such architectures for sequential and session-based recommendation problems. We show how to build a session-based recommendation model with [XLNET](https://arxiv.org/abs/1906.08237), and train and evaluate it with optimized [NVTabular Pytorch dataloader](https://nvidia.github.io/NVTabular/main/training/pytorch.html). The XLNet architecture was designed to leverage the best of both auto-regressive language modeling and auto-encoding with its Permutation Language Modeling training method. In this example we will use XLNET with masked language modeling (MLM) training method, which showed very promising results in the experiments conducted in our [ACM RecSys'21 paper](https://github.com/NVIDIA-Merlin/publications/blob/main/2021_acm_recsys_transformers4rec/recsys21_transformers4rec_paper.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad7aca",
   "metadata": {},
   "source": [
    "In the previous notebook we went through our ETL pipeline with NVTabular library, and created session-based features to be used in training a session-based recommendation model. In this notebook we will learn:\n",
    "\n",
    "- Accelerating data loading of multiple features on PyTorch using NVTabular library\n",
    "- Training and evaluating a Transformer-based (XLNET-MLM) session-based recommendation model with side (additional features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec87bb5",
   "metadata": {},
   "source": [
    "## Build a DL model with Transformers4Rec library  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae86f09",
   "metadata": {},
   "source": [
    "- import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d777b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch \n",
    "\n",
    "import transformers4rec.torch as torch4rec\n",
    "from transformers4rec.torch import TabularSequenceFeatures, MLPBlock, SequentialBlock, Head, TransformerBlock\n",
    "\n",
    "from transformers4rec.utils.schema import DatasetSchema\n",
    "from transformers4rec.torch.model.head import NextItemPredictionTask\n",
    "from transformers4rec.config import transformer\n",
    "from transformers4rec.torch.ranking_metric import NDCGAt, AvgPrecisionAt, RecallAt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a8338",
   "metadata": {},
   "source": [
    "Transformers4Rec library relies on schema object in creation of TabularSequence features. As you can see below schema.pb is a protobuf file contains metadata including statistics about features such as cardinality, min and max values and also tags each feature based on their characteristics and dtypes (e.g., categorical, continuous, list, integer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c5a0b",
   "metadata": {},
   "source": [
    "- Manually set the schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a9d980b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"session_id\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"session_id\"\n",
      "    min: 1\n",
      "    max: 11562158\n",
      "    is_categorical: false\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"groupby_col\"\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"category-list_trim\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 20\n",
      "  }\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"category-list_trim\"\n",
      "    min: 1\n",
      "    max: 334\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"list\"\n",
      "    tag: \"categorical\"\n",
      "    tag: \"item\"\n"
     ]
    }
   ],
   "source": [
    "# Define schema object to pass it to the SequentialTabularFeatures\n",
    "SCHEMA_PATH = \"./schema.pb\"\n",
    "schema = DatasetSchema.from_proto(SCHEMA_PATH)\n",
    "!head -30 $SCHEMA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b67b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the features for trainining\n",
    "schema = schema.select_by_name(['item_id-list_trim', 'category-list_trim', 'timestamp/weekday/sin-list_trim','timestamp/age_days-list_trim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a6797",
   "metadata": {},
   "source": [
    "### Define the sequential input module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb982f",
   "metadata": {},
   "source": [
    "Below we define our `input` block using `TabularSequenceFeatures` [class](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/features/sequence.py#L91). The `from_schema` module directly parse schema and accepts categorical and continuous sequential inputs and supports data augmentation, data aggregation with `sequential-concat`, `elementwise-sum` and `lement-wise sum & item multiplication ` techniques, the projection of the interaction embeddings and the masking tasks.\n",
    "\n",
    "`max_sequence_length` defines the maximum sequence length of our sequential input, and if `continuous_projection` argument is set,  all numerical features are concatenated and projected with an MLP block so that continuous features are represented by a vector of size defined by user, which is `64` in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9334442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = TabularSequenceFeatures.from_schema(\n",
    "        schema,\n",
    "        max_sequence_length=20,\n",
    "        continuous_projection=64,\n",
    "        d_output=100,\n",
    "        masking=\"mlm\",\n",
    "    )\n",
    "inputs.masking.device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3942de24",
   "metadata": {},
   "source": [
    "### Define the Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04dca42",
   "metadata": {},
   "source": [
    "- LM task + HF Transformer architecture + Next item-prediction task. \n",
    "    - We build a [T4RecConfig](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/config/transformer.py#L8) class to update the config class of the transformer architecture with the specified arguments, then load the related model. Here we use it to instantiate an XLNET model according to the  arguments (d_model, n_head, etc.), defining the model architecture.\n",
    "    - [TransformerBlock](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/block/transformer.py#L37) class is created to support HF Transformers for session-based and sequential-based recommendation models.\n",
    "    - [NextItemPredictionTask](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/model/head.py#L238) is the class to support next item prediction task. We also support other predictions [tasks](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/model/head.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59413ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XLNetConfig class and set default parameters for HF XLNet config  \n",
    "transformer_config = transformer.XLNetConfig.build(\n",
    "    d_model=64, n_head=4, n_layer=2, total_seq_length=20\n",
    ")\n",
    "# Define the model block including: inputs, masking, projection and transformer block.\n",
    "body = torch4rec.SequentialBlock(\n",
    "    inputs, torch4rec.MLPBlock([64]), torch4rec.TransformerBlock(transformer_config, masking=inputs.masking)\n",
    ")\n",
    "\n",
    "# Define the head related to next item prediction task \n",
    "head = torch4rec.Head(\n",
    "    body,\n",
    "    torch4rec.NextItemPredictionTask(weight_tying=True, hf_format=True, \n",
    "                                     metrics=[NDCGAt(top_ks=[100, 200], labels_onehot=True),  RecallAt(top_ks=[100, 200], labels_onehot=True)]),\n",
    "    inputs=inputs,\n",
    ")\n",
    "\n",
    "# Get the end-to-end Model class \n",
    "model = torch4rec.Model(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46baad8b",
   "metadata": {},
   "source": [
    "Note that we can easily define an RNN-based model inside the `SequentialBlock` instead of a Transformer-based model. You can explore this [tutorial](https://github.com/NVIDIA-Merlin/Transformers4Rec/tree/main/tutorial) for a GRU-based model example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e465bc1",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3da33d6",
   "metadata": {},
   "source": [
    "As data loader we use optimized NVTabular PyTorch Dataloader. You can learn more about it [here](https://nvidia.github.io/NVTabular/main/training/pytorch.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7759b4",
   "metadata": {},
   "source": [
    "- **Set Training arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f6e27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers4rec.config.trainer import T4RecTrainingArguments\n",
    "from transformers4rec.torch import Trainer\n",
    "#Set argumentd for training \n",
    "train_args = T4RecTrainingArguments(local_rank = -1, dataloader_drop_last = True, data_loader_engine='nvtabular',\n",
    "                                  report_to = [], debug = [\"r\"], gradient_accumulation_steps = 1,\n",
    "                                  per_device_train_batch_size = 256, per_device_eval_batch_size = 32,\n",
    "                                  output_dir = \".\", lr_scheduler_type='cosine', \n",
    "                                  learning_rate_num_cosine_cycles_by_epoch=1.5,\n",
    "                                  max_sequence_length=20, fp16=False, no_cuda=False, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b231cd6",
   "metadata": {},
   "source": [
    "# Daily Fine-Tuning: Training over a time window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e6197d",
   "metadata": {},
   "source": [
    "Here we do daily fine-tuning meaning that we use the first day to train and second day to evaluate, then we use the second day data to train the model by resuming from the first step, and evaluate on the third day, so on so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8477693f",
   "metadata": {},
   "source": [
    "In this example, the evaluation of the session-based recommendation model is performed using traditional Top-N ranking metrics such as Normalized Discounted Cumulative Gain (NDCG@20) and Hit Rate (HR@20). NDCG accounts for rank of the relevant item in the recommendation list and is a more fine-grained metric than HR, which only verifies whether the relevant item is among the top-n items. HR@n is equivalent to Recall@n when there is only one relevant item in the recommendation list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26f0ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the T4Rec Trainer, which manages training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    schema=schema,\n",
    "    compute_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "382b11b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/workspace/data/sessions_by_day/1/train.parquet']\n",
      "********************\n",
      "Launch training for day 1 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1024\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1024\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1024\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1024\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 2 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " epoch = 3.0\n",
      " eval_loss = 10.860979080200195\n",
      " eval_ndcgat_100 = 0.0\n",
      " eval_ndcgat_200 = 0.0\n",
      " eval_recallat_100 = 0.0\n",
      " eval_recallat_200 = 0.0\n",
      " eval_runtime = 0.0746\n",
      " eval_samples_per_second = 857.947\n",
      " eval_steps_per_second = 13.405\n",
      "['/workspace/data/sessions_by_day/2/train.parquet']\n",
      "********************\n",
      "Launch training for day 2 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 1024\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1024\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 3 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " epoch = 3.0\n",
      " eval_loss = 10.81531047821045\n",
      " eval_ndcgat_100 = 0.0\n",
      " eval_ndcgat_200 = 0.0005674263811670244\n",
      " eval_recallat_100 = 0.0\n",
      " eval_recallat_200 = 0.004273504484444857\n",
      " eval_runtime = 0.092\n",
      " eval_samples_per_second = 1043.754\n",
      " eval_steps_per_second = 10.872\n",
      "['/workspace/data/sessions_by_day/3/train.parquet']\n",
      "********************\n",
      "Launch training for day 3 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 1024\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1024\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 4 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " epoch = 3.0\n",
      " eval_loss = 10.836214065551758\n",
      " eval_ndcgat_100 = 0.0\n",
      " eval_ndcgat_200 = 0.0\n",
      " eval_recallat_100 = 0.0\n",
      " eval_recallat_200 = 0.0\n",
      " eval_runtime = 0.0746\n",
      " eval_samples_per_second = 858.235\n",
      " eval_steps_per_second = 13.41\n",
      "['/workspace/data/sessions_by_day/4/train.parquet']\n",
      "********************\n",
      "Launch training for day 4 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 1024\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1024\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 5 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " epoch = 3.0\n",
      " eval_loss = 10.765353202819824\n",
      " eval_ndcgat_100 = 0.0\n",
      " eval_ndcgat_200 = 0.0\n",
      " eval_recallat_100 = 0.0\n",
      " eval_recallat_200 = 0.0\n",
      " eval_runtime = 0.0982\n",
      " eval_samples_per_second = 977.112\n",
      " eval_steps_per_second = 10.178\n",
      "['/workspace/data/sessions_by_day/5/train.parquet']\n",
      "********************\n",
      "Launch training for day 5 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 1024\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1024\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 6 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " epoch = 3.0\n",
      " eval_loss = 10.712799072265625\n",
      " eval_ndcgat_100 = 0.0\n",
      " eval_ndcgat_200 = 0.0\n",
      " eval_recallat_100 = 0.0\n",
      " eval_recallat_200 = 0.0\n",
      " eval_runtime = 0.0987\n",
      " eval_samples_per_second = 972.559\n",
      " eval_steps_per_second = 10.131\n",
      "['/workspace/data/sessions_by_day/6/train.parquet']\n",
      "********************\n",
      "Launch training for day 6 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 7 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " epoch = 3.0\n",
      " eval_loss = 10.695266723632812\n",
      " eval_ndcgat_100 = 0.0\n",
      " eval_ndcgat_200 = 0.00139007403049618\n",
      " eval_recallat_100 = 0.0\n",
      " eval_recallat_200 = 0.009389671497046947\n",
      " eval_runtime = 0.0979\n",
      " eval_samples_per_second = 980.18\n",
      " eval_steps_per_second = 10.21\n"
     ]
    }
   ],
   "source": [
    "start_time_window_index = 1\n",
    "final_time_window_index = 7\n",
    "for time_index in range(start_time_window_index, final_time_window_index):\n",
    "    # Set data \n",
    "    time_index_train = time_index\n",
    "    time_index_eval = time_index + 1\n",
    "    train_paths = glob.glob(f\"/workspace/data/sessions_by_day/{time_index_train}/train.parquet\")\n",
    "    eval_paths = glob.glob(f\"/workspace/data/sessions_by_day/{time_index_eval}/valid.parquet\")  \n",
    "    print(train_paths)\n",
    "    # Train on day related to time_index \n",
    "    print('*'*20)\n",
    "    print(\"Launch training for day %s are:\" %time_index)\n",
    "    print('*'*20 + '\\n')\n",
    "    trainer.train_dataset_or_path = train_paths\n",
    "    trainer.reset_lr_scheduler()\n",
    "    trainer.train()\n",
    "    trainer.state.global_step +=1\n",
    "    # Evaluate on the following day\n",
    "    trainer.eval_dataset_or_path = eval_paths\n",
    "    train_metrics = trainer.evaluate(metric_key_prefix='eval')\n",
    "    print('*'*20)\n",
    "    print(\"Eval results for day %s are:\\t\" %time_index_eval)\n",
    "    print('\\n' + '*'*20 + '\\n')\n",
    "    for key in sorted(train_metrics.keys()):\n",
    "        print(\" %s = %s\" % (key, str(train_metrics[key]))) \n",
    "    trainer.wipe_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89048e2",
   "metadata": {},
   "source": [
    "* **Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49515c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-13\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"
     ]
    }
   ],
   "source": [
    "trainer._save_model_and_checkpoint(save_model_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906c215",
   "metadata": {},
   "source": [
    "* **Reload the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "980c5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_model_trainer_states_from_checkpoint('./checkpoint-%s'%trainer.state.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20de4b7",
   "metadata": {},
   "source": [
    "- **Re-compute eval metrics of validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0566342",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_paths = glob.glob(f\"/workspace/data/sessions_by_day/7/valid.parquet\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d3559b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch = 3.0\n",
      "  eval_loss = 10.681629180908203\n",
      "  eval_ndcgat_100 = 0.0\n",
      "  eval_ndcgat_200 = 0.0012942211469635367\n",
      "  eval_recallat_100 = 0.0\n",
      "  eval_recallat_200 = 0.008733624592423439\n",
      "  eval_runtime = 0.1121\n",
      "  eval_samples_per_second = 856.627\n",
      "  eval_steps_per_second = 8.923\n"
     ]
    }
   ],
   "source": [
    "# set new data from day 7\n",
    "eval_metrics = trainer.evaluate(eval_dataset=eval_data_paths, metric_key_prefix='eval')\n",
    "for key in sorted(eval_metrics.keys()):\n",
    "    print(\"  %s = %s\" % (key, str(eval_metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6587287f",
   "metadata": {},
   "source": [
    "We can easily log and visualize model training and evaluation on [Weights & Biases (W&B)](https://wandb.ai/home), [Tensorboard](https://www.tensorflow.org/tensorboard) and [NVIDIA DLLogger](https://github.com/NVIDIA/dllogger). By default, Huggingface uses Weights & Biases (W&B) to log training and evaluation metrics. It allows a nice management of experiments, including config logging, and provides plots with the evolution of losses and metrics over time. Please visit [here](https://github.com/NVIDIA-Merlin/Transformers4Rec/tree/main/transformers4rec) for instructions on using  Weights & Biases."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b543a88d374ac88bf8df97911b380f671b13649694a5b49eb21e60fd27eb479"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
