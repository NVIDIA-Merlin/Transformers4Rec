{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97250792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2228da",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com//notebooks/dlsw-notebooks/remtting-started-session-based-03-serving-session-based-model-torch-backend/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Serving a Session-based Recommendation model with Torch Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127de28-a7ce-4ff7-8dcc-1575a70ca7c8",
   "metadata": {},
   "source": [
    "This notebook is created using the latest stable [merlin-pytorch](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-pytorch/tags) container.\n",
    "\n",
    "At this point, when you reach out to this notebook, we expect that you have already executed the `01-ETL-with-NVTabular.ipynb` and `02-session-based-XLNet-with-PyT.ipynb` notebooks, and saved the NVT workflow and the trained session-based model.\n",
    "\n",
    "In this notebook, you are going to learn how you can serve a trained Transformer-based PyTorch model on NVIDIA [Triton Inference Server](https://github.com/triton-inference-server/server)  (TIS) with Torch backend using [Merlin systems](https://github.com/NVIDIA-Merlin/systems) library. One common way to do inference with a trained model is to use TorchScript, an intermediate representation of a PyTorch model that can be run in Python as well as in a high performance environment like C++. [TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html) is actually the recommended model format for scaled inference and deployment. TIS [PyTorch (LibTorch) backend](https://github.com/triton-inference-server/pytorch_backend) is designed to run TorchScript models using the PyTorch C++ API.\n",
    "\n",
    "[Triton Inference Server](https://github.com/triton-inference-server/server) (TIS) simplifies the deployment of AI models at scale in production. TIS provides a cloud and edge inferencing solution optimized for both CPUs and GPUs. It supports a number of different machine learning frameworks such as TensorFlow and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599efc90",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba89970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/usr/local/lib/python3.8/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (NDCGAt). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "/usr/local/lib/python3.8/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (DCGAt). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "/usr/local/lib/python3.8/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (AvgPrecisionAt). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "/usr/local/lib/python3.8/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (PrecisionAt). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "/usr/local/lib/python3.8/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (RecallAt). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import cudf\n",
    "import glob\n",
    "import torch \n",
    "\n",
    "from transformers4rec import torch as tr\n",
    "from merlin.io import Dataset\n",
    "\n",
    "from merlin.core.dispatch import make_df  # noqa\n",
    "from merlin.systems.dag import Ensemble  # noqa\n",
    "from merlin.systems.dag.ops.pytorch import PredictPyTorch  # noqa\n",
    "from merlin.systems.dag.ops.workflow import TransformWorkflow\n",
    "from merlin.systems.triton.utils import run_ensemble_on_tritonserver  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2aebe1-66a9-4c71-9bbc-1874625bc4e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "We define the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bc4a16b-32be-42b7-9b6f-e4f4cde3f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", \"/workspace/data\")\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", f\"{INPUT_DATA_DIR}/sessions_by_day\")\n",
    "model_path= os.environ.get(\"OUTPUT_DIR\", f\"{INPUT_DATA_DIR}/saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510b6ef",
   "metadata": {},
   "source": [
    "### Set the schema object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a0518a-eb01-4ac4-9c6d-36b328985765",
   "metadata": {},
   "source": [
    "We create the schema object by reading the `schema.pbtxt` file generated by NVTabular pipeline in the previous, `01-ETL-with-NVTabular`, notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d1299fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin_standard_lib import Schema\n",
    "SCHEMA_PATH = os.environ.get(\"INPUT_SCHEMA_PATH\", \"/workspace/data/processed_nvt/schema.pbtxt\")\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81764b19-4495-45c0-9cb5-6d937962d2bc",
   "metadata": {},
   "source": [
    "We need to load the saved model to be able to serve it on TIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bbc34a4-2277-4961-8061-59aadaa5116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "loaded_model = cloudpickle.load(\n",
    "                open(os.path.join(model_path, \"t4rec_model_class.pkl\"), \"rb\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ab3c7-a576-4fa0-a6f3-fe2bd11effc9",
   "metadata": {},
   "source": [
    "Switch the model to eval mode. We call `model.eval()` before tracing to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this might yield inconsistent inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e516a78d-2e1a-4124-ba46-f60b245d3329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (heads): ModuleList(\n",
       "    (0): Head(\n",
       "      (body): SequentialBlock(\n",
       "        (0): TabularSequenceFeatures(\n",
       "          (to_merge): ModuleDict(\n",
       "            (continuous_module): SequentialBlock(\n",
       "              (0): ContinuousFeatures(\n",
       "                (filter_features): FilterFeatures()\n",
       "                (_aggregation): ConcatFeatures()\n",
       "              )\n",
       "              (1): SequentialBlock(\n",
       "                (0): DenseBlock(\n",
       "                  (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (2): AsTabular()\n",
       "            )\n",
       "            (categorical_module): SequenceEmbeddingFeatures(\n",
       "              (filter_features): FilterFeatures()\n",
       "              (embedding_tables): ModuleDict(\n",
       "                (item_id-list): Embedding(490, 64, padding_idx=0)\n",
       "                (category-list): Embedding(177, 64, padding_idx=0)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (_aggregation): ConcatFeatures()\n",
       "          (projection_module): SequentialBlock(\n",
       "            (0): DenseBlock(\n",
       "              (0): Linear(in_features=192, out_features=100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (_masking): MaskedLanguageModeling()\n",
       "        )\n",
       "        (1): SequentialBlock(\n",
       "          (0): DenseBlock(\n",
       "            (0): Linear(in_features=100, out_features=64, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (2): TansformerBlock(\n",
       "          (transformer): XLNetModel(\n",
       "            (word_embedding): Embedding(1, 64)\n",
       "            (layer): ModuleList(\n",
       "              (0): XLNetLayer(\n",
       "                (rel_attn): XLNetRelativeAttention(\n",
       "                  (layer_norm): LayerNorm((64,), eps=0.03, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (ff): XLNetFeedForward(\n",
       "                  (layer_norm): LayerNorm((64,), eps=0.03, elementwise_affine=True)\n",
       "                  (layer_1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (layer_2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "              (1): XLNetLayer(\n",
       "                (rel_attn): XLNetRelativeAttention(\n",
       "                  (layer_norm): LayerNorm((64,), eps=0.03, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (ff): XLNetFeedForward(\n",
       "                  (layer_norm): LayerNorm((64,), eps=0.03, elementwise_affine=True)\n",
       "                  (layer_1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (layer_2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (masking): MaskedLanguageModeling()\n",
       "        )\n",
       "      )\n",
       "      (prediction_task_dict): ModuleDict(\n",
       "        (next-item): NextItemPredictionTask(\n",
       "          (sequence_summary): SequenceSummary(\n",
       "            (summary): Identity()\n",
       "            (activation): Identity()\n",
       "            (first_dropout): Identity()\n",
       "            (last_dropout): Identity()\n",
       "          )\n",
       "          (metrics): ModuleList(\n",
       "            (0): NDCGAt()\n",
       "            (1): RecallAt()\n",
       "          )\n",
       "          (loss): NLLLoss()\n",
       "          (embeddings): SequenceEmbeddingFeatures(\n",
       "            (filter_features): FilterFeatures()\n",
       "            (embedding_tables): ModuleDict(\n",
       "              (item_id-list): Embedding(490, 64, padding_idx=0)\n",
       "              (category-list): Embedding(177, 64, padding_idx=0)\n",
       "            )\n",
       "          )\n",
       "          (item_embedding_table): Embedding(490, 64, padding_idx=0)\n",
       "          (masking): MaskedLanguageModeling()\n",
       "          (pre): Block(\n",
       "            (module): NextItemPredictionTask(\n",
       "              (item_embedding_table): Embedding(490, 64, padding_idx=0)\n",
       "              (log_softmax): LogSoftmax(dim=-1)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = loaded_model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c17c2-c81d-4f41-9577-bd380ae10921",
   "metadata": {},
   "source": [
    "### Trace the model\n",
    "\n",
    "We serve the model with the PyTorch backend that is used to execute TorchScript models. All models created in PyTorch using the python API must be traced/scripted to produce a TorchScript model. For tracing the model, we use [torch.jit.trace](https://pytorch.org/docs/stable/generated/torch.jit.trace.html) api that takes the model as a Python function or torch.nn.Module, and an example input  that will be passed to the function while tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6273c8e5-db62-4cc6-a4f3-945155a463d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = os.path.join(OUTPUT_DIR, f\"{1}/train.parquet\")\n",
    "dataset = Dataset(train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef5867e4-769a-4af9-aed7-38586a75f18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_max = {'age_days-list': 20,\n",
    " 'weekday_sin-list': 20,\n",
    " 'item_id-list': 20,\n",
    " 'category-list': 20}\n",
    "\n",
    "from transformers4rec.torch.utils.data_utils import MerlinDataLoader\n",
    "\n",
    "def generate_dataloader(schema, dataset, batch_size=128, seq_length=20):\n",
    "    loader = MerlinDataLoader.from_schema(\n",
    "            schema,\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            max_sequence_length=seq_length,\n",
    "            shuffle=False,\n",
    "            sparse_as_dense=True,\n",
    "            sparse_max=sparse_max\n",
    "        )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb4b5f-9b8c-487b-9fa1-cd4b6d3c090f",
   "metadata": {},
   "source": [
    "Create a dict of tensors to feed it as example inputs in the `torch.jit.trace()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1552deb1-1605-4e80-b70f-36f92d64afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = generate_dataloader(schema, dataset)\n",
    "train_dict = next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f18ec0-fabe-494e-a963-8cae5882b9d1",
   "metadata": {},
   "source": [
    "Let's check out the `item_id-list` column in the `train_dict` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46e676f8-0d82-42ec-b27d-04aa2b7c6a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4, 11, 17,  ..., 30,  0,  0],\n",
       "        [36, 49,  9,  ...,  0,  0,  0],\n",
       "        [29, 12, 18,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [21, 23, 40,  ...,  0,  0,  0],\n",
       "        [13, 67, 32,  ...,  0,  0,  0],\n",
       "        [93, 11, 41,  ...,  0,  0,  0]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict['item_id-list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e789a3-2423-44eb-ae5d-0c154557424f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "traced_model = torch.jit.trace(model, train_dict, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404bdb6-6e19-4899-8172-214adef384a8",
   "metadata": {},
   "source": [
    "Generate model input and output schemas to feed in the `PredictPyTorch` operator below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6a814aa-4954-404b-bd2d-161ed8066f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = model.input_schema\n",
    "output_schema = model.output_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "757cd0c5-f581-488b-a8de-b8d1188820d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.int_domain.min</th>\n",
       "      <th>properties.int_domain.max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age_days-list</td>\n",
       "      <td>(Tags.LIST, Tags.CONTINUOUS)</td>\n",
       "      <td>float32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weekday_sin-list</td>\n",
       "      <td>(Tags.LIST, Tags.CONTINUOUS)</td>\n",
       "      <td>float32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>item_id-list</td>\n",
       "      <td>(Tags.ITEM, Tags.CATEGORICAL, Tags.LIST, Tags....</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>category-list</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.LIST)</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'age_days-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CONTINUOUS: 'continuous'>}, 'properties': {'int_domain': {'min': 0, 'max': 0}}, 'dtype': dtype('float32'), 'is_list': True, 'is_ragged': False}, {'name': 'weekday_sin-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CONTINUOUS: 'continuous'>}, 'properties': {'int_domain': {'min': 0, 'max': 0}}, 'dtype': dtype('float32'), 'is_list': True, 'is_ragged': False}, {'name': 'item_id-list', 'tags': {<Tags.ITEM: 'item'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.LIST: 'list'>, <Tags.ID: 'id'>, <Tags.ITEM_ID: 'item_id'>}, 'properties': {'int_domain': {'min': 0, 'max': 489}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': False}, {'name': 'category-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.LIST: 'list'>}, 'properties': {'int_domain': {'min': 0, 'max': 176}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': False}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a8bfc-c6e1-44dd-ab65-cbdab2135e8e",
   "metadata": {},
   "source": [
    "Let's create a folder that we can store the exported models and the config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b2deb2b-e223-4b5d-b655-810e1aefa7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "ens_model_path = os.environ.get(\"ens_model_path\", f\"{INPUT_DATA_DIR}/models\")\n",
    "# Make sure we have a clean stats space for Dask\n",
    "if os.path.isdir(ens_model_path):\n",
    "    shutil.rmtree(ens_model_path)\n",
    "os.mkdir(ens_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3449615-2120-402d-b5c3-1544ee3224dd",
   "metadata": {},
   "source": [
    "We use `PredictPyTorch` operator that takes a pytorch model and packages it correctly for tritonserver to run on the PyTorch backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f96597c-1c05-4fb0-ad3e-c55c21599158",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_op = input_schema.column_names >> PredictPyTorch(\n",
    "    traced_model, input_schema, output_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b0f90-5392-4d7a-9a48-57737cf63cd1",
   "metadata": {},
   "source": [
    "The last step is to create the ensemble artifacts that Triton Inference Server can consume. To make these artifacts, we import the Ensemble class. The class is responsible for interpreting the graph and exporting the correct files for the server.\n",
    "\n",
    "When we create an `Ensemble` object we supply the graph and a schema representing the starting input of the graph. The inputs to the ensemble graph are the inputs to the first operator of out graph. After we created the Ensemble we export the graph, supplying an export path for the `ensemble.export` function. This returns an ensemble config which represents the entire inference pipeline and a list of node-specific configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b0d14bb-7765-45e8-8fd0-9d508dc3ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = Ensemble(torch_op, input_schema)\n",
    "ens_config, node_configs = ensemble.export(ens_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36169a5-f218-44b5-b034-7d299ce718ed",
   "metadata": {},
   "source": [
    "## Starting Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb507766-ac9b-4c8c-8339-c1c951648428",
   "metadata": {},
   "source": [
    "It is time to deploy all the models as an ensemble model to Triton Inference Serve TIS. After we export the ensemble, we are ready to start the TIS. You can start triton server by using the following command on your terminal:\n",
    "\n",
    "`tritonserver --model-repository=<ensemble_export_path>`\n",
    "\n",
    "For the `--model-repository` argument, specify the same path as the export_path that you specified previously in the `ensemble.export` method. This command will launch the server and load all the models to the server. Once all the models are loaded successfully, you should see READY status printed out in the terminal for each loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46a86c8d-9ec1-4422-8f8c-4d49e83f6783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as client\n",
    "\n",
    "# Create a triton client\n",
    "try:\n",
    "    triton_client = client.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab41dd0-0155-4d97-8bdf-2451177d46f1",
   "metadata": {},
   "source": [
    "After we create the client and verified it is connected to the server instance, we can communicate with the server and ensure all the models are loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda3f852-a019-4bf1-831b-f63b750a1192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n",
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '115'}>\n",
      "bytearray(b'[{\"name\":\"0_predictpytorch\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"ensemble_model\",\"version\":\"1\",\"state\":\"READY\"}]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': '0_predictpytorch', 'version': '1', 'state': 'READY'},\n",
       " {'name': 'ensemble_model', 'version': '1', 'state': 'READY'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure triton is in a good state\n",
    "triton_client.is_server_live()\n",
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415146b1-9e9c-4c72-a9b9-697041ba27ef",
   "metadata": {},
   "source": [
    "### Send request to Triton and get the response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f218f19-63d8-498f-aec8-b5dfa56ca3f3",
   "metadata": {},
   "source": [
    "The last step of a machine learning (ML)/deep learning (DL) pipeline is to deploy the model to production, and get responses for a given query or a set of queries.\n",
    "In this section, we generate a dataframe that we can serve as a request to TIS. Note that this is a transformed dataframe. We also need out dataset list columns to be padded to the max sequence length that was set in the ETL pipeline.\n",
    "\n",
    "We do not serve the raw dataframe because in the production setting, we want to transform the input data as done during training (ETL). We need to apply the same mean/std for continuous features and use the same categorical mapping to convert the categories to continuous integer before we use the deployed DL model for a prediction. Therefore, we use a transformed dataset that is processed similarly as train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0acd5649-31fe-4f3f-87a2-2607477638b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_days-list</th>\n",
       "      <th>weekday_sin-list</th>\n",
       "      <th>item_id-list</th>\n",
       "      <th>category-list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.47162586, 0.9286565, 0.8286713, 0.7888927, ...</td>\n",
       "      <td>[0.8412576, 0.4806973, 0.26910275, 0.7267265, ...</td>\n",
       "      <td>[82, 2, 69, 101, 2, 26, 25, 20, 32, 5, 1, 5, 0...</td>\n",
       "      <td>[21, 1, 19, 27, 1, 7, 7, 5, 4, 2, 1, 2, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.3916677, 0.92222315, 0.23826516, 0.7312075,...</td>\n",
       "      <td>[0.11462939, 0.5831296, 0.8735699, 0.6819625, ...</td>\n",
       "      <td>[10, 1, 2, 67, 15, 64, 193, 41, 16, 15, 0, 0, ...</td>\n",
       "      <td>[3, 1, 1, 18, 6, 16, 50, 10, 6, 6, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.18798462, 0.9052097, 0.63714063, 0.5033676,...</td>\n",
       "      <td>[0.43194288, 0.45363078, 0.81598556, 0.8821798...</td>\n",
       "      <td>[31, 8, 6, 21, 11, 18, 48, 25, 21, 22, 0, 0, 0...</td>\n",
       "      <td>[9, 4, 2, 5, 3, 5, 13, 7, 5, 5, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.8022848, 0.74478865, 0.25033632, 0.5172018,...</td>\n",
       "      <td>[0.4466279, 0.9708794, 0.77920324, 0.11763577,...</td>\n",
       "      <td>[9, 7, 24, 3, 2, 17, 36, 58, 7, 6, 0, 0, 0, 0,...</td>\n",
       "      <td>[2, 2, 7, 1, 1, 6, 11, 15, 2, 2, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.37031004, 0.09800986, 0.8980817, 0.9975142,...</td>\n",
       "      <td>[0.29334334, 0.11195588, 0.7300642, 0.27448815...</td>\n",
       "      <td>[12, 30, 11, 64, 33, 19, 26, 4, 51, 0, 0, 0, 0...</td>\n",
       "      <td>[3, 8, 3, 16, 9, 4, 7, 1, 14, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       age_days-list  \\\n",
       "0  [0.47162586, 0.9286565, 0.8286713, 0.7888927, ...   \n",
       "1  [0.3916677, 0.92222315, 0.23826516, 0.7312075,...   \n",
       "2  [0.18798462, 0.9052097, 0.63714063, 0.5033676,...   \n",
       "3  [0.8022848, 0.74478865, 0.25033632, 0.5172018,...   \n",
       "4  [0.37031004, 0.09800986, 0.8980817, 0.9975142,...   \n",
       "\n",
       "                                    weekday_sin-list  \\\n",
       "0  [0.8412576, 0.4806973, 0.26910275, 0.7267265, ...   \n",
       "1  [0.11462939, 0.5831296, 0.8735699, 0.6819625, ...   \n",
       "2  [0.43194288, 0.45363078, 0.81598556, 0.8821798...   \n",
       "3  [0.4466279, 0.9708794, 0.77920324, 0.11763577,...   \n",
       "4  [0.29334334, 0.11195588, 0.7300642, 0.27448815...   \n",
       "\n",
       "                                        item_id-list  \\\n",
       "0  [82, 2, 69, 101, 2, 26, 25, 20, 32, 5, 1, 5, 0...   \n",
       "1  [10, 1, 2, 67, 15, 64, 193, 41, 16, 15, 0, 0, ...   \n",
       "2  [31, 8, 6, 21, 11, 18, 48, 25, 21, 22, 0, 0, 0...   \n",
       "3  [9, 7, 24, 3, 2, 17, 36, 58, 7, 6, 0, 0, 0, 0,...   \n",
       "4  [12, 30, 11, 64, 33, 19, 26, 4, 51, 0, 0, 0, 0...   \n",
       "\n",
       "                                       category-list  \n",
       "0  [21, 1, 19, 27, 1, 7, 7, 5, 4, 2, 1, 2, 0, 0, ...  \n",
       "1  [3, 1, 1, 18, 6, 16, 50, 10, 6, 6, 0, 0, 0, 0,...  \n",
       "2  [9, 4, 2, 5, 3, 5, 13, 7, 5, 5, 0, 0, 0, 0, 0,...  \n",
       "3  [2, 2, 7, 1, 1, 6, 11, 15, 2, 2, 0, 0, 0, 0, 0...  \n",
       "4  [3, 8, 3, 16, 9, 4, 7, 1, 14, 0, 0, 0, 0, 0, 0...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_paths = os.path.join(OUTPUT_DIR, f\"{1}/valid.parquet\")\n",
    "eval_dataset = Dataset(eval_paths, shuffle=False)\n",
    "eval_loader = generate_dataloader(schema, eval_dataset, batch_size=32)\n",
    "test_dict = next(iter(eval_loader))\n",
    "\n",
    "df_cols = {}\n",
    "for name, tensor in test_dict.items():\n",
    "    if name in input_schema.column_names:\n",
    "        dtype = input_schema[name].dtype\n",
    "\n",
    "        df_cols[name] = tensor.cpu().numpy().astype(dtype)\n",
    "        if len(tensor.shape) > 1:\n",
    "            df_cols[name] = list(df_cols[name])\n",
    "\n",
    "df = make_df(df_cols)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52832509-6444-4fd2-8834-a02eec429e14",
   "metadata": {},
   "source": [
    "Once our models are successfully loaded to the TIS, we can now easily send a request to TIS and get a response for our query with send_triton_request utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42091c25-7676-414e-bb8c-8432aeb58297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.triton.utils import send_triton_request\n",
    "response = send_triton_request(input_schema, df[input_schema.column_names], output_schema.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "430555ac-d427-48ac-b93a-da2ea41b86d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'next-item': array([[ -9.547688 ,  -3.3062646,  -3.5631118, ...,  -9.250848 ,\n",
       "          -9.964631 , -10.170028 ],\n",
       "        [ -9.54715  ,  -3.306591 ,  -3.5619133, ...,  -9.250428 ,\n",
       "          -9.9653635, -10.169994 ],\n",
       "        [ -9.547378 ,  -3.306465 ,  -3.562431 , ...,  -9.25062  ,\n",
       "          -9.964928 , -10.170043 ],\n",
       "        ...,\n",
       "        [ -9.54731  ,  -3.306385 ,  -3.5624921, ...,  -9.250595 ,\n",
       "          -9.9651   , -10.169995 ],\n",
       "        [ -9.54718  ,  -3.3064706,  -3.5621893, ...,  -9.25048  ,\n",
       "          -9.965321 , -10.169971 ],\n",
       "        [ -9.546991 ,  -3.3065925,  -3.5617704, ...,  -9.25033  ,\n",
       "          -9.965618 , -10.169941 ]], dtype=float32)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fa425a4-9c00-45ed-a4b1-fd75ca4bf819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 490)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['next-item'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0581195-d496-4536-a93b-3071ed9088ea",
   "metadata": {},
   "source": [
    "We return a response for each request in the df. Each row in the `response['next-item']` array corresponds to the logit values per item in the catalog, and one logit score corresponding to the null, OOV and padded items. The first score of each array in each row corresponds to the score for the padded item, OOV or null item. Note that we dont have OOV or null items in our syntheticall generated datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab16c64-4371-4696-b2d6-3bff66e67fdb",
   "metadata": {},
   "source": [
    "This is the end of this suit of examples. You successfully performed feature engineering with NVTabular trained transformer architecture based session-based recommendation models with Transformers4Rec deployed a trained model to Triton Inference Server with Torch backend, sent request and got responses from the server. If you would like to learn how to serve a TF4Rec model with Python backend please visit this [example](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/examples/end-to-end-session-based/02-End-to-end-session-based-with-Yoochoose-PyT.ipynb)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b543a88d374ac88bf8df97911b380f671b13649694a5b49eb21e60fd27eb479"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
