{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b739e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcc82a7",
   "metadata": {},
   "source": [
    "# Session-based Recommendation with XLNET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca66b80",
   "metadata": {},
   "source": [
    "In this notebook we introduce the [Transformers4Rec](https://github.com/NVIDIA-Merlin/Transformers4Rec) library for sequential and session-based recommendation. This notebook uses the PyTorch API, but a TensorFlow API is also available. Transformers4Rec integrates with the popular [HuggingFaceâ€™s Transformers](https://github.com/huggingface/transformers) and make it possible to experiment with cutting-edge implementation of the latest NLP Transformer architectures.  \n",
    "\n",
    "We demonstrate how to build a session-based recommendation model with the [XLNET](https://arxiv.org/abs/1906.08237) Transformer architecture. The XLNet architecture was designed to leverage the best of both auto-regressive language modeling and auto-encoding with its Permutation Language Modeling training method. In this example we will use XLNET with masked language modeling (MLM) training method, which showed very promising results in the experiments conducted in our [ACM RecSys'21 paper](https://github.com/NVIDIA-Merlin/publications/blob/main/2021_acm_recsys_transformers4rec/recsys21_transformers4rec_paper.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d6dc1",
   "metadata": {},
   "source": [
    "In the previous notebook we went through our ETL pipeline with NVTabular library, and created sequential features to be used in training a session-based recommendation model. In this notebook we will learn:\n",
    "\n",
    "- Accelerating data loading of parquet files with multiple features on PyTorch using NVTabular library\n",
    "- Training and evaluating a Transformer-based (XLNET-MLM) session-based recommendation model with multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43546c56",
   "metadata": {},
   "source": [
    "## Build a DL model with Transformers4Rec library  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2f3cb",
   "metadata": {},
   "source": [
    "### Imports required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e3d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch \n",
    "\n",
    "from transformers4rec import torch as tr\n",
    "from transformers4rec.torch.ranking_metric import NDCGAt, AvgPrecisionAt, RecallAt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb1e7f",
   "metadata": {},
   "source": [
    "Transformers4Rec library relies on a schema object to automatically build all necessary layers to represent, normalize and aggregate input features. As you can see below, `schema.pb` is a protobuf file that contains metadata including statistics about features such as cardinality, min and max values and also tags features based on their characteristics and dtypes (e.g., categorical, continuous, list, integer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f6564",
   "metadata": {},
   "source": [
    "### Manually set the schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86769216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"session_id\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"session_id\"\n",
      "    min: 1\n",
      "    max: 100001\n",
      "    is_categorical: false\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"groupby_col\"\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"category-list_trim\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 20\n",
      "  }\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"category-list_trim\"\n",
      "    min: 1\n",
      "    max: 400\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"list\"\n",
      "    tag: \"categorical\"\n",
      "    tag: \"item\"\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"item_id-list_trim\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 20\n",
      "  }\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"item_id/list\"\n",
      "    min: 1\n",
      "    max: 50005\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"item_id\"\n",
      "    tag: \"list\"\n",
      "    tag: \"categorical\"\n",
      "    tag: \"item\"\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"timestamp/age_days-list_trim\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 20\n",
      "  }\n",
      "  type: FLOAT\n",
      "  float_domain {\n",
      "    name: \"timestamp/age_days-list_trim\"\n",
      "    min: 0.0000003\n",
      "    max: 0.9999999\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"continuous\"\n",
      "    tag: \"list\"\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"timestamp/weekday/sin-list_trim\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 20\n",
      "  }\n",
      "  type: FLOAT\n",
      "  float_domain {\n",
      "    name: \"timestamp/weekday-sin_trim\"\n",
      "    min: 0.0000003\n",
      "    max: 0.9999999\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"time\"\n",
      "    tag: \"list\"\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "from merlin_standard_lib import Schema\n",
    "SCHEMA_PATH = \"schema.pb\"\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "!cat $SCHEMA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968bb3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can select a subset of features for training\n",
    "schema = schema.select_by_name(['item_id-list_trim', \n",
    "                                'category-list_trim', \n",
    "                                'timestamp/weekday/sin-list_trim',\n",
    "                                'timestamp/age_days-list_trim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8843792f",
   "metadata": {},
   "source": [
    "### Define the sequential input module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24002b4",
   "metadata": {},
   "source": [
    "Below we define our `input` block using the `TabularSequenceFeatures` [class](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/features/sequence.py#L91). The `from_schema()` method processes the schema and creates the necessary layers to represent features and aggregate them. It keeps only features tagged as `categorical` and `continuous` and supports data aggregation methods like `concat` and `elementwise-sum` techniques. It also support data augmentation techniques like stochastic swap noise. It outputs an interaction representation after combining all features and also the input mask according to the training task (more on this later).\n",
    "\n",
    "The `max_sequence_length` argument defines the maximum sequence length of our sequential input, and if `continuous_projection` argument is set, all numerical features are concatenated and projected by an MLP block so that continuous features are represented by a vector of size defined by user, which is `64` in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a9830f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tr.TabularSequenceFeatures.from_schema(\n",
    "        schema,\n",
    "        max_sequence_length=20,\n",
    "        continuous_projection=64,\n",
    "        d_output=100,\n",
    "        masking=\"mlm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073a776d",
   "metadata": {},
   "source": [
    "### Define the Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c096596",
   "metadata": {},
   "source": [
    "In the next cell, the whole model is build with a few lines of code. \n",
    "Here is a brief explanation of the main classes:  \n",
    "- [XLNetConfig](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/config/transformer.py#L261) - We have injected in the HF transformers config classes like `XLNetConfig`the `build()` method, that provides default configuration to Transformer architectures for session-based recommendation. Here we use it to instantiate and configure an XLNET architecture.  \n",
    "- [TransformerBlock](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/block/transformer.py#L37) class integrates with HF Transformers, which are made available as a sequence processing module for session-based and sequential-based recommendation models.  \n",
    "- [NextItemPredictionTask](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/model/head.py#L238) supports the next-item prediction task. We also support other predictions [tasks](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/model/head.py), like classification and regression for the whole sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc8a0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XLNetConfig class and set default parameters for HF XLNet config  \n",
    "transformer_config = tr.XLNetConfig.build(\n",
    "    d_model=64, n_head=4, n_layer=2, total_seq_length=20\n",
    ")\n",
    "# Define the model block including: inputs, masking, projection and transformer block.\n",
    "body = tr.SequentialBlock(\n",
    "    inputs, tr.MLPBlock([64]), tr.TransformerBlock(transformer_config, masking=inputs.masking)\n",
    ")\n",
    "\n",
    "# Defines the evaluation top-N metrics and the cut-offs\n",
    "metrics = [NDCGAt(top_ks=[20, 40], labels_onehot=True),  \n",
    "           RecallAt(top_ks=[20, 40], labels_onehot=True)]\n",
    "\n",
    "# Define a head related to next item prediction task \n",
    "head = tr.Head(\n",
    "    body,\n",
    "    tr.NextItemPredictionTask(weight_tying=True, hf_format=True, \n",
    "                              metrics=metrics),\n",
    "    inputs=inputs,\n",
    ")\n",
    "\n",
    "# Get the end-to-end Model class \n",
    "model = tr.Model(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171af2e",
   "metadata": {},
   "source": [
    "Note that we can easily define an RNN-based model inside the `SequentialBlock` instead of a Transformer-based model. You can explore this [tutorial](https://github.com/NVIDIA-Merlin/Transformers4Rec/tree/main/tutorial) for a GRU-based model example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acac90b",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb603c6",
   "metadata": {},
   "source": [
    "We use the NVTabular PyTorch Dataloader for optimized loading of multiple features from input parquet files. You can learn more about this data loader [here](https://nvidia.github.io/NVTabular/main/training/pytorch.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46535a73",
   "metadata": {},
   "source": [
    "### **Set Training arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ba53f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers4rec.config.trainer import T4RecTrainingArguments\n",
    "from transformers4rec.torch import Trainer\n",
    "# Set hyperparameters for training \n",
    "train_args = T4RecTrainingArguments(data_loader_engine='nvtabular', \n",
    "                                    dataloader_drop_last = True,\n",
    "                                    report_to = [], \n",
    "                                    gradient_accumulation_steps = 1,\n",
    "                                    per_device_train_batch_size = 256, \n",
    "                                    per_device_eval_batch_size = 32,\n",
    "                                    output_dir = \"./tmp\", \n",
    "                                    learning_rate=0.0005,\n",
    "                                    lr_scheduler_type='cosine', \n",
    "                                    learning_rate_num_cosine_cycles_by_epoch=1.5,\n",
    "                                    num_train_epochs=5,\n",
    "                                    max_sequence_length=20, \n",
    "                                    no_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95f1508",
   "metadata": {},
   "source": [
    "## Daily Fine-Tuning: Training over a time window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef15fd4e",
   "metadata": {},
   "source": [
    "We have extended the HuggingFace transformers `Trainer` class (PyTorch only) to support evaluation of RecSys metrics.\n",
    "\n",
    "In this example, the evaluation of the session-based recommendation model is performed using traditional Top-N ranking metrics such as Normalized Discounted Cumulative Gain (NDCG@20) and Hit Rate (HR@20). NDCG accounts for rank of the relevant item in the recommendation list and is a more fine-grained metric than HR, which only verifies whether the relevant item is among the top-n items. HR@n is equivalent to Recall@n when there is only one relevant item in the recommendation list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1182f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the T4Rec Trainer, which manages training and evaluation for the PyTorch API\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    schema=schema,\n",
    "    compute_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064a99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/workspace/data/sessions_by_day/1/train.parquet']\n",
      "********************\n",
      "Launch training for day 1 are:\n",
      "********************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time_window_index = 1\n",
    "final_time_window_index = 7\n",
    "#Iterating over days of one week\n",
    "for time_index in range(start_time_window_index, final_time_window_index):\n",
    "    # Set data \n",
    "    time_index_train = time_index\n",
    "    time_index_eval = time_index + 1\n",
    "    train_paths = glob.glob(f\"/workspace/data/sessions_by_day/{time_index_train}/train.parquet\")\n",
    "    eval_paths = glob.glob(f\"/workspace/data/sessions_by_day/{time_index_eval}/valid.parquet\")  \n",
    "    print(train_paths)\n",
    "    \n",
    "    # Train on day related to time_index \n",
    "    print('*'*20)\n",
    "    print(\"Launch training for day %s are:\" %time_index)\n",
    "    print('*'*20 + '\\n')\n",
    "    trainer.train_dataset_or_path = train_paths\n",
    "    trainer.reset_lr_scheduler()\n",
    "    trainer.train()\n",
    "    trainer.state.global_step +=1\n",
    "    print('finished')\n",
    "    \n",
    "    # Evaluate on the following day\n",
    "    trainer.eval_dataset_or_path = eval_paths\n",
    "    train_metrics = trainer.evaluate(metric_key_prefix='eval')\n",
    "    print('*'*20)\n",
    "    print(\"Eval results for day %s are:\\t\" %time_index_eval)\n",
    "    print('\\n' + '*'*20 + '\\n')\n",
    "    for key in sorted(train_metrics.keys()):\n",
    "        print(\" %s = %s\" % (key, str(train_metrics[key]))) \n",
    "    trainer.wipe_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966e3a0",
   "metadata": {},
   "source": [
    "### Saves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddec8a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._save_model_and_checkpoint(save_model_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f57da22",
   "metadata": {},
   "source": [
    "### Reloads the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48bddd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_model_trainer_states_from_checkpoint('./tmp/checkpoint-%s'%trainer.state.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82777931",
   "metadata": {},
   "source": [
    "### Re-compute eval metrics of validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fe3b2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_paths = glob.glob(f\"/workspace/data/sessions_by_day/7/valid.parquet\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2a0ff4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch = 5.0\n",
      "  eval/loss = 7.975919246673584\n",
      "  eval/next-item/ndcg_at_20 = 0.10461577773094177\n",
      "  eval/next-item/ndcg_at_40 = 0.15722110867500305\n",
      "  eval/next-item/recall_at_20 = 0.3229166865348816\n",
      "  eval/next-item/recall_at_40 = 0.5833333730697632\n",
      "  eval_runtime = 0.0951\n",
      "  eval_samples_per_second = 1008.997\n",
      "  eval_steps_per_second = 10.51\n"
     ]
    }
   ],
   "source": [
    "# set new data from day 7\n",
    "eval_metrics = trainer.evaluate(eval_dataset=eval_data_paths, metric_key_prefix='eval')\n",
    "for key in sorted(eval_metrics.keys()):\n",
    "    print(\"  %s = %s\" % (key, str(eval_metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09cec70",
   "metadata": {},
   "source": [
    "That's it!  \n",
    "You have just trained your session-based recommendation model using Transformers4Rec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fe60a-d385-45fc-8fee-5aa200e499cc",
   "metadata": {},
   "source": [
    "Tip: We can easily log and visualize model training and evaluation on [Weights & Biases (W&B)](https://wandb.ai/home), [Tensorboard](https://www.tensorflow.org/tensorboard) and [NVIDIA DLLogger](https://github.com/NVIDIA/dllogger). By default, the HuggingFace transformers `Trainer` (which we extend) uses Weights & Biases (W&B) to log training and evaluation metrics, which provides nice results visualization and comparison between different runs."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b543a88d374ac88bf8df97911b380f671b13649694a5b49eb21e60fd27eb479"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
