{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3643614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import numpy\n",
    "import pandas as pd \n",
    "import cudf\n",
    "import cupy\n",
    "import nvtabular as nvt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41769047",
   "metadata": {},
   "source": [
    "### Create random input similar to pre-processed Yoochoose dataset structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "551db053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>day</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category</th>\n",
       "      <th>timestamp/age_days</th>\n",
       "      <th>timestamp/weekday/sin</th>\n",
       "      <th>purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76430</td>\n",
       "      <td>5</td>\n",
       "      <td>25517</td>\n",
       "      <td>260</td>\n",
       "      <td>0.676600</td>\n",
       "      <td>0.436413</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75338</td>\n",
       "      <td>5</td>\n",
       "      <td>5870</td>\n",
       "      <td>295</td>\n",
       "      <td>0.601036</td>\n",
       "      <td>0.470999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77142</td>\n",
       "      <td>8</td>\n",
       "      <td>36169</td>\n",
       "      <td>311</td>\n",
       "      <td>0.560507</td>\n",
       "      <td>0.534578</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76523</td>\n",
       "      <td>1</td>\n",
       "      <td>43690</td>\n",
       "      <td>266</td>\n",
       "      <td>0.103528</td>\n",
       "      <td>0.153887</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76827</td>\n",
       "      <td>1</td>\n",
       "      <td>50699</td>\n",
       "      <td>313</td>\n",
       "      <td>0.371085</td>\n",
       "      <td>0.739300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>72601</td>\n",
       "      <td>2</td>\n",
       "      <td>39736</td>\n",
       "      <td>72</td>\n",
       "      <td>0.010964</td>\n",
       "      <td>0.740469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>76877</td>\n",
       "      <td>9</td>\n",
       "      <td>45391</td>\n",
       "      <td>103</td>\n",
       "      <td>0.033774</td>\n",
       "      <td>0.052996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>72260</td>\n",
       "      <td>8</td>\n",
       "      <td>23967</td>\n",
       "      <td>308</td>\n",
       "      <td>0.869237</td>\n",
       "      <td>0.094876</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>75266</td>\n",
       "      <td>6</td>\n",
       "      <td>48404</td>\n",
       "      <td>240</td>\n",
       "      <td>0.463385</td>\n",
       "      <td>0.360476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>72406</td>\n",
       "      <td>6</td>\n",
       "      <td>17711</td>\n",
       "      <td>121</td>\n",
       "      <td>0.377236</td>\n",
       "      <td>0.502299</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_id  day  item_id  category  timestamp/age_days  \\\n",
       "0          76430    5    25517       260            0.676600   \n",
       "1          75338    5     5870       295            0.601036   \n",
       "2          77142    8    36169       311            0.560507   \n",
       "3          76523    1    43690       266            0.103528   \n",
       "4          76827    1    50699       313            0.371085   \n",
       "...          ...  ...      ...       ...                 ...   \n",
       "9995       72601    2    39736        72            0.010964   \n",
       "9996       76877    9    45391       103            0.033774   \n",
       "9997       72260    8    23967       308            0.869237   \n",
       "9998       75266    6    48404       240            0.463385   \n",
       "9999       72406    6    17711       121            0.377236   \n",
       "\n",
       "      timestamp/weekday/sin  purchase  \n",
       "0                  0.436413         1  \n",
       "1                  0.470999         1  \n",
       "2                  0.534578         1  \n",
       "3                  0.153887         0  \n",
       "4                  0.739300         1  \n",
       "...                     ...       ...  \n",
       "9995               0.740469         0  \n",
       "9996               0.052996         0  \n",
       "9997               0.094876         1  \n",
       "9998               0.360476         0  \n",
       "9999               0.502299         1  \n",
       "\n",
       "[10000 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_ROWS = 10000\n",
    "session_length = 20\n",
    "batch_size = 100\n",
    "inputs = {\n",
    "    'session_id': numpy.random.randint(70000, 80000, NUM_ROWS),\n",
    "    'day': numpy.random.randint(1, 10, NUM_ROWS),\n",
    "    'item_id': numpy.random.randint(1, 51996, NUM_ROWS),\n",
    "    'category': numpy.random.randint(0, 332, NUM_ROWS),\n",
    "    'timestamp/age_days': numpy.random.uniform(0, 1, NUM_ROWS),\n",
    "    'timestamp/weekday/sin' : numpy.random.uniform(0, 1, NUM_ROWS),\n",
    "    'purchase': numpy.random.randint(0, 2, NUM_ROWS)\n",
    "    }\n",
    "random_data = cudf.DataFrame(inputs)\n",
    "random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1827df",
   "metadata": {},
   "source": [
    "### NVTabular workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f83f8f",
   "metadata": {},
   "source": [
    "- #TODO : Change the workflow using tagging API once it is finalized  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed85e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvtabular/nvtabular/workflow/node.py:43: FutureWarning: The `[\"a\", \"b\", \"c\"] >> ops.Operator` syntax for creating a `ColumnGroup` has been deprecated in NVTabular 21.09 and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Groupby Workflow\n",
    "groupby_features = list(inputs.keys()) >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"session_id\"], \n",
    "    aggs={\n",
    "        \"item_id\": [\"list\"],\n",
    "        \"category\": [\"list\"],     \n",
    "        \"day\": [\"first\"],\n",
    "        \"purchase\": [\"first\"],\n",
    "        \"timestamp/age_days\": [\"list\"],\n",
    "        'timestamp/weekday/sin': [\"list\"],\n",
    "        },\n",
    "    name_sep=\"-\")\n",
    "# Trim sessions to first 20 items \n",
    "groupby_features_nonlist = [x for x in groupby_features.selector if '-list' not in x]\n",
    "groupby_features_nonlist\n",
    "groupby_features_trim = ((groupby_features - groupby_features_nonlist)) >> nvt.ops.ListSlice(0,20) >> nvt.ops.Rename(postfix = '_trim')\n",
    "\n",
    "workflow = nvt.Workflow(groupby_features + groupby_features_trim )\n",
    "dataset = nvt.Dataset(random_data, cpu=False)\n",
    "workflow.fit(dataset)\n",
    "sessions_gdf = workflow.transform(dataset).to_ddf().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb01c7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day-first</th>\n",
       "      <th>timestamp/age_days-list</th>\n",
       "      <th>timestamp/weekday/sin-list</th>\n",
       "      <th>purchase-first</th>\n",
       "      <th>category-list</th>\n",
       "      <th>item_id-list</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp/age_days-list_trim</th>\n",
       "      <th>timestamp/weekday/sin-list_trim</th>\n",
       "      <th>category-list_trim</th>\n",
       "      <th>item_id-list_trim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>[0.5554684822734157]</td>\n",
       "      <td>[0.1325290970584656]</td>\n",
       "      <td>0</td>\n",
       "      <td>[331]</td>\n",
       "      <td>[33190]</td>\n",
       "      <td>70000</td>\n",
       "      <td>[0.5554684822734157]</td>\n",
       "      <td>[0.1325290970584656]</td>\n",
       "      <td>[331]</td>\n",
       "      <td>[33190]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>[0.19969787990764942, 0.5538677006954508, 0.70...</td>\n",
       "      <td>[0.40387194895713885, 0.5651163370425829, 0.08...</td>\n",
       "      <td>1</td>\n",
       "      <td>[157, 57, 298]</td>\n",
       "      <td>[14923, 42290, 43953]</td>\n",
       "      <td>70002</td>\n",
       "      <td>[0.19969787990764942, 0.5538677006954508, 0.70...</td>\n",
       "      <td>[0.40387194895713885, 0.5651163370425829, 0.08...</td>\n",
       "      <td>[157, 57, 298]</td>\n",
       "      <td>[14923, 42290, 43953]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>[0.46310031101115046]</td>\n",
       "      <td>[0.13407156620578908]</td>\n",
       "      <td>1</td>\n",
       "      <td>[76]</td>\n",
       "      <td>[47985]</td>\n",
       "      <td>70004</td>\n",
       "      <td>[0.46310031101115046]</td>\n",
       "      <td>[0.13407156620578908]</td>\n",
       "      <td>[76]</td>\n",
       "      <td>[47985]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day-first                            timestamp/age_days-list  \\\n",
       "0          8                               [0.5554684822734157]   \n",
       "1          6  [0.19969787990764942, 0.5538677006954508, 0.70...   \n",
       "2          6                              [0.46310031101115046]   \n",
       "\n",
       "                          timestamp/weekday/sin-list  purchase-first  \\\n",
       "0                               [0.1325290970584656]               0   \n",
       "1  [0.40387194895713885, 0.5651163370425829, 0.08...               1   \n",
       "2                              [0.13407156620578908]               1   \n",
       "\n",
       "    category-list           item_id-list  session_id  \\\n",
       "0           [331]                [33190]       70000   \n",
       "1  [157, 57, 298]  [14923, 42290, 43953]       70002   \n",
       "2            [76]                [47985]       70004   \n",
       "\n",
       "                        timestamp/age_days-list_trim  \\\n",
       "0                               [0.5554684822734157]   \n",
       "1  [0.19969787990764942, 0.5538677006954508, 0.70...   \n",
       "2                              [0.46310031101115046]   \n",
       "\n",
       "                     timestamp/weekday/sin-list_trim category-list_trim  \\\n",
       "0                               [0.1325290970584656]              [331]   \n",
       "1  [0.40387194895713885, 0.5651163370425829, 0.08...     [157, 57, 298]   \n",
       "2                              [0.13407156620578908]               [76]   \n",
       "\n",
       "       item_id-list_trim  \n",
       "0                [33190]  \n",
       "1  [14923, 42290, 43953]  \n",
       "2                [47985]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions_gdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "909251f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#workflow.save('workflow_inference_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba34c9",
   "metadata": {},
   "source": [
    "### Export pre-processed data by day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7476bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a Datset and write out hive-partitioned data to disk\n",
    "nvt_output_path_tmp ='./output_nvt_tmp/'\n",
    "PARTITION_COL = 'day-first'\n",
    "nvt.Dataset(sessions_gdf).to_parquet(nvt_output_path_tmp, partition_on=[PARTITION_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e329fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694\n",
      "670\n",
      "687\n",
      "713\n",
      "721\n",
      "755\n",
      "685\n",
      "687\n",
      "708\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FOLDER = \"./preproc_sessions_by_day_ts/\"\n",
    "!mkdir -p $OUTPUT_FOLDER\n",
    "days_folders = [f for f in sorted(os.listdir(nvt_output_path_tmp)) if f.startswith(PARTITION_COL)]\n",
    "for day_folder in days_folders:\n",
    "    df = cudf.read_parquet(os.path.join(nvt_output_path_tmp, day_folder))\n",
    "    print(len(df))\n",
    "    \n",
    "    out_folder = os.path.join(OUTPUT_FOLDER, day_folder.replace('day-first=', ''))\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    df.to_parquet(os.path.join(out_folder, 'train.parquet'))\n",
    "    \n",
    "    random_values = cupy.random.rand(len(df))\n",
    "    \n",
    "    #Extracts 10% for valid and test set. Those sessions are also in the train set, but as evaluation\n",
    "    #happens only for the subsequent day of training, that is not an issue, and we can keep the train set larger.\n",
    "    valid_set = df[random_values <= 0.10]\n",
    "    valid_set.to_parquet(os.path.join(out_folder, 'valid.parquet'))\n",
    "    \n",
    "    test_set = df[random_values >= 0.90]\n",
    "    test_set.to_parquet(os.path.join(out_folder, 'test.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f76c4",
   "metadata": {},
   "source": [
    "# Transformers4rec model  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e86a186",
   "metadata": {},
   "source": [
    "- Manually set the schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e41a4b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_file = 'schema.pb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9883e52e",
   "metadata": {},
   "source": [
    "### Define the sequential input module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f74589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import transformers4rec.torch as torch4rec\n",
    "from transformers4rec.torch import SequentialTabularFeatures, MLPBlock, SequentialBlock, Head, TransformerBlock\n",
    "\n",
    "from transformers4rec.utils.schema import DatasetSchema\n",
    "\n",
    "from transformers4rec.torch.head import NextItemPredictionTask\n",
    "\n",
    "from transformers4rec.config import transformer\n",
    "from transformers4rec.torch.ranking_metric import NDCGAt, AvgPrecisionAt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77a123bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nimport transformers4rec.torch as torch4rec\\n\\ninputs.masking.device = 'cuda'\\ntransformer_config = transformer.XLNetConfig.build(\\n    d_model=64, n_head=4, n_layer=2, total_seq_length=20\\n)\\nbody = torch4rec.SequentialBlock(\\n    inputs, torch4rec.MLPBlock([64]), torch4rec.TransformerBlock(transformer=transformer_config, masking=inputs.masking)\\n)\\n\\nhead = torch4rec.Head(\\n    body,\\n    torch4rec.NextItemPredictionTask(weight_tying=True, hf_format=True),\\n    inputs=inputs,\\n)\\nmodel = torch4rec.Model(head)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import transformers4rec.torch as torch4rec\n",
    "\n",
    "inputs.masking.device = 'cuda'\n",
    "transformer_config = transformer.XLNetConfig.build(\n",
    "    d_model=64, n_head=4, n_layer=2, total_seq_length=20\n",
    ")\n",
    "body = torch4rec.SequentialBlock(\n",
    "    inputs, torch4rec.MLPBlock([64]), torch4rec.TransformerBlock(transformer=transformer_config, masking=inputs.masking)\n",
    ")\n",
    "\n",
    "head = torch4rec.Head(\n",
    "    body,\n",
    "    torch4rec.NextItemPredictionTask(weight_tying=True, hf_format=True),\n",
    "    inputs=inputs,\n",
    ")\n",
    "model = torch4rec.Model(head)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d484ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TO be removed once from_schema is defined: To be removed by directly reading the schema and passing it SequentialTabularFeatures: \n",
    "schema = DatasetSchema.from_schema(\"schema.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "516942f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the tabular module that converts the inputs and aggregate them into one single interaction tensor \n",
    "# the current supported aggregations are: sequential_concat and element-wise-sum\n",
    "\n",
    "inputs = SequentialTabularFeatures.from_schema(\n",
    "        schema,\n",
    "        max_sequence_length=20,\n",
    "        continuous_projection=64,\n",
    "        d_output=100,\n",
    "        masking=\"causal\",\n",
    "    )\n",
    "\n",
    "inputs.masking.device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce0cff",
   "metadata": {},
   "source": [
    "### End-to-end session-based Transformer-based model for item prediction:\n",
    "    - LM task + HF Transformer architecture + item-prediction task "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79039c7",
   "metadata": {},
   "source": [
    "   * 1. Define the Transformer block :  LM task + HF Transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c1cd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case-1: Define XLNetConfig class and set default parameters \n",
    "\n",
    "transformer_config = transformer.XLNetConfig.build(\n",
    "    d_model=64, n_head=4, n_layer=2, total_seq_length=20\n",
    ")\n",
    "\n",
    "body = torch4rec.SequentialBlock(\n",
    "    inputs, torch4rec.MLPBlock([64]), torch4rec.TransformerBlock(transformer=transformer_config, masking=inputs.masking)\n",
    ")\n",
    "\n",
    "\n",
    "head = torch4rec.Head(\n",
    "    body,\n",
    "    torch4rec.NextItemPredictionTask(weight_tying=True, hf_format=True),\n",
    "    inputs=inputs,\n",
    ")\n",
    "model = torch4rec.Model(head)\n",
    "\n",
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c1027",
   "metadata": {},
   "source": [
    "## [To review]:  Define Item prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5594fd5",
   "metadata": {},
   "source": [
    "- The sequential item prediction task includes two formats of output:        \n",
    "     * **HF_format=True:** it returns the dictionary expected by HF recsys_trainer. \n",
    "     \n",
    "     {\n",
    "        \"loss\": loss,\n",
    "        \"labels\": labels_all,\n",
    "        \"predictions\": x,\n",
    "        \"pred_metadata\": {},\n",
    "        \"model_outputs\": []\n",
    "    }\n",
    " \n",
    " *N.B : pred_metadata and model_outputs still need to be implemented*\n",
    "     \n",
    "     * **HF_format=False:** it returns the tensor of predictions.\n",
    "\n",
    "\n",
    "- The option of **weight tying** is also included "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252fc69",
   "metadata": {},
   "source": [
    "# Train the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d0fe2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data using old NVTabular dataloader  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c0a16c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers4rec.recsys_args import DataArguments, ModelArguments, TrainingArguments\n",
    "\n",
    "TrainingArguments.local_rank = -1\n",
    "TrainingArguments.world_size = 1\n",
    "TrainingArguments.dataloader_drop_last = True\n",
    "TrainingArguments.device = \"cuda\"\n",
    "TrainingArguments.report_to = []\n",
    "TrainingArguments.debug = [\"r\"]\n",
    "TrainingArguments.n_gpu = 1\n",
    "TrainingArguments.gradient_accumulation_steps = 32\n",
    "TrainingArguments.train_batch_size = 512\n",
    "TrainingArguments.per_device_train_batch_size = 512\n",
    "TrainingArguments.per_device_eval_batch_size = 512\n",
    "TrainingArguments.output_dir = \"\"\n",
    "TrainingArguments.world_size = 1\n",
    "\n",
    "\n",
    "DataArguments.data_path = \"./preproc_sessions_by_day_ts/\"\n",
    "DataArguments.data_loader_engine = \"nvtabular\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3b4a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# NVTabular dependencies\n",
    "from nvtabular import Dataset as NVTDataset\n",
    "from nvtabular.loader.torch import DLDataLoader\n",
    "from nvtabular.loader.torch import TorchAsyncItr as NVTDataLoader\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n",
    "\n",
    "# BATCH_SIZE = 16\n",
    "SESSION_LENGTH_MAX = 20\n",
    "\n",
    "x_cat_names, x_cont_names = ['item_id-list_trim', 'category-list_trim'], ['timestamp/weekday/sin-list_trim','timestamp/age_days-list_trim']\n",
    "\n",
    "sparse_features_max = {\n",
    "    fname: SESSION_LENGTH_MAX\n",
    "    for fname in x_cat_names + x_cont_names\n",
    "}\n",
    "\n",
    "train_data_paths = glob.glob(\"./preproc_sessions_by_day_ts/*/train.parquet\")\n",
    "train_dataset = NVTDataset(\n",
    "    train_data_paths,\n",
    "    engine=\"parquet\",\n",
    ")\n",
    "\n",
    "def dataloader_collate_dict(inputs):\n",
    "    # Gets only the features dict\n",
    "    inputs = inputs[0][0]\n",
    "    return inputs\n",
    "\n",
    "class DLDataLoaderWrapper(DLDataLoader):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        if \"batch_size\" in kwargs:\n",
    "            # Setting the batch size directly to DLDataLoader makes it 3x slower. \n",
    "            # So we set as an alternative attribute and use it within RecSysTrainer during evaluation\n",
    "            self._batch_size = kwargs.pop(\"batch_size\")\n",
    "        super().__init__(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8ba308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = NVTDataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=TrainingArguments.train_batch_size,\n",
    "    shuffle=False,\n",
    "    cats=x_cat_names,\n",
    "    conts=x_cont_names,\n",
    "    device=0,\n",
    "    labels=[],\n",
    "    sparse_names=x_cat_names + x_cont_names,\n",
    "    sparse_max=sparse_features_max,\n",
    "    sparse_as_dense=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "dl_loader = DLDataLoaderWrapper(\n",
    "    loader, collate_fn=dataloader_collate_dict, batch_size=TrainingArguments.train_batch_size\n",
    "    )\n",
    "out = next(iter(dl_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de8e51f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 20])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['item_id-list_trim'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "068b2b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_id-list_trim': tensor([[31296,     0,     0,  ...,     0,     0,     0],\n",
       "         [29195,     0,     0,  ...,     0,     0,     0],\n",
       "         [48996, 30165,     0,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [19036, 47548,     0,  ...,     0,     0,     0],\n",
       "         [ 6137, 40418,     0,  ...,     0,     0,     0],\n",
       "         [19656, 16276,     0,  ...,     0,     0,     0]], device='cuda:0'),\n",
       " 'category-list_trim': tensor([[214,   0,   0,  ...,   0,   0,   0],\n",
       "         [230,   0,   0,  ...,   0,   0,   0],\n",
       "         [ 58, 319,   0,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [207, 120,   0,  ...,   0,   0,   0],\n",
       "         [ 55,  73,   0,  ...,   0,   0,   0],\n",
       "         [258,  39,   0,  ...,   0,   0,   0]], device='cuda:0'),\n",
       " 'timestamp/weekday/sin-list_trim': tensor([[0.3474, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5581, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5541, 0.2348, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.5307, 0.0517, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.6584, 0.3037, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.9694, 0.4814, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "        device='cuda:0'),\n",
       " 'timestamp/age_days-list_trim': tensor([[0.1109, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.8681, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.1013, 0.9417, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0224, 0.1926, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2736, 0.2014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.1012, 0.5608, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51854b19",
   "metadata": {},
   "source": [
    "- Test the output of the model : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18a69745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# NVTabular dependencies\n",
    "from nvtabular import Dataset as NVTDataset\n",
    "from nvtabular.loader.torch import DLDataLoader\n",
    "from nvtabular.loader.torch import TorchAsyncItr as NVTDataLoader\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n",
    "\n",
    "\n",
    "SESSION_LENGTH_MAX = 20\n",
    "\n",
    "x_cat_names, x_cont_names = ['item_id-list_trim', 'category-list_trim'], ['timestamp/weekday/sin-list_trim','timestamp/age_days-list_trim']\n",
    "\n",
    "sparse_features_max = {\n",
    "    fname: SESSION_LENGTH_MAX\n",
    "    for fname in x_cat_names + x_cont_names\n",
    "}\n",
    "\n",
    "train_data_paths = glob.glob(\"./preproc_sessions_by_day_ts/*/train.parquet\")\n",
    "train_dataset = NVTDataset(\n",
    "    train_data_paths,\n",
    "    engine=\"parquet\",\n",
    ")\n",
    "\n",
    "def dataloader_collate_dict(inputs):\n",
    "    # Gets only the features dict\n",
    "    inputs = inputs[0][0]\n",
    "    return inputs\n",
    "\n",
    "class DLDataLoaderWrapper(DLDataLoader):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        if \"batch_size\" in kwargs:\n",
    "            # Setting the batch size directly to DLDataLoader makes it 3x slower. \n",
    "            # So we set as an alternative attribute and use it within RecSysTrainer during evaluation\n",
    "            self._batch_size = kwargs.pop(\"batch_size\")\n",
    "        super().__init__(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a32cdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFWrapper(torch.nn.Module): \n",
    "    def __init__(self, model): \n",
    "        super().__init__()\n",
    "        self.model = model \n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        inputs = kwargs\n",
    "        return model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d4353db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wp = HFWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3bb295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_wp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcdfee6",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ef136",
   "metadata": {},
   "source": [
    "- Basic fit  and evaluate to test the model training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5a4cb",
   "metadata": {},
   "source": [
    "- Load arguments:  \n",
    "    N.B: These classes will be updated to keep only what required by recsys_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2020e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the RecSysTrainer, which manages training and evaluation\n",
    "from transformers4rec.recsys_trainer import RecSysTrainer, DatasetType\n",
    "\n",
    "trainer = RecSysTrainer(\n",
    "    model=model_wp,\n",
    "    args=TrainingArguments,\n",
    "    model_args=ModelArguments,\n",
    "    data_args=DataArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a1ada5",
   "metadata": {},
   "source": [
    "- Fit the model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5215c65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=16.240479787190754, metrics={'train_runtime': 2.0837, 'train_samples_per_second': 1.44, 'train_steps_per_second': 1.44, 'total_flos': 0.0, 'train_loss': 16.240479787190754, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.set_train_dataloader(dl_loader)\n",
    "trainer.reset_lr_scheduler()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd31697-b4fe-4888-b288-b4d63aabd143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76b2dd79-2cd4-4235-9e4d-a8ea048b922c",
   "metadata": {},
   "source": [
    "- Evaluate the model : \n",
    "    - N.B: evaluate model will be updated to take into account metrics defined in the prediction_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415da28-13b8-44ec-9845-eda3a9efde7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.set_eval_dataloader(dl_loader)\n",
    "# train_metrics = trainer.evaluate(metric_key_prefix=DatasetType.train.value)\n",
    "# for key in sorted(train_metrics.keys()):\n",
    "#     print(\"  %s = %s\" % (key, str(train_metrics[key])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
