{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nominated-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cudf\n",
    "import cupy\n",
    "import nvtabular as nvt\n",
    "import transformers4rec as tr\n",
    "from transformers4rec import preprocess as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instrumental-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/romeyn/data/yoochoose/\"\n",
    "FILENAME_PATTERN = 'yoochoose-clicks.dat'\n",
    "DATA_PATH = os.path.join(DATA_FOLDER, FILENAME_PATTERN)\n",
    "\n",
    "OUTPUT_FOLDER = \"/romeyn/data/yoochoose_transformed\"\n",
    "OVERWRITE = False\n",
    "MINIMUM_SESSION_LENGTH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-drove",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "buried-french",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count with in-session repeated interactions: 33003944\n",
      "Count after removed in-session repeated interactions: 28971543\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category</th>\n",
       "      <th>itemid_ts_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>623</td>\n",
       "      <td>2014-04-06 20:07:04.303</td>\n",
       "      <td>214839373</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-01 03:41:31.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>624</td>\n",
       "      <td>2014-04-06 12:40:39.889</td>\n",
       "      <td>214637025</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-02 17:40:22.149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>624</td>\n",
       "      <td>2014-04-06 12:44:12.485</td>\n",
       "      <td>214636355</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-02 15:17:18.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>626</td>\n",
       "      <td>2014-04-07 19:11:17.562</td>\n",
       "      <td>214827007</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-01 10:36:38.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>626</td>\n",
       "      <td>2014-04-07 19:11:47.407</td>\n",
       "      <td>214826925</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-03 07:10:23.818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id               timestamp    item_id category  \\\n",
       "0         623 2014-04-06 20:07:04.303  214839373        0   \n",
       "1         624 2014-04-06 12:40:39.889  214637025        0   \n",
       "2         624 2014-04-06 12:44:12.485  214636355        0   \n",
       "3         626 2014-04-07 19:11:17.562  214827007        0   \n",
       "4         626 2014-04-07 19:11:47.407  214826925        0   \n",
       "\n",
       "          itemid_ts_first  \n",
       "0 2014-04-01 03:41:31.325  \n",
       "1 2014-04-02 17:40:22.149  \n",
       "2 2014-04-02 15:17:18.195  \n",
       "3 2014-04-01 10:36:38.889  \n",
       "4 2014-04-03 07:10:23.818  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df = cudf.read_csv(DATA_PATH, sep=',', \n",
    "                                names=['session_id','timestamp', 'item_id', 'category'], \n",
    "                                parse_dates=['timestamp'])\n",
    "interactions_df = pp.remove_consecutive_interactions(interactions_df)\n",
    "items_first_ts_df = interactions_df.groupby('item_id').agg({'timestamp': 'min'}).reset_index().rename(columns={'timestamp': 'itemid_ts_first'})\n",
    "interactions_merged_df = interactions_df.merge(items_first_ts_df, on=['item_id'], how='left')\n",
    "\n",
    "interactions_merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-throat",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "warming-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogNormalize = pp.Ops(nvt.ops.LogOp(), nvt.ops.Normalize(), auto_renaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "resident-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['session_id', 'timestamp']\n",
    "\n",
    "# Temporal features\n",
    "features += LogNormalize([\"timestamp\"] >> pp.ItemRecency(\"itemid_ts_first\"), add=True)\n",
    "features += pp.TimestampFeatures(add_cycled=True, delimiter=\"/\")([\"timestamp\"])\n",
    "\n",
    "# Categorical features\n",
    "categorical = ['item_id', 'category'] >> nvt.ops.Categorify()\n",
    "features += categorical\n",
    "\n",
    "# Group-by session\n",
    "session_features = features >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"session_id\"], \n",
    "    sort_cols=[\"ts\"],\n",
    "    aggs=pp.create_session_aggs(features, extra_aggs=dict(item_id=\"count\", ts=[\"first\", \"last\"], timestamp=\"first\"), to_ignore=[\"timestamp\"]),\n",
    "    name_sep=\"/\"\n",
    ")\n",
    "rename_cols = {\"item_id/count\": \"session_size\"} \n",
    "session_features = session_features >> nvt.ops.Rename(lambda col: rename_cols.get(col, col))\n",
    "session_features += pp.SessionDay()(session_features.filter(\"timestamp/first\"))\n",
    "\n",
    "filtered_sessions = session_features >> nvt.ops.Filter(f=lambda df: df[\"session_size\"] >= MINIMUM_SESSION_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sporting-paragraph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_id/list',\n",
       " 'timestamp/hour_cos/list',\n",
       " 'ts/first',\n",
       " 'timestamp/first',\n",
       " 'timestamp/year/list',\n",
       " 'timestamp/month/list',\n",
       " 'timestamp/hour/list',\n",
       " 'timestamp/age_days/list',\n",
       " 'ts/list',\n",
       " 'ts/last',\n",
       " 'timestamp/weekday_cos/list',\n",
       " 'timestamp/day/list',\n",
       " 'timestamp/weekday_sin/list',\n",
       " 'session_id',\n",
       " 'timestamp/age_days/LogOp/Normalize/list',\n",
       " 'session_size',\n",
       " 'timestamp/hour_sin/list',\n",
       " 'category/list',\n",
       " 'timestamp/weekday/list',\n",
       " 'day_idx']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(OUTPUT_FOLDER) or OVERWRITE:\n",
    "    workflow = nvt.Workflow(filtered_sessions)\n",
    "    dataset = nvt.Dataset(interactions_merged_df, cpu=False)\n",
    "    workflow.fit(dataset)\n",
    "    # new_gdf = workflow.transform(dataset).to_ddf().compute()\n",
    "    pp.save_time_based_splits(workflow.transform(dataset), OUTPUT_FOLDER)\n",
    "    workflow.save(OUTPUT_FOLDER)\n",
    "else:\n",
    "    workflow = nvt.Workflow.load(OUTPUT_FOLDER)\n",
    "    \n",
    "workflow.column_group.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-diamond",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sustainable-boost",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mromeyn\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">/tmp/</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/romeyn/huggingface\" target=\"_blank\">https://wandb.ai/romeyn/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/romeyn/huggingface/runs/1667spju\" target=\"_blank\">https://wandb.ai/romeyn/huggingface/runs/1667spju</a><br/>\n",
       "                Run data is saved locally in <code>/romeyn/src/Transformers4Rec/notebooks/wandb/run-20210602_144811-1667spju</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:379: UserWarning: Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4907' max='16509' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4907/16509 24:55 < 58:57, 3.28 it/s, Epoch 0.89/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>9.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>8.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>8.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>8.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>8.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>7.939100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>7.929800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>7.888700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>7.791500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:379: UserWarning: Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:379: UserWarning: Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:379: UserWarning: Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:379: UserWarning: Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:379: UserWarning: Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:379: UserWarning: Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:379: UserWarning: Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:379: UserWarning: Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:379: UserWarning: Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1a9a2578c3b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_lr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1298\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                             \u001b[0;31m# Revert to normal clipping otherwise, handling Apex or full precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m                             torch.nn.utils.clip_grad_norm_(\n\u001b[0m\u001b[1;32m   1301\u001b[0m                                 \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_apex\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_args = tr.DataArguments(\n",
    "    data_path=OUTPUT_FOLDER, \n",
    "    feature_config=workflow,\n",
    "    label_name=\"item_id/list\",\n",
    "    start_time_window_index=1,\n",
    "    final_time_window_index=15,\n",
    "    time_window_folder_pad_digits=4\n",
    ")\n",
    "model_args = tr.ModelArguments()\n",
    "training_args = tr.TrainingArguments(\"/tmp/\", log_attention_weights=True)\n",
    "\n",
    "rec_model, trainer = tr.get_model_and_trainer(model_args, data_args, training_args)\n",
    "\n",
    "trainer.reset_lr_scheduler()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-november",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
