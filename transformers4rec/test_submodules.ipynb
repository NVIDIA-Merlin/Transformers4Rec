{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import yaml \n",
    "\n",
    "from torch import nn \n",
    "\n",
    "from recsys_data import get_nvtabular_dataloader\n",
    "\n",
    "from feature_process import get_feature_process \n",
    "from mask_sequence import MLM, CLM, PLM, RTD, get_masking_task\n",
    "from tower_model import TowerModel \n",
    "from prediction_head import ItemPrediction\n",
    "from meta_model import MetaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The meta-architecture is converted to 4 main submodules: \n",
    "\n",
    "\n",
    "   - **FeatureProcess:** \n",
    "        * Process multiple FeatureGroup to create the list of interactions embeddings. \n",
    "        * A FeatureGroup is defined as the combination of categoricals and continuous features (sequential or not) with the same shape. \n",
    "        * Each FeatureGroup is affected to a config file that specifies the representation types of each input and the aggregation mode. \n",
    "        * FeatureProcessOutput contains also the list of LabelFeature classes, supporting multi-task prediction : classification and/or regression and/or item prediction\n",
    "        * Each LabelFeature is an inventory dataclass containing three variables : type, label_column and dimension \n",
    "         \n",
    "         \n",
    "   - **MaskSequence:** \n",
    "        * Create the masking schema and prepare the masked inputs and labels for the selected LM task. \n",
    "        * A base MaskSequence class is created to init common parameters and four PyTorch modules are defined : CLM, MLM, PLM and RTD\n",
    "        * The MaskSequenceOutput contains four tensors: masked_input, masked_label, mask_schema, plm_target_mapping, plm_perm_mask. \n",
    "       \n",
    "       \n",
    "   -  **TowerModel:** \n",
    "       * Define the model block related to a given group of features.\n",
    "       * The input is either FeatureGroup or MaskSequenceOutput.\n",
    "       * The supported models are: HF Transformers, AvgSeq, LSTM, GRU and Gru4Rec.\n",
    "       * The module returns TowerOutput containing two information: the sequence hidden representation and the tuple (attention_weights, hidden_states).\n",
    "     \n",
    "     \n",
    "   - **PredictionHead** \n",
    "       * Extend Merlin Model \"Task\" class defined by Marc to define ItemPrediction Task \n",
    "       * Define the prediction task related to a given group of features. \n",
    "       * The supported tasks are: item prediction, classification and regression. \n",
    "       * The inputs are:  TowerOutput\n",
    "       * The module returns predictions tensor\n",
    "       \n",
    "- The general **MetaModel** runs the end-to-end workflow and currently support item-prediction task \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To test the outputs of Meta-model submodules, we consider two feature maps for two FeatureGroups from the ecomrees46 dataset : \n",
    "\n",
    "        - The first FeatureGroup uses all features present in ecomrees dataset. \n",
    "        \n",
    "        - The second FeatureGroup contains only the item-id column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_configs = [{ 'name' : 'session_based_features_all',\n",
    "                          'feature_map' : \"/workspace/transformerlib/Transformers4Rec/datasets/ecommerce_rees46/config/features/session_based_features_all.yaml\"},\n",
    "                         \n",
    "                         { 'name' : 'session_based_features_itemid',\n",
    "                          'feature_map' : \"/workspace/transformerlib/Transformers4Rec/datasets/ecommerce_rees46/config/features/session_based_features_itemid.yaml\"}\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a batch of ecomrees data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvtabular/nvtabular/io/dataset.py:253: UserWarning: Using very large partitions sizes for Dask. Memory-related errors are likely.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class training_args: \n",
    "    local_rank = -1\n",
    "    dataloader_drop_last = True\n",
    "    \n",
    "@dataclass \n",
    "class data_args: \n",
    "    session_seq_length_max = 20\n",
    "    nvt_part_mem_fraction = 0.7\n",
    "    nvt_part_size = None\n",
    "    \n",
    "data_paths = ['/data/0001/train.parquet', '/data/0002/train.parquet']\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "feature_maps = []\n",
    "for config in feature_group_configs: \n",
    "    with open(config['feature_map']) as yaml_file:\n",
    "        feature_maps.append(yaml.load(yaml_file, Loader=yaml.FullLoader))\n",
    "general_feature_map = feature_maps[0]\n",
    "general_feature_map.update(feature_maps[1])\n",
    "loader = get_nvtabular_dataloader(data_args, training_args, general_feature_map, data_paths, batch_size)\n",
    "it = iter(loader)\n",
    "first = next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-End Meta-Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-class for next item prediction with all features \n",
    "meta_model = MetaModel(feature_group_config=[feature_group_configs[0]], model_type='xlnet', masking_task='mlm', max_seq_length=20, n_head=4, n_layer=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaModel(\n",
       "  (feature_group): FeatureGroupProcess(\n",
       "    (aggregate): Aggregation()\n",
       "  )\n",
       "  (mask_task): MLM()\n",
       "  (tower_model): TowerModel(\n",
       "    (model): XLNetModel(\n",
       "      (word_embedding): Embedding(1, 1408)\n",
       "      (layer): ModuleList(\n",
       "        (0): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((1408,), eps=0.03, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((1408,), eps=0.03, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (layer_2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (1): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((1408,), eps=0.03, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((1408,), eps=0.03, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (layer_2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (body): Linear(in_features=1408, out_features=128, bias=True)\n",
       "  (prediction_head): ItemPrediction(\n",
       "    (metrics): ModuleList()\n",
       "    (loss): NLLLoss()\n",
       "    (body): Linear(in_features=1408, out_features=128, bias=True)\n",
       "    (item_embedding_table): Embedding(390000, 128, padding_idx=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = meta_model(first, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'labels', 'predictions', 'model_outputs'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0488,  0.0187,  0.0107,  ...,  0.1496,  0.0336,  0.0942],\n",
       "        [-0.0674,  0.0841,  0.0347,  ...,  0.0957,  0.0773,  0.0918],\n",
       "        [-0.0330,  0.0829, -0.0055,  ...,  0.0299,  0.0080,  0.1231],\n",
       "        ...,\n",
       "        [-0.0165, -0.0401, -0.0442,  ..., -0.0633, -0.0176,  0.1336],\n",
       "        [-0.0735,  0.0090, -0.0296,  ...,  0.0582, -0.0360,  0.0660],\n",
       "        [ 0.0124, -0.0245,  0.0098,  ..., -0.0351, -0.0303, -0.0415]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define FeatureProcess class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get FeatureProcess module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_process = get_feature_process(feature_group_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check FeatureProcess output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = feature_process(first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Aggregated output of the first sequence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 20, 1408])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.feature_groups[0].values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Aggregated output of the second sequence:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 20, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.feature_groups[1].values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Label columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabelFeature(type='item_prediction', label_column='sess_pid_seq', dimension=390000),\n",
       " LabelFeature(type='classification', label_column='sess_ccid_seq', dimension=150),\n",
       " LabelFeature(type='item_prediction', label_column='sess_pid_seq', dimension=390000)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.label_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- columns to log as metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sess_price_log_norm_seq',\n",
       " 'sess_relative_price_to_avg_category_seq',\n",
       " 'sess_prod_recency_days_log_norm_seq',\n",
       " 'sess_et_hour_sin_seq',\n",
       " 'sess_et_hour_cos_seq',\n",
       " 'sess_et_dayofweek_sin_seq',\n",
       " 'sess_et_dayofweek_cos_seq',\n",
       " 'sess_pid_seq',\n",
       " 'sess_ccid_seq',\n",
       " 'sess_csid_seq',\n",
       " 'sess_bid_seq',\n",
       " 'sess_pid_seq']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.metadata_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Masking class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each sequence is related to its own masking scheme "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking the first sequence with all features using plm \n",
    "mask_module_1 = PLM(hidden_size=1408)\n",
    "\n",
    "# masking the second sequence with item-id using mlm \n",
    "mask_module_2 = MLM(hidden_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Masking first sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = out.feature_groups[0].values\n",
    "itemid_seq =  first[feature_process.feature_groups[0].itemid_name]\n",
    "plm_out = mask_module_1(input_sequence, itemid_seq, training = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      0,      0,    251,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [  8218,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,   3641,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,   1822,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,      0,      0, 107833,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [   830,   2520,   1389,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,   7624,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,      0,    119,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plm_out.masked_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Masking second sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = out.feature_groups[1].values\n",
    "itemid_seq =  first[feature_process.feature_groups[1].itemid_name]\n",
    "mlm_out = mask_module_2(input_sequence,  itemid_seq,   True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      0,   1406,    251,   1661,    319,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [  8218,   9600,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,  10804,      0,   6258,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,   1822,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,      0,      0,      0,  46551,      0, 107833,      0, 107833,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,      0,   1389,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,   7624,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [    95,      0,      0,      0,      0,   1474,   4947,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_out.masked_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tower models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the model block for each feature group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = TowerModel(max_seq_length=20, model_type='xlnet', hidden_size=1408, n_head=4, n_layer=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TowerModel(\n",
       "  (model): XLNetModel(\n",
       "    (word_embedding): Embedding(1, 1408)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1408,), eps=0.03, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1408,), eps=0.03, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "          (layer_2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1408,), eps=0.03, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1408,), eps=0.03, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "          (layer_2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get tower outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 20, 1408])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1(plm_out).hidden_rep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Prediction Head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = nn.Linear(1408, 128).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# flatten hidden_representation vectors and get predictions only for masked positions \n",
    "def remove_pad_3d(inp_tensor, non_pad_mask):\n",
    "    # inp_tensor: (n_batch x seqlen x emb_dim)\n",
    "    inp_tensor = inp_tensor.flatten(end_dim=1)\n",
    "    inp_tensor_fl = torch.masked_select(\n",
    "        inp_tensor, non_pad_mask.unsqueeze(1).expand_as(inp_tensor)\n",
    "    )\n",
    "    out_tensor = inp_tensor_fl.view(-1, inp_tensor.size(1))\n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_flat = plm_out.masked_label.flatten()\n",
    "non_pad_mask = trg_flat != 0\n",
    "labels_all = torch.masked_select(trg_flat, non_pad_mask)\n",
    "pred_all = remove_pad_3d(model_1(plm_out).hidden_rep, non_pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ItemPrediction(loss=nn.NLLLoss(ignore_index=0), task =out.label_groups[0], body = body, feature_process=feature_process.feature_groups[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.compute_loss(inputs=pred_all, targets=labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 390000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(pred_all).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}