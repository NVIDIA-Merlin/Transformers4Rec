{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dramatic-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import yaml \n",
    "\n",
    "from torch import nn \n",
    "\n",
    "from recsys_data import get_nvtabular_dataloader\n",
    "\n",
    "from feature_process import get_feature_process \n",
    "from mask_sequence import MLM, CLM, PLM, RTD, get_masking_task\n",
    "from tower_model import TowerModel \n",
    "from prediction_head import ItemPrediction\n",
    "from meta_model import MetaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-paper",
   "metadata": {},
   "source": [
    "- The meta-architecture is converted to 4 main submodules: \n",
    "\n",
    "   - **FeatureProcess:** \n",
    "        * Process multiple FeatureGroup to create the list of interactions embeddings. \n",
    "        * A FeatureGroup is defined as the combination of categoricals and continuous features (sequential or not) with the same shape. \n",
    "        * Each FeatureGroup is affected to a config file that specifies the representation types of each input and the aggregation mode. \n",
    "        * FeatureProcessOutput contains also the list of LabelFeature classes, supporting multi-task prediction : classification and/or regression and/or item prediction\n",
    "        * Each LabelFeature is an inventory dataclass containing three variables : type, label_column and dimension \n",
    "         \n",
    "         \n",
    "   - **MaskSequence:** \n",
    "        * Create the masking schema and prepare the masked inputs and labels for the selected LM task. \n",
    "        * A base MaskSequence class is created to init common parameters and four PyTorch modules are defined : CLM, MLM, PLM and RTD\n",
    "        * The MaskSequenceOutput contains four tensors: masked_input, masked_label, mask_schema, plm_target_mapping, plm_perm_mask. \n",
    "       \n",
    "       \n",
    "   -  **TowerModel:** \n",
    "       * Define the model block related to a given group of features.\n",
    "       * The input is either FeatureGroup or MaskSequenceOutput.\n",
    "       * The supported models are: HF Transformers, AvgSeq, LSTM, GRU and Gru4Rec.\n",
    "       * The module returns TowerOutput containing two information: the sequence hidden representation and the tuple (attention_weights, hidden_states).\n",
    "     \n",
    "     \n",
    "   - **PredictionHead** \n",
    "       * Extend Merlin Model \"Task\" class defined by Marc to define ItemPrediction Task \n",
    "       * Define the prediction task related to a given group of features. \n",
    "       * The supported tasks are: item prediction, classification and regression. \n",
    "       * The inputs are:  TowerOutput\n",
    "       * The module returns predictions tensor\n",
    "       \n",
    "- The general **MetaModel** runs the end-to-end workflow and currently support item-prediction task \n",
    "\n",
    "\n",
    "<center><img src=\"./meta-archi.png\"></center>\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-needle",
   "metadata": {},
   "source": [
    "- To test the outputs of Meta-model submodules, we consider two feature maps for two FeatureGroups from the ecomrees46 dataset : \n",
    "\n",
    "        - The first FeatureGroup uses all features present in ecomrees dataset. \n",
    "        \n",
    "        - The second FeatureGroup contains only the item-id column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dimensional-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_configs = [{ 'name' : 'session_based_features_all',\n",
    "                          'feature_map' : \"/workspace/transformerlib/Transformers4Rec/datasets/ecommerce_rees46/config/features/session_based_features_all.yaml\"},\n",
    "                         \n",
    "                         { 'name' : 'session_based_features_itemid',\n",
    "                          'feature_map' : \"/workspace/transformerlib/Transformers4Rec/datasets/ecommerce_rees46/config/features/session_based_features_itemid.yaml\"}\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-hands",
   "metadata": {},
   "source": [
    "### Load a batch of ecomrees data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "everyday-initial",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvtabular/nvtabular/io/dataset.py:253: UserWarning: Using very large partitions sizes for Dask. Memory-related errors are likely.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class training_args: \n",
    "    local_rank = -1\n",
    "    dataloader_drop_last = True\n",
    "    \n",
    "@dataclass \n",
    "class data_args: \n",
    "    session_seq_length_max = 20\n",
    "    nvt_part_mem_fraction = 0.7\n",
    "    nvt_part_size = None\n",
    "    \n",
    "data_paths = ['/data/0001/train.parquet', '/data/0002/train.parquet']\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "feature_maps = []\n",
    "for config in feature_group_configs: \n",
    "    with open(config['feature_map']) as yaml_file:\n",
    "        feature_maps.append(yaml.load(yaml_file, Loader=yaml.FullLoader))\n",
    "\n",
    "general_feature_map = feature_maps[0]\n",
    "general_feature_map.update(feature_maps[1])\n",
    "loader = get_nvtabular_dataloader(data_args, training_args, general_feature_map, data_paths, batch_size)\n",
    "it = iter(loader)\n",
    "first = next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-slide",
   "metadata": {},
   "source": [
    "### End-to-End Meta-Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "substantial-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-class for next item prediction with all features \n",
    "meta_model = MetaModel(feature_group_config=[feature_group_configs[0]], model_type='xlnet', masking_task='mlm', max_seq_length=20, n_head=4, n_layer=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "supreme-colon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaModel(\n",
       "  (feature_group): FeatureGroupProcess(\n",
       "    (aggregate): SequenceAggregator(\n",
       "      (aggregator): ElementwiseSum()\n",
       "    )\n",
       "  )\n",
       "  (mask_task): MLM()\n",
       "  (tower_model): TowerModel(\n",
       "    (model): XLNetModel(\n",
       "      (word_embedding): Embedding(1, 128)\n",
       "      (layer): ModuleList(\n",
       "        (0): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((128,), eps=0.03, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((128,), eps=0.03, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (layer_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (1): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((128,), eps=0.03, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((128,), eps=0.03, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (layer_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (body): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (prediction_head): ItemPrediction(\n",
       "    (metrics): ModuleList()\n",
       "    (loss): NLLLoss()\n",
       "    (body): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (item_embedding_table): Embedding(390000, 128, padding_idx=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "whole-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = meta_model(first, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distinguished-sight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'labels', 'predictions', 'model_outputs'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conditional-nomination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0150, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exotic-generic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0361,  0.0734, -0.1109,  ..., -0.1112,  0.0275, -0.0129],\n",
       "        [-0.1325, -0.0129, -0.1116,  ..., -0.1599, -0.0181,  0.0465],\n",
       "        [-0.0934,  0.0689,  0.0181,  ..., -0.0872,  0.0204,  0.0204],\n",
       "        ...,\n",
       "        [ 0.0041,  0.0724, -0.0644,  ..., -0.0605, -0.0320,  0.0434],\n",
       "        [-0.1888, -0.0161, -0.0072,  ..., -0.1397, -0.0707, -0.0089],\n",
       "        [-0.1278,  0.1179, -0.1276,  ..., -0.1183, -0.0045,  0.0599]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-maryland",
   "metadata": {},
   "source": [
    "## Step by step testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-motel",
   "metadata": {},
   "source": [
    "### Define FeatureProcess class "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-trading",
   "metadata": {},
   "source": [
    "- Get FeatureProcess module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "secret-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_process = get_feature_process(feature_group_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-treasure",
   "metadata": {},
   "source": [
    "- Check FeatureProcess output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "expensive-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = feature_process(first)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-australian",
   "metadata": {},
   "source": [
    "    - Aggregated output of the first sequence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "designed-intelligence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 20, 1408])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.feature_groups[0].values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-advance",
   "metadata": {},
   "source": [
    "    - Aggregated output of the second sequence:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "instrumental-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 20, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.feature_groups[1].values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-virtue",
   "metadata": {},
   "source": [
    "    - Label columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "overall-drain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabelFeature(type='item_prediction', label_column='sess_pid_seq', dimension=390000),\n",
       " LabelFeature(type='classification', label_column='sess_ccid_seq', dimension=150),\n",
       " LabelFeature(type='item_prediction', label_column='sess_pid_seq', dimension=390000)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.label_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-hamburg",
   "metadata": {},
   "source": [
    "- columns to log as metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "further-cambridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sess_price_log_norm_seq',\n",
       " 'sess_relative_price_to_avg_category_seq',\n",
       " 'sess_prod_recency_days_log_norm_seq',\n",
       " 'sess_et_hour_sin_seq',\n",
       " 'sess_et_hour_cos_seq',\n",
       " 'sess_et_dayofweek_sin_seq',\n",
       " 'sess_et_dayofweek_cos_seq',\n",
       " 'sess_pid_seq',\n",
       " 'sess_ccid_seq',\n",
       " 'sess_csid_seq',\n",
       " 'sess_bid_seq',\n",
       " 'sess_pid_seq']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.metadata_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-exhibit",
   "metadata": {},
   "source": [
    "### Define Masking class "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-honolulu",
   "metadata": {},
   "source": [
    "- Each sequence is related to its own masking scheme "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "yellow-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking the first sequence with all features using plm \n",
    "mask_module_1 = PLM(hidden_size=1408)\n",
    "\n",
    "# masking the second sequence with item-id using mlm \n",
    "mask_module_2 = MLM(hidden_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-postage",
   "metadata": {},
   "source": [
    "- Masking first sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "elementary-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = out.feature_groups[0].values\n",
    "itemid_seq =  first[feature_process.feature_groups[0].itemid_name]\n",
    "plm_out = mask_module_1(input_sequence, itemid_seq, training = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "inappropriate-fireplace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      0,      0,    251,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [  8218,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,      0,      0,      0,      0,      0,   3641,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,   1822,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,      0,      0, 107833,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [   830,   2520,   1389,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,   7624,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,      0,    119,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plm_out.masked_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-transcript",
   "metadata": {},
   "source": [
    "- Masking second sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "listed-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = out.feature_groups[1].values\n",
    "itemid_seq =  first[feature_process.feature_groups[1].itemid_name]\n",
    "mlm_out = mask_module_2(input_sequence,  itemid_seq,   True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "after-dominant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      0,   1406,    251,   1661,    319,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [  8218,   9600,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,  10804,      0,   6258,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,   1822,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,      0,      0,      0,  46551,      0, 107833,      0, 107833,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,      0,   1389,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [     0,   7624,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [    95,      0,      0,      0,      0,   1474,   4947,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_out.masked_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-audit",
   "metadata": {},
   "source": [
    "### Define Tower models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-center",
   "metadata": {},
   "source": [
    "- Define the model block for each feature group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "assumed-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = TowerModel(max_seq_length=20, model_type='xlnet', hidden_size=1408, n_head=4, n_layer=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sharp-diploma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TowerModel(\n",
       "  (model): XLNetModel(\n",
       "    (word_embedding): Embedding(1, 1408)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1408,), eps=0.03, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1408,), eps=0.03, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "          (layer_2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1408,), eps=0.03, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1408,), eps=0.03, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "          (layer_2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-dependence",
   "metadata": {},
   "source": [
    "- Get tower outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "controlled-programmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 20, 1408])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1(plm_out).hidden_rep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-uganda",
   "metadata": {},
   "source": [
    "### Define Prediction Head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "strong-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = nn.Linear(1408, 128).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "duplicate-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# flatten hidden_representation vectors and get predictions only for masked positions \n",
    "def remove_pad_3d(inp_tensor, non_pad_mask):\n",
    "    # inp_tensor: (n_batch x seqlen x emb_dim)\n",
    "    inp_tensor = inp_tensor.flatten(end_dim=1)\n",
    "    inp_tensor_fl = torch.masked_select(\n",
    "        inp_tensor, non_pad_mask.unsqueeze(1).expand_as(inp_tensor)\n",
    "    )\n",
    "    out_tensor = inp_tensor_fl.view(-1, inp_tensor.size(1))\n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "musical-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_flat = plm_out.masked_label.flatten()\n",
    "non_pad_mask = trg_flat != 0\n",
    "labels_all = torch.masked_select(trg_flat, non_pad_mask)\n",
    "pred_all = remove_pad_3d(model_1(plm_out).hidden_rep, non_pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "wrapped-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ItemPrediction(loss=nn.NLLLoss(ignore_index=0), task =out.label_groups[0], body = body, feature_process=feature_process.feature_groups[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "elegant-affiliate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.compute_loss(inputs=pred_all, targets=labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "international-anime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 390000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(pred_all).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-portal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "medium-extension",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "multiply() received an invalid combination of arguments - got (), but expected one of:\n * (Tensor other)\n * (Number other)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-47cab27c6a6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmy_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#torch.Size([5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: multiply() received an invalid combination of arguments - got (), but expected one of:\n * (Tensor other)\n * (Number other)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "my_list = [torch.randn(3, 5), torch.randn(3, 5)]\n",
    "result = torch.stack(my_list, dim=0).multiply()\n",
    "print(result.shape) #torch.Size([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "behavioral-orientation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7277,  1.4060, -0.6467, -2.3037, -0.1646],\n",
       "        [ 0.3878, -0.0680, -1.4208,  2.6731,  0.0610],\n",
       "        [-1.3735,  0.6219,  0.8574,  1.7337, -1.0896]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "amazing-banner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7277,  1.4060, -0.6467, -2.3037, -0.1646],\n",
       "        [ 0.3878, -0.0680, -1.4208,  2.6731,  0.0610],\n",
       "        [-1.3735,  0.6219,  0.8574,  1.7337, -1.0896]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list[0]+my_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
