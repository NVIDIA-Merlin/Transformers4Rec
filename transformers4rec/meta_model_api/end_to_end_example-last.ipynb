{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aggressive-neutral",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING and FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-korean",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Define Data Input and Output Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prostate-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cudf\n",
    "import cupy\n",
    "import nvtabular as nvt\n",
    "from nvtabular.column import Column\n",
    "from nvtabular.column_group import ColumnGroup, Tag\n",
    "\n",
    "from preprocess import remove_consecutive_interactions, save_time_based_splits, create_session_aggs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liquid-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/workspace/yoochoose-data/\"\n",
    "FILENAME_PATTERN = 'yoochoose-clicks.dat'\n",
    "DATA_PATH = os.path.join(DATA_FOLDER, FILENAME_PATTERN)\n",
    "\n",
    "OUTPUT_FOLDER = \"/workspace/yoochoose-data/yoochoose_transformed\"\n",
    "OVERWRITE = False\n",
    "MINIMUM_SESSION_LENGTH = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blond-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = cudf.read_csv(DATA_PATH, sep=',', \n",
    "                                names=['session_id','timestamp', 'item_id', 'category'], \n",
    "                                parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-livestock",
   "metadata": {},
   "source": [
    "## 2. Load and clean raw data : \n",
    "    1- Remove repeated interactions within the same session \n",
    "    2- Create date when item was seen for the first-time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "neither-valuation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count with in-session repeated interactions: 33003944\n",
      "Count after removed in-session repeated interactions: 28971543\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category</th>\n",
       "      <th>itemid_ts_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4547</td>\n",
       "      <td>2014-04-06 15:55:47.055</td>\n",
       "      <td>214821277</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-01 03:34:33.782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4547</td>\n",
       "      <td>2014-04-06 15:58:15.621</td>\n",
       "      <td>214821292</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-01 03:44:49.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4547</td>\n",
       "      <td>2014-04-06 15:59:18.978</td>\n",
       "      <td>214820383</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-01 05:46:04.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4547</td>\n",
       "      <td>2014-04-06 16:00:29.551</td>\n",
       "      <td>214821277</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-01 03:34:33.782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4547</td>\n",
       "      <td>2014-04-06 16:02:41.959</td>\n",
       "      <td>214821290</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-01 03:25:12.166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id               timestamp    item_id category  \\\n",
       "0        4547 2014-04-06 15:55:47.055  214821277        0   \n",
       "1        4547 2014-04-06 15:58:15.621  214821292        0   \n",
       "2        4547 2014-04-06 15:59:18.978  214820383        0   \n",
       "3        4547 2014-04-06 16:00:29.551  214821277        0   \n",
       "4        4547 2014-04-06 16:02:41.959  214821290        0   \n",
       "\n",
       "          itemid_ts_first  \n",
       "0 2014-04-01 03:34:33.782  \n",
       "1 2014-04-01 03:44:49.530  \n",
       "2 2014-04-01 05:46:04.863  \n",
       "3 2014-04-01 03:34:33.782  \n",
       "4 2014-04-01 03:25:12.166  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df = remove_consecutive_interactions(interactions_df, session_id_col=\"session_id\", item_id_col=\"item_id\", timestamp_col=\"timestamp\")\n",
    "items_first_ts_df = interactions_df.groupby('item_id').agg({'timestamp': 'min'}).reset_index().rename(columns={'timestamp': 'itemid_ts_first'})\n",
    "interactions_merged_df = interactions_df.merge(items_first_ts_df, on=['item_id'], how='left')\n",
    "interactions_merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-housing",
   "metadata": {},
   "source": [
    "## 3. Generate session-based features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "anticipated-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ColumnGroup([\"session_id\", \"timestamp\"])\n",
    "\n",
    "# Categorical features: Tag item-id column as Target\n",
    "features += [\n",
    "    Column(\"item_id\", tags=[Tag.ITEM_ID, Tag.ITEM]),\n",
    "    Column(\"category\", tags=Tag.TARGETS_MULTI_CLASS),\n",
    "] >> nvt.ops.Categorify()\n",
    "\n",
    "# create time features\n",
    "sessionTime = ColumnGroup(['timestamp'])\n",
    "sessionTime_timestamp = (\n",
    "    sessionTime >> \n",
    "    nvt.ops.LambdaOp(lambda col: (col.astype(int) / 1e6).astype(int)) >> \n",
    "    nvt.ops.Rename(f = lambda col: \"ts\")\n",
    ")\n",
    "features+=sessionTime_timestamp\n",
    "\n",
    "\n",
    "# Create session-level feature : list columns \n",
    "session_features = features >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"session_id\"], \n",
    "    sort_cols=[\"ts\"],\n",
    "    aggs=create_session_aggs(features, extra_aggs=dict(item_id=\"count\",  day_index=\"last\", \n",
    "                                                       ts=[\"first\", \"last\"], timestamp=\"first\"), to_ignore=[\"timestamp\"]),\n",
    "    name_sep=\"/\"\n",
    ")\n",
    "rename_cols = {\"item_id/count\": \"session_size\"} \n",
    "session_features = session_features >> nvt.ops.Rename(lambda col: rename_cols.get(col, col))\n",
    "\n",
    "# Create day index \n",
    "day =  (session_features['timestamp/first'] >>   nvt.ops.LambdaOp(lambda col: (col.max() - col).dt.days + 1) >> \n",
    "    nvt.ops.Rename(f = lambda col: \"day_index\")\n",
    ")  \n",
    "session_features += day \n",
    "\n",
    "day_idx_padded = day >> (lambda col: col.astype(str).str.pad(4,fillchar='0')) >> nvt.ops.Rename(f = lambda col: \"day_idx_padded\")\n",
    "\n",
    "# Trim sequences to first 20 items \n",
    "non_sequence_cols = [col for col in session_features.columns.names() if 'list' not in col]\n",
    "groupby_features_trim = ((session_features - non_sequence_cols)) >> nvt.ops.ListSlice(20) >> nvt.ops.Rename(postfix = '_trim') \n",
    "\n",
    "processing = session_features + groupby_features_trim + day_idx_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "breathing-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow(processing)\n",
    "dataset = nvt.Dataset(interactions_merged_df, cpu=False)\n",
    "workflow.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acute-lawrence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='session_id', tags=['groupby_col'], properties={}),\n",
       " Column(name='timestamp/first', tags=['first'], properties={}),\n",
       " Column(name='session_size', tags=['count'], properties={}),\n",
       " Column(name='item_id/list', tags=[<DefaultTags.ITEM: ['item']>, 'categorical', <DefaultTags.ITEM_ID: ['item', 'item_id']>, 'list'], properties={}),\n",
       " Column(name='category/list', tags=['target', 'categorical', 'multi_class', 'list'], properties={}),\n",
       " Column(name='ts/last', tags=['last'], properties={}),\n",
       " Column(name='ts/first', tags=['first'], properties={}),\n",
       " Column(name='ts/list', tags=['list'], properties={}),\n",
       " Column(name='day_index', tags=None, properties={}),\n",
       " Column(name='item_id/list_trim', tags=[<DefaultTags.ITEM: ['item']>, 'categorical', <DefaultTags.ITEM_ID: ['item', 'item_id']>, 'list'], properties={}),\n",
       " Column(name='category/list_trim', tags=['target', 'categorical', 'multi_class', 'list'], properties={}),\n",
       " Column(name='ts/list_trim', tags=['list'], properties={}),\n",
       " Column(name='day_idx_padded', tags=None, properties={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.column_group.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "purple-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = workflow.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "honey-collective",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp/first</th>\n",
       "      <th>session_size</th>\n",
       "      <th>item_id/list</th>\n",
       "      <th>category/list</th>\n",
       "      <th>ts/last</th>\n",
       "      <th>ts/first</th>\n",
       "      <th>ts/list</th>\n",
       "      <th>day_index</th>\n",
       "      <th>item_id/list_trim</th>\n",
       "      <th>category/list_trim</th>\n",
       "      <th>ts/list_trim</th>\n",
       "      <th>day_idx_padded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07 10:54:09.868</td>\n",
       "      <td>3</td>\n",
       "      <td>[7340, 19523, 15499]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>1396868220305</td>\n",
       "      <td>1396868049868</td>\n",
       "      <td>[1396868049868, 1396868086998, 1396868220305]</td>\n",
       "      <td>176</td>\n",
       "      <td>[7340, 19523, 15499]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1396868049868, 1396868086998, 1396868220305]</td>\n",
       "      <td>0176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2014-04-07 13:56:37.614</td>\n",
       "      <td>5</td>\n",
       "      <td>[1968, 8039, 5178, 9410, 605]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>1396879356889</td>\n",
       "      <td>1396878997614</td>\n",
       "      <td>[1396878997614, 1396879117446, 1396879190710, ...</td>\n",
       "      <td>176</td>\n",
       "      <td>[1968, 8039, 5178, 9410, 605]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[1396878997614, 1396879117446, 1396879190710, ...</td>\n",
       "      <td>0176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2014-04-02 13:17:46.940</td>\n",
       "      <td>3</td>\n",
       "      <td>[81, 2161, 2342]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>1396445412318</td>\n",
       "      <td>1396444666940</td>\n",
       "      <td>[1396444666940, 1396445162515, 1396445412318]</td>\n",
       "      <td>181</td>\n",
       "      <td>[81, 2161, 2342]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1396444666940, 1396445162515, 1396445412318]</td>\n",
       "      <td>0181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2014-04-07 12:09:10.948</td>\n",
       "      <td>2</td>\n",
       "      <td>[2463, 5660]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1396873585416</td>\n",
       "      <td>1396872550948</td>\n",
       "      <td>[1396872550948, 1396873585416]</td>\n",
       "      <td>176</td>\n",
       "      <td>[2463, 5660]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[1396872550948, 1396873585416]</td>\n",
       "      <td>0176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2014-04-06 16:58:20.848</td>\n",
       "      <td>2</td>\n",
       "      <td>[3212, 206]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1396803746976</td>\n",
       "      <td>1396803500848</td>\n",
       "      <td>[1396803500848, 1396803746976]</td>\n",
       "      <td>177</td>\n",
       "      <td>[3212, 206]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[1396803500848, 1396803746976]</td>\n",
       "      <td>0177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id         timestamp/first  session_size  \\\n",
       "0           1 2014-04-07 10:54:09.868             3   \n",
       "1           2 2014-04-07 13:56:37.614             5   \n",
       "2           3 2014-04-02 13:17:46.940             3   \n",
       "3           4 2014-04-07 12:09:10.948             2   \n",
       "4           6 2014-04-06 16:58:20.848             2   \n",
       "\n",
       "                    item_id/list    category/list        ts/last  \\\n",
       "0           [7340, 19523, 15499]        [1, 1, 1]  1396868220305   \n",
       "1  [1968, 8039, 5178, 9410, 605]  [1, 1, 1, 1, 1]  1396879356889   \n",
       "2               [81, 2161, 2342]        [1, 1, 1]  1396445412318   \n",
       "3                   [2463, 5660]           [1, 1]  1396873585416   \n",
       "4                    [3212, 206]           [1, 1]  1396803746976   \n",
       "\n",
       "        ts/first                                            ts/list  \\\n",
       "0  1396868049868      [1396868049868, 1396868086998, 1396868220305]   \n",
       "1  1396878997614  [1396878997614, 1396879117446, 1396879190710, ...   \n",
       "2  1396444666940      [1396444666940, 1396445162515, 1396445412318]   \n",
       "3  1396872550948                     [1396872550948, 1396873585416]   \n",
       "4  1396803500848                     [1396803500848, 1396803746976]   \n",
       "\n",
       "   day_index              item_id/list_trim category/list_trim  \\\n",
       "0        176           [7340, 19523, 15499]          [1, 1, 1]   \n",
       "1        176  [1968, 8039, 5178, 9410, 605]    [1, 1, 1, 1, 1]   \n",
       "2        181               [81, 2161, 2342]          [1, 1, 1]   \n",
       "3        176                   [2463, 5660]             [1, 1]   \n",
       "4        177                    [3212, 206]             [1, 1]   \n",
       "\n",
       "                                        ts/list_trim day_idx_padded  \n",
       "0      [1396868049868, 1396868086998, 1396868220305]           0176  \n",
       "1  [1396878997614, 1396879117446, 1396879190710, ...           0176  \n",
       "2      [1396444666940, 1396445162515, 1396445412318]           0181  \n",
       "3                     [1396872550948, 1396873585416]           0176  \n",
       "4                     [1396803500848, 1396803746976]           0177  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-completion",
   "metadata": {},
   "source": [
    "## 4. Save NVTabular Workflow and Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "closed-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.save(OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-column",
   "metadata": {},
   "source": [
    "- A protobuf schema is generated to save ColumnGroups structure with their Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "subject-guatemala",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/yoochoose-data/yoochoose_transformed/schema.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls $OUTPUT_FOLDER'/schema.pbtxt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-blake",
   "metadata": {},
   "source": [
    "- save the data by partitioning based on the day_idx_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "russian-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_time_based_splits(workflow.transform(dataset), OUTPUT_FOLDER, partition_col='day_idx_padded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-quantity",
   "metadata": {},
   "source": [
    "# MODEL BUILDING and TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-belize",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "upset-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import yaml \n",
    "\n",
    "from torch import nn \n",
    "\n",
    "from recsys_data import get_nvtabular_dataloader\n",
    "\n",
    "from feature_process import get_feature_process \n",
    "from mask_sequence import MLM, CLM, PLM, RTD, get_masking_task\n",
    "from tower_model import TowerModel \n",
    "from prediction_head import ItemPrediction\n",
    "from meta_model import MetaModel\n",
    "\n",
    "from training import TrainingArguments, DataArguments, ModelArguments\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-stranger",
   "metadata": {},
   "source": [
    "## 2. Define Feature_Map\n",
    "\n",
    "To test the outputs of Meta-model submodules, we'll consider the feature_map related to the pre-processing of yoochoose dataset :\n",
    "- We can load features from three options possible: the Tagging schema / protobuf txt file / yaml.config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "twenty-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_configs = {'name' : 'session_based_features_itemid',\n",
    "                        'feature_map' : \"./datasets/session_based_features_itemid.yaml\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-governor",
   "metadata": {},
   "source": [
    "## 3. Set Data Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "awful-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataArguments.data_path = '/workspace/yoochoose-data/yoochoose_transformed/'  \n",
    "DataArguments.feature_config = feature_group_configs['feature_map'] \n",
    "DataArguments.data_loader_engine = 'nvtabular'  # also supporting petastorm and pyarrow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-million",
   "metadata": {},
   "source": [
    "## 4. Set Time-window for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "functioning-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataArguments.start_time_window_index = 1\n",
    "DataArguments.final_time_window_index = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-luxembourg",
   "metadata": {},
   "source": [
    "## 5. Set Training Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bacterial-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingArguments.n_gpu = 1\n",
    "TrainingArguments.train_batch_size = 512 * 8\n",
    "TrainingArguments.learning_rate = 1e-3\n",
    "TrainingArguments.num_train_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-promise",
   "metadata": {},
   "source": [
    "## 6. Load a batch of the Processed Data¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "flexible-software",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_id/list_trim': {'dtype': 'categorical',\n",
       "  'cardinality': 52740,\n",
       "  'is_seq_label': True,\n",
       "  'is_itemid': True,\n",
       "  'emb_table': 'item_id-list',\n",
       "  'log_with_preds_as_metadata': True}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(feature_group_configs['feature_map']) as yaml_file:\n",
    "    feature_map = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bulgarian-disorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_id/list_trim': tensor([[ 6106, 12442,  7888,  ...,     0,     0,     0],\n",
       "         [  156,     0,     0,  ...,     0,     0,     0],\n",
       "         [  897,  5558,  6555,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 6804,  8359, 14987,  ...,     0,     0,     0],\n",
       "         [ 1452,     0,     0,  ...,     0,     0,     0],\n",
       "         [ 4973,  2758,  3932,  ...,     0,     0,     0]], device='cuda:0')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_paths = ['/workspace/yoochoose-data/yoochoose_transformed/0002/train.parquet']\n",
    "loader = get_nvtabular_dataloader(DataArguments, TrainingArguments, feature_map, data_paths, TrainingArguments.train_batch_size)\n",
    "it = iter(loader)\n",
    "first = next(it)\n",
    "first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-daily",
   "metadata": {},
   "source": [
    "## 7. Instantiate an End-to-End Meta-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-concentration",
   "metadata": {},
   "source": [
    "-  Define a meta-class for next item prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "subjective-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = MetaModel(feature_group_config=[feature_group_configs], \n",
    "                       model_type='xlnet', \n",
    "                       masking_task='mlm',\n",
    "                       max_seq_length=20,\n",
    "                       n_head=4,\n",
    "                       n_layer=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "trained-stage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaModel(\n",
       "  (feature_group): FeatureGroupProcess(\n",
       "    (aggregate): SequenceAggregator(\n",
       "      (aggregator): ElementwiseSum()\n",
       "    )\n",
       "  )\n",
       "  (mask_task): MLM()\n",
       "  (tower_model): TowerModel(\n",
       "    (model): XLNetModel(\n",
       "      (word_embedding): Embedding(1, 128)\n",
       "      (layer): ModuleList(\n",
       "        (0): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((128,), eps=0.03, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((128,), eps=0.03, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (layer_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (1): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((128,), eps=0.03, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((128,), eps=0.03, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (layer_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (body): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (prediction_head): ItemPrediction(\n",
       "    (metrics): ModuleList()\n",
       "    (loss): NLLLoss()\n",
       "    (body): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (item_embedding_table): Embedding(52740, 128, padding_idx=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the meta_model' layers.\n",
    "meta_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-helmet",
   "metadata": {},
   "source": [
    "- Generate the output for the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "greenhouse-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = meta_model(training=True, **first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "everyday-split",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'labels', 'predictions', 'pred_metadata', 'model_outputs'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "engaged-abuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0043, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sixth-aurora",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0667,  0.0909, -0.0206,  ..., -0.0252, -0.0784,  0.0009],\n",
       "        [-0.0078,  0.0419, -0.0154,  ...,  0.0145,  0.0219,  0.0413],\n",
       "        [-0.0409, -0.0108, -0.0398,  ..., -0.0663,  0.0204, -0.0548],\n",
       "        ...,\n",
       "        [-0.0382, -0.0014,  0.0032,  ..., -0.0135,  0.0194, -0.0348],\n",
       "        [ 0.0426,  0.0629,  0.0010,  ..., -0.0189,  0.0489, -0.0416],\n",
       "        [-0.1488, -0.0173,  0.0454,  ..., -0.1199, -0.0235, -0.0948]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-assist",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mysterious-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the RecSysTrainer, which manages training and evaluation\n",
    "from transformers4rec.recsys_trainer import RecSysTrainer, DatasetType\n",
    "trainer = RecSysTrainer(\n",
    "    model=meta_model,\n",
    "    args=TrainingArguments,\n",
    "    model_args=ModelArguments,\n",
    "    data_args=DataArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "proprietary-sister",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "************* Training (time indices:1-3) *************\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Evaluation *************\n",
      "\n",
      "\n",
      "Evaluating on train set (time index:[1, 2, 3])....\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 01:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all data has been set. Are you sure you passed all values?\n",
      "Not all data has been set. Are you sure you passed all values?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train results (time index): [1, 2, 3])*****\n",
      "\n",
      "  epoch = 1.0\n",
      "  eval_mem_cpu_alloc_delta = -954368\n",
      "  eval_mem_cpu_peaked_delta = 90005504\n",
      "  eval_mem_gpu_alloc_delta = 0\n",
      "  eval_mem_gpu_peaked_delta = 3772212224\n",
      "  train_avg_precision@10 = 3.71969992758952e-05\n",
      "  train_avg_precision@1000 = 0.0011366652809859563\n",
      "  train_avg_precision@20 = 0.0007463916467208441\n",
      "  train_loss = -0.05457285439802541\n",
      "  train_ndcg@10 = 8.258292614805719e-05\n",
      "  train_ndcg@1000 = 0.01546501052669353\n",
      "  train_ndcg@20 = 0.0024290756967578395\n",
      "  train_precision@10 = 2.441406286379788e-05\n",
      "  train_precision@1000 = 0.0001207953593823024\n",
      "  train_precision@20 = 0.0004523383283109676\n",
      "  train_recall@10 = 0.000244140625\n",
      "  train_recall@1000 = 0.12079535590277778\n",
      "  train_recall@20 = 0.009046766493055556\n",
      "  train_runtime = 15.5062\n",
      "  train_samples_per_second = 5283.054\n",
      "\n",
      "Evaluating on test set (time index:4)....\n",
      "\n",
      "***** eval results (time index): 4)*****\n",
      "\n",
      "\t  epoch = 1.0\n",
      "\t  eval_avg_precision@10 = 3.71969992758952e-05\n",
      "\t  eval_avg_precision@1000 = 0.0011366652809859563\n",
      "\t  eval_avg_precision@20 = 0.0007463916467208441\n",
      "\t  eval_loss = -0.05457285439802541\n",
      "\t  eval_mem_cpu_alloc_delta = 266240\n",
      "\t  eval_mem_cpu_peaked_delta = 89571328\n",
      "\t  eval_mem_gpu_alloc_delta = 0\n",
      "\t  eval_mem_gpu_peaked_delta = 3772212224\n",
      "\t  eval_ndcg@10 = 8.258292614805719e-05\n",
      "\t  eval_ndcg@1000 = 0.01546501052669353\n",
      "\t  eval_ndcg@20 = 0.0024290756967578395\n",
      "\t  eval_precision@10 = 2.441406286379788e-05\n",
      "\t  eval_precision@1000 = 0.0001207953593823024\n",
      "\t  eval_precision@20 = 0.0004523383283109676\n",
      "\t  eval_recall@10 = 0.000244140625\n",
      "\t  eval_recall@1000 = 0.12079535590277778\n",
      "\t  eval_recall@20 = 0.009046766493055556\n",
      "\t  eval_runtime = 15.7797\n",
      "\t  eval_samples_per_second = 4672.317\n"
     ]
    }
   ],
   "source": [
    "from training import fit_and_evaluate\n",
    "fit_and_evaluate(trainer, start_time_window_index=1, final_time_window_index=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-devon",
   "metadata": {},
   "source": [
    "## 9. Incremental training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "important-baseball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "************* Training (time indices:1-1) *************\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Evaluation *************\n",
      "\n",
      "\n",
      "Evaluating on train set (time index:[1])....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all data has been set. Are you sure you passed all values?\n",
      "Not all data has been set. Are you sure you passed all values?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train results (time index): [1])*****\n",
      "\n",
      "  epoch = 1.0\n",
      "  eval_mem_cpu_alloc_delta = 1384448\n",
      "  eval_mem_cpu_peaked_delta = 84787200\n",
      "  eval_mem_gpu_alloc_delta = 0\n",
      "  eval_mem_gpu_peaked_delta = 3763692544\n",
      "  train_avg_precision@10 = 0.015532613359391689\n",
      "  train_avg_precision@1000 = 0.01710147727280855\n",
      "  train_avg_precision@20 = 0.015566132962703705\n",
      "  train_loss = -0.13807205557823182\n",
      "  train_ndcg@10 = 0.016115390695631504\n",
      "  train_ndcg@1000 = 0.048832619190216066\n",
      "  train_ndcg@20 = 0.0162384457886219\n",
      "  train_precision@10 = 0.0018115234561264515\n",
      "  train_precision@1000 = 0.0002700683602597564\n",
      "  train_precision@20 = 0.000930175743997097\n",
      "  train_recall@10 = 0.018115234375\n",
      "  train_recall@1000 = 0.270068359375\n",
      "  train_recall@20 = 0.018603515625\n",
      "  train_runtime = 3.6751\n",
      "  train_samples_per_second = 22290.6\n",
      "\n",
      "Evaluating on test set (time index:2)....\n",
      "\n",
      "***** eval results (time index): 2)*****\n",
      "\n",
      "\t  epoch = 1.0\n",
      "\t  eval_avg_precision@10 = 0.015532613359391689\n",
      "\t  eval_avg_precision@1000 = 0.01710147727280855\n",
      "\t  eval_avg_precision@20 = 0.015566132962703705\n",
      "\t  eval_loss = -0.13807205557823182\n",
      "\t  eval_mem_cpu_alloc_delta = -610304\n",
      "\t  eval_mem_cpu_peaked_delta = 88047616\n",
      "\t  eval_mem_gpu_alloc_delta = 0\n",
      "\t  eval_mem_gpu_peaked_delta = 3763692544\n",
      "\t  eval_ndcg@10 = 0.016115390695631504\n",
      "\t  eval_ndcg@1000 = 0.048832619190216066\n",
      "\t  eval_ndcg@20 = 0.0162384457886219\n",
      "\t  eval_precision@10 = 0.0018115234561264515\n",
      "\t  eval_precision@1000 = 0.0002700683602597564\n",
      "\t  eval_precision@20 = 0.000930175743997097\n",
      "\t  eval_recall@10 = 0.018115234375\n",
      "\t  eval_recall@1000 = 0.270068359375\n",
      "\t  eval_recall@20 = 0.018603515625\n",
      "\t  eval_runtime = 3.7549\n",
      "\t  eval_samples_per_second = 5454.266\n",
      "\n",
      "\n",
      "************* Training (time indices:2-2) *************\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Evaluation *************\n",
      "\n",
      "\n",
      "Evaluating on train set (time index:[2])....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all data has been set. Are you sure you passed all values?\n",
      "Not all data has been set. Are you sure you passed all values?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train results (time index): [2])*****\n",
      "\n",
      "  epoch = 1.0\n",
      "  eval_mem_cpu_alloc_delta = 5423104\n",
      "  eval_mem_cpu_peaked_delta = 81014784\n",
      "  eval_mem_gpu_alloc_delta = 0\n",
      "  eval_mem_gpu_peaked_delta = 3761792000\n",
      "  train_avg_precision@10 = 0.007889375323429704\n",
      "  train_avg_precision@1000 = 0.012157281395047903\n",
      "  train_avg_precision@20 = 0.010199231095612049\n",
      "  train_loss = -0.2331487573683262\n",
      "  train_ndcg@10 = 0.008895107661373913\n",
      "  train_ndcg@1000 = 0.05885584093630314\n",
      "  train_ndcg@20 = 0.016506997868418694\n",
      "  train_precision@10 = 0.0011901855177711695\n",
      "  train_precision@1000 = 0.00037261964462231845\n",
      "  train_precision@20 = 0.002017211925704032\n",
      "  train_recall@10 = 0.01190185546875\n",
      "  train_recall@1000 = 0.37261962890625\n",
      "  train_recall@20 = 0.04034423828125\n",
      "  train_runtime = 3.2536\n",
      "  train_samples_per_second = 25178.593\n",
      "\n",
      "Evaluating on test set (time index:3)....\n",
      "\n",
      "***** eval results (time index): 3)*****\n",
      "\n",
      "\t  epoch = 1.0\n",
      "\t  eval_avg_precision@10 = 0.007889375323429704\n",
      "\t  eval_avg_precision@1000 = 0.012157281395047903\n",
      "\t  eval_avg_precision@20 = 0.010199231095612049\n",
      "\t  eval_loss = -0.2331487573683262\n",
      "\t  eval_mem_cpu_alloc_delta = -3092480\n",
      "\t  eval_mem_cpu_peaked_delta = 84213760\n",
      "\t  eval_mem_gpu_alloc_delta = 0\n",
      "\t  eval_mem_gpu_peaked_delta = 3761792000\n",
      "\t  eval_ndcg@10 = 0.008895107661373913\n",
      "\t  eval_ndcg@1000 = 0.05885584093630314\n",
      "\t  eval_ndcg@20 = 0.016506997868418694\n",
      "\t  eval_precision@10 = 0.0011901855177711695\n",
      "\t  eval_precision@1000 = 0.00037261964462231845\n",
      "\t  eval_precision@20 = 0.002017211925704032\n",
      "\t  eval_recall@10 = 0.01190185546875\n",
      "\t  eval_recall@1000 = 0.37261962890625\n",
      "\t  eval_recall@20 = 0.04034423828125\n",
      "\t  eval_runtime = 2.9702\n",
      "\t  eval_samples_per_second = 5516.064\n",
      "\n",
      "\n",
      "************* Training (time indices:3-3) *************\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Evaluation *************\n",
      "\n",
      "\n",
      "Evaluating on train set (time index:[3])....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all data has been set. Are you sure you passed all values?\n",
      "Not all data has been set. Are you sure you passed all values?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train results (time index): [3])*****\n",
      "\n",
      "  epoch = 1.0\n",
      "  eval_mem_cpu_alloc_delta = 4964352\n",
      "  eval_mem_cpu_peaked_delta = 80162816\n",
      "  eval_mem_gpu_alloc_delta = 0\n",
      "  eval_mem_gpu_peaked_delta = 3765396480\n",
      "  train_avg_precision@10 = 0.0023144918626972605\n",
      "  train_avg_precision@1000 = 0.0037628881566758665\n",
      "  train_avg_precision@20 = 0.0024774001046483007\n",
      "  train_loss = -0.29610590849603924\n",
      "  train_ndcg@10 = 0.004306757490017584\n",
      "  train_ndcg@1000 = 0.03707056971532958\n",
      "  train_ndcg@20 = 0.004945235992116588\n",
      "  train_precision@10 = 0.0011056082855377878\n",
      "  train_precision@1000 = 0.00027518135695053\n",
      "  train_precision@20 = 0.0006835937633046082\n",
      "  train_recall@10 = 0.011056082589285714\n",
      "  train_recall@1000 = 0.27518136160714285\n",
      "  train_recall@20 = 0.013671875\n",
      "  train_runtime = 5.1086\n",
      "  train_samples_per_second = 16035.833\n",
      "\n",
      "Evaluating on test set (time index:4)....\n",
      "\n",
      "***** eval results (time index): 4)*****\n",
      "\n",
      "\t  epoch = 1.0\n",
      "\t  eval_avg_precision@10 = 0.0023144918626972605\n",
      "\t  eval_avg_precision@1000 = 0.0037628881566758665\n",
      "\t  eval_avg_precision@20 = 0.0024774001046483007\n",
      "\t  eval_loss = -0.29610590849603924\n",
      "\t  eval_mem_cpu_alloc_delta = -3514368\n",
      "\t  eval_mem_cpu_peaked_delta = 83886080\n",
      "\t  eval_mem_gpu_alloc_delta = 0\n",
      "\t  eval_mem_gpu_peaked_delta = 3765396480\n",
      "\t  eval_ndcg@10 = 0.004306757490017584\n",
      "\t  eval_ndcg@1000 = 0.03707056971532958\n",
      "\t  eval_ndcg@20 = 0.004945235992116588\n",
      "\t  eval_precision@10 = 0.0011056082855377878\n",
      "\t  eval_precision@1000 = 0.00027518135695053\n",
      "\t  eval_precision@20 = 0.0006835937633046082\n",
      "\t  eval_recall@10 = 0.011056082589285714\n",
      "\t  eval_recall@1000 = 0.27518136160714285\n",
      "\t  eval_recall@20 = 0.013671875\n",
      "\t  eval_runtime = 5.0471\n",
      "\t  eval_samples_per_second = 5680.835\n"
     ]
    }
   ],
   "source": [
    "from training import incremental_fit_and_evaluate\n",
    "incremental_fit_and_evaluate(trainer, start_time_window_index=1, final_time_window_index=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-martial",
   "metadata": {},
   "source": [
    "## 10. SERVING TO THE TRITON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "extreme-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "from time import time\n",
    "import warnings \n",
    "\n",
    "from tritonclient.utils import *\n",
    "import tritonclient.grpc as grpcclient\n",
    "import nvtabular\n",
    "import cudf\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "realistic-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(TrainingArguments.output_dir, TrainingArguments.model_name)\n",
    "INPUT_DATA_DIR = \"/workspace/yoochoose-data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-holmes",
   "metadata": {},
   "source": [
    "- VERIFY IF TRITON IS RUNNING CORRECTLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "prescription-postcard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tritonhttpclient/__init__.py:30: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tritonhttpclient\n",
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-third",
   "metadata": {},
   "source": [
    "- Load the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "judicial-shelter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "triton_client.load_model(model_name='yoochoose_xlnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-conditioning",
   "metadata": {},
   "source": [
    "- Get prediction for batch of data : 'first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "rubber-approval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing output for input data:\n",
      "\t {'item_id/list_trim': tensor([[ 6106, 12442,  7888,  ...,     0,     0,     0],\n",
      "        [  156,     0,     0,  ...,     0,     0,     0],\n",
      "        [  897,  5558,  6555,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 6804,  8359, 14987,  ...,     0,     0,     0],\n",
      "        [ 1452,     0,     0,  ...,     0,     0,     0],\n",
      "        [ 4973,  2758,  3932,  ...,     0,     0,     0]], device='cuda:0')}\n",
      "result:\n",
      " [[ 0.01365081  0.03654207  0.03806695 ... -0.00918404  0.12739913\n",
      "  -0.08028503]\n",
      " [-0.05451949  0.0035963   0.03438552 ...  0.03322484  0.04080514\n",
      "  -0.11493868]\n",
      " [-0.06734393  0.05595984  0.02335898 ...  0.03544397  0.04862121\n",
      "  -0.08146254]\n",
      " ...\n",
      " [ 0.00092665  0.03755305  0.086696   ... -0.05103946  0.09755504\n",
      "  -0.1181016 ]\n",
      " [-0.04559753  0.05380863 -0.07840014 ... -0.06347984  0.01734107\n",
      "  -0.05125437]\n",
      " [-0.07515392 -0.04484848 -0.00732597 ...  0.05014362 -0.00692284\n",
      "  -0.0051174 ]]\n"
     ]
    }
   ],
   "source": [
    "from inference import get_inference\n",
    "get_inference(triton_client, first) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
