{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "italian-webcam",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING and FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-nylon",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Define Data Input and Output Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sudden-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cudf\n",
    "import cupy\n",
    "import nvtabular as nvt\n",
    "from nvtabular.column import Column\n",
    "from nvtabular.column_group import ColumnGroup, Tag\n",
    "\n",
    "from preprocess import remove_consecutive_interactions, save_time_based_splits, create_session_aggs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comic-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/workspace/yoochoose-data/\"\n",
    "FILENAME_PATTERN = 'yoochoose-clicks.dat'\n",
    "DATA_PATH = os.path.join(DATA_FOLDER, FILENAME_PATTERN)\n",
    "\n",
    "OUTPUT_FOLDER = \"/workspace/yoochoose-data/yoochoose_transformed\"\n",
    "OVERWRITE = False\n",
    "MINIMUM_SESSION_LENGTH = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "productive-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = cudf.read_csv(DATA_PATH, sep=',', \n",
    "                                names=['session_id','timestamp', 'item_id', 'category'], \n",
    "                                parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-flash",
   "metadata": {},
   "source": [
    "## 2. Load and clean raw data : \n",
    "    1- Remove repeated interactions within the same session \n",
    "    2- Create date when item was seen for the first-time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "radio-arena",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count with in-session repeated interactions: 33003944\n",
      "Count after removed in-session repeated interactions: 28971543\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category</th>\n",
       "      <th>itemid_ts_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1626</td>\n",
       "      <td>2014-04-02 20:38:38.904</td>\n",
       "      <td>214644313</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-01 06:47:53.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1626</td>\n",
       "      <td>2014-04-02 20:39:17.414</td>\n",
       "      <td>214821011</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-01 03:30:29.541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1626</td>\n",
       "      <td>2014-04-02 20:40:06.533</td>\n",
       "      <td>214835445</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-01 09:36:04.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1627</td>\n",
       "      <td>2014-04-05 23:09:14.658</td>\n",
       "      <td>214826844</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-01 03:28:33.582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1628</td>\n",
       "      <td>2014-04-02 21:13:37.579</td>\n",
       "      <td>214819552</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-01 03:08:47.161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id               timestamp    item_id category  \\\n",
       "0        1626 2014-04-02 20:38:38.904  214644313        0   \n",
       "1        1626 2014-04-02 20:39:17.414  214821011        0   \n",
       "2        1626 2014-04-02 20:40:06.533  214835445        0   \n",
       "3        1627 2014-04-05 23:09:14.658  214826844        0   \n",
       "4        1628 2014-04-02 21:13:37.579  214819552        0   \n",
       "\n",
       "          itemid_ts_first  \n",
       "0 2014-04-01 06:47:53.138  \n",
       "1 2014-04-01 03:30:29.541  \n",
       "2 2014-04-01 09:36:04.606  \n",
       "3 2014-04-01 03:28:33.582  \n",
       "4 2014-04-01 03:08:47.161  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df = remove_consecutive_interactions(interactions_df)\n",
    "items_first_ts_df = interactions_df.groupby('item_id').agg({'timestamp': 'min'}).reset_index().rename(columns={'timestamp': 'itemid_ts_first'})\n",
    "interactions_merged_df = interactions_df.merge(items_first_ts_df, on=['item_id'], how='left')\n",
    "interactions_merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-zambia",
   "metadata": {},
   "source": [
    "## 3. Generate session-based features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "orange-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ColumnGroup([\"session_id\", \"timestamp\"])\n",
    "\n",
    "# Categorical features: Tag item-id column as Target\n",
    "features += [\n",
    "    Column(\"item_id\", tags=[Tag.ITEM_ID]),\n",
    "    Column(\"category\", tags=Tag.ITEM),\n",
    "] >> nvt.ops.Categorify()\n",
    "\n",
    "# create time features\n",
    "sessionTime = ColumnGroup(['timestamp'])\n",
    "sessionTime_timestamp = (\n",
    "    sessionTime >> \n",
    "    nvt.ops.LambdaOp(lambda col: (col.astype(int) / 1e6).astype(int)) >> \n",
    "    nvt.ops.Rename(f = lambda col: \"ts\")\n",
    ")\n",
    "features+=sessionTime_timestamp\n",
    "\n",
    "\n",
    "# Create session-level feature : list columns \n",
    "session_features = features >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"session_id\"], \n",
    "    sort_cols=[\"ts\"],\n",
    "    aggs=create_session_aggs(features, extra_aggs=dict(item_id=\"count\",  day_index=\"last\", \n",
    "                                                       ts=[\"first\", \"last\"], timestamp=\"first\"), to_ignore=[\"timestamp\"]),\n",
    "    name_sep=\"/\"\n",
    ")\n",
    "rename_cols = {\"item_id/count\": \"session_size\"} \n",
    "session_features = session_features >> nvt.ops.Rename(lambda col: rename_cols.get(col, col))\n",
    "\n",
    "# Create day index \n",
    "day =  (session_features['timestamp/first'] >>   nvt.ops.LambdaOp(lambda col: (col.max() - col).dt.days + 1) >> \n",
    "    nvt.ops.Rename(f = lambda col: \"day_index\")\n",
    ")  \n",
    "session_features += day \n",
    "\n",
    "day_idx_padded = day >> (lambda col: col.astype(str).str.pad(4,fillchar='0')) >> nvt.ops.Rename(f = lambda col: \"day_idx_padded\")\n",
    "\n",
    "# Trim sequences to first 20 items \n",
    "non_sequence_cols = [col for col in session_features.columns.names() if 'list' not in col]\n",
    "groupby_features_trim = ((session_features - non_sequence_cols)) >> nvt.ops.ListSlice(20) >> nvt.ops.Rename(postfix = '_trim') \n",
    "\n",
    "processing = session_features + groupby_features_trim + day_idx_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "following-portsmouth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='session_id', tags=['groupby_col'], properties={}),\n",
       " Column(name='timestamp/first', tags=['first'], properties={}),\n",
       " Column(name='session_size', tags=['count'], properties={}),\n",
       " Column(name='item_id/list', tags=[<DefaultTags.ITEM_ID: ['item', 'item_id']>, 'list', 'categorical'], properties={}),\n",
       " Column(name='category/list', tags=['item', 'list', 'categorical'], properties={}),\n",
       " Column(name='ts/first', tags=['first'], properties={}),\n",
       " Column(name='ts/list', tags=['list'], properties={}),\n",
       " Column(name='ts/last', tags=['last'], properties={}),\n",
       " Column(name='day_index', tags=None, properties={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coral-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow(processing)\n",
    "dataset = nvt.Dataset(interactions_merged_df, cpu=False)\n",
    "workflow.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "changed-buddy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='session_id', tags=['groupby_col'], properties={}),\n",
       " Column(name='timestamp/first', tags=['first'], properties={}),\n",
       " Column(name='session_size', tags=['count'], properties={}),\n",
       " Column(name='item_id/list', tags=[<DefaultTags.ITEM_ID: ['item', 'item_id']>, 'list', 'categorical'], properties={}),\n",
       " Column(name='category/list', tags=['item', 'list', 'categorical'], properties={}),\n",
       " Column(name='ts/first', tags=['first'], properties={}),\n",
       " Column(name='ts/list', tags=['list'], properties={}),\n",
       " Column(name='ts/last', tags=['last'], properties={}),\n",
       " Column(name='day_index', tags=None, properties={}),\n",
       " Column(name='item_id/list_trim', tags=[<DefaultTags.ITEM_ID: ['item', 'item_id']>, 'list', 'categorical'], properties={}),\n",
       " Column(name='category/list_trim', tags=['item', 'list', 'categorical'], properties={}),\n",
       " Column(name='ts/list_trim', tags=['list'], properties={}),\n",
       " Column(name='day_idx_padded', tags=None, properties={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.column_group.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reliable-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = workflow.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reverse-dollar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp/first</th>\n",
       "      <th>session_size</th>\n",
       "      <th>item_id/list</th>\n",
       "      <th>category/list</th>\n",
       "      <th>ts/first</th>\n",
       "      <th>ts/list</th>\n",
       "      <th>ts/last</th>\n",
       "      <th>day_index</th>\n",
       "      <th>item_id/list_trim</th>\n",
       "      <th>category/list_trim</th>\n",
       "      <th>ts/list_trim</th>\n",
       "      <th>day_idx_padded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07 10:54:09.868</td>\n",
       "      <td>3</td>\n",
       "      <td>[7340, 19523, 15499]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>1396868049868</td>\n",
       "      <td>[1396868049868, 1396868086998, 1396868220305]</td>\n",
       "      <td>1396868220305</td>\n",
       "      <td>176</td>\n",
       "      <td>[7340, 19523, 15499]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1396868049868, 1396868086998, 1396868220305]</td>\n",
       "      <td>0176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2014-04-07 13:56:37.614</td>\n",
       "      <td>5</td>\n",
       "      <td>[1968, 8039, 5178, 9410, 605]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>1396878997614</td>\n",
       "      <td>[1396878997614, 1396879117446, 1396879190710, ...</td>\n",
       "      <td>1396879356889</td>\n",
       "      <td>176</td>\n",
       "      <td>[1968, 8039, 5178, 9410, 605]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[1396878997614, 1396879117446, 1396879190710, ...</td>\n",
       "      <td>0176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2014-04-02 13:17:46.940</td>\n",
       "      <td>3</td>\n",
       "      <td>[81, 2161, 2342]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>1396444666940</td>\n",
       "      <td>[1396444666940, 1396445162515, 1396445412318]</td>\n",
       "      <td>1396445412318</td>\n",
       "      <td>181</td>\n",
       "      <td>[81, 2161, 2342]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1396444666940, 1396445162515, 1396445412318]</td>\n",
       "      <td>0181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2014-04-07 12:09:10.948</td>\n",
       "      <td>2</td>\n",
       "      <td>[2463, 5660]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1396872550948</td>\n",
       "      <td>[1396872550948, 1396873585416]</td>\n",
       "      <td>1396873585416</td>\n",
       "      <td>176</td>\n",
       "      <td>[2463, 5660]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[1396872550948, 1396873585416]</td>\n",
       "      <td>0176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2014-04-06 16:58:20.848</td>\n",
       "      <td>2</td>\n",
       "      <td>[3212, 206]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1396803500848</td>\n",
       "      <td>[1396803500848, 1396803746976]</td>\n",
       "      <td>1396803746976</td>\n",
       "      <td>177</td>\n",
       "      <td>[3212, 206]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[1396803500848, 1396803746976]</td>\n",
       "      <td>0177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id         timestamp/first  session_size  \\\n",
       "0           1 2014-04-07 10:54:09.868             3   \n",
       "1           2 2014-04-07 13:56:37.614             5   \n",
       "2           3 2014-04-02 13:17:46.940             3   \n",
       "3           4 2014-04-07 12:09:10.948             2   \n",
       "4           6 2014-04-06 16:58:20.848             2   \n",
       "\n",
       "                    item_id/list    category/list       ts/first  \\\n",
       "0           [7340, 19523, 15499]        [1, 1, 1]  1396868049868   \n",
       "1  [1968, 8039, 5178, 9410, 605]  [1, 1, 1, 1, 1]  1396878997614   \n",
       "2               [81, 2161, 2342]        [1, 1, 1]  1396444666940   \n",
       "3                   [2463, 5660]           [1, 1]  1396872550948   \n",
       "4                    [3212, 206]           [1, 1]  1396803500848   \n",
       "\n",
       "                                             ts/list        ts/last  \\\n",
       "0      [1396868049868, 1396868086998, 1396868220305]  1396868220305   \n",
       "1  [1396878997614, 1396879117446, 1396879190710, ...  1396879356889   \n",
       "2      [1396444666940, 1396445162515, 1396445412318]  1396445412318   \n",
       "3                     [1396872550948, 1396873585416]  1396873585416   \n",
       "4                     [1396803500848, 1396803746976]  1396803746976   \n",
       "\n",
       "   day_index              item_id/list_trim category/list_trim  \\\n",
       "0        176           [7340, 19523, 15499]          [1, 1, 1]   \n",
       "1        176  [1968, 8039, 5178, 9410, 605]    [1, 1, 1, 1, 1]   \n",
       "2        181               [81, 2161, 2342]          [1, 1, 1]   \n",
       "3        176                   [2463, 5660]             [1, 1]   \n",
       "4        177                    [3212, 206]             [1, 1]   \n",
       "\n",
       "                                        ts/list_trim day_idx_padded  \n",
       "0      [1396868049868, 1396868086998, 1396868220305]           0176  \n",
       "1  [1396878997614, 1396879117446, 1396879190710, ...           0176  \n",
       "2      [1396444666940, 1396445162515, 1396445412318]           0181  \n",
       "3                     [1396872550948, 1396873585416]           0176  \n",
       "4                     [1396803500848, 1396803746976]           0177  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-shaft",
   "metadata": {},
   "source": [
    "## 4. Save NVTabular Workflow and Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spread-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.save(OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-spotlight",
   "metadata": {},
   "source": [
    "- save the data by partitioning based on the day_idx_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "horizontal-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_time_based_splits(workflow.transform(dataset), OUTPUT_FOLDER, partition_col='day_idx_padded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-forwarding",
   "metadata": {},
   "source": [
    "# MODEL BUILDING and TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-stress",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "challenging-ferry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import yaml \n",
    "\n",
    "from torch import nn \n",
    "\n",
    "from recsys_data import get_nvtabular_dataloader\n",
    "\n",
    "from feature_process import get_feature_process \n",
    "from mask_sequence import MLM, CLM, PLM, RTD, get_masking_task\n",
    "from tower_model import TowerModel \n",
    "from prediction_head import ItemPrediction\n",
    "from meta_model import MetaModel\n",
    "\n",
    "from training import TrainingArguments, DataArguments, ModelArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-infection",
   "metadata": {},
   "source": [
    "## 2. Define Feature_Map\n",
    "\n",
    "To test the outputs of Meta-model submodules, we'll consider the feature_map related to the pre-processing of yoochoose dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "muslim-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_configs = {'name' : 'session_based_features_itemid',\n",
    "                        'feature_map' : \"./datasets/session_based_features_itemid.yaml\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-jacket",
   "metadata": {},
   "source": [
    "## 3. Set Data Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "instant-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataArguments.data_path = '/workspace/yoochoose-data/yoochoose_transformed/'  \n",
    "DataArguments.feature_config = feature_group_configs['feature_map'] \n",
    "DataArguments.data_loader_engine = 'nvtabular'  # also supporting petastorm and pyarrow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-glenn",
   "metadata": {},
   "source": [
    "## 4. Set Time-window for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "solar-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataArguments.start_time_window_index = 1\n",
    "DataArguments.final_time_window_index = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-decimal",
   "metadata": {},
   "source": [
    "## 5. Set Training Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dominican-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingArguments.n_gpu = 1\n",
    "TrainingArguments.train_batch_size = 512 * 8\n",
    "TrainingArguments.learning_rate = 1e-3\n",
    "TrainingArguments.num_train_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-mailing",
   "metadata": {},
   "source": [
    "## 6. Load a batch of the Processed DataÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adaptive-welsh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_id/list_trim': {'dtype': 'categorical',\n",
       "  'cardinality': 52740,\n",
       "  'is_seq_label': True,\n",
       "  'is_itemid': True,\n",
       "  'emb_table': 'item_id-list',\n",
       "  'log_with_preds_as_metadata': True}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(feature_group_configs['feature_map']) as yaml_file:\n",
    "    feature_map = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accomplished-capture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_id/list_trim': tensor([[ 6106, 12442,  7888,  ...,     0,     0,     0],\n",
       "         [  156,     0,     0,  ...,     0,     0,     0],\n",
       "         [  897,  5558,  6555,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 6804,  8359, 14987,  ...,     0,     0,     0],\n",
       "         [ 1452,     0,     0,  ...,     0,     0,     0],\n",
       "         [ 4973,  2758,  3932,  ...,     0,     0,     0]], device='cuda:0')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_paths = ['/workspace/yoochoose-data/yoochoose_transformed/0002/train.parquet']\n",
    "loader = get_nvtabular_dataloader(DataArguments, TrainingArguments, feature_map, data_paths, TrainingArguments.train_batch_size)\n",
    "it = iter(loader)\n",
    "first = next(it)\n",
    "first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-temperature",
   "metadata": {},
   "source": [
    "## 7. Instantiate an End-to-End Meta-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-waterproof",
   "metadata": {},
   "source": [
    "-  Define a meta-class for next item prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "center-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = MetaModel(feature_group_config=[feature_group_configs], \n",
    "                       model_type='xlnet', \n",
    "                       masking_task='mlm',\n",
    "                       max_seq_length=20,\n",
    "                       n_head=4,\n",
    "                       n_layer=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "careful-wright",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaModel(\n",
       "  (feature_group): FeatureGroupProcess(\n",
       "    (aggregate): SequenceAggregator(\n",
       "      (aggregator): ElementwiseSum()\n",
       "    )\n",
       "  )\n",
       "  (mask_task): MLM()\n",
       "  (tower_model): TowerModel(\n",
       "    (model): XLNetModel(\n",
       "      (word_embedding): Embedding(1, 128)\n",
       "      (layer): ModuleList(\n",
       "        (0): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((128,), eps=0.03, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((128,), eps=0.03, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (layer_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (1): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((128,), eps=0.03, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((128,), eps=0.03, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (layer_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (body): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (prediction_head): ItemPrediction(\n",
       "    (metrics): ModuleList()\n",
       "    (loss): NLLLoss()\n",
       "    (body): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (item_embedding_table): Embedding(52740, 128, padding_idx=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the meta_model' layers.\n",
    "meta_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-platform",
   "metadata": {},
   "source": [
    "- Generate the output for the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "interpreted-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = meta_model(training=True, **first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "forced-cause",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'labels', 'predictions', 'pred_metadata', 'model_outputs'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "boxed-syntax",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0002, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "historic-music",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0817, -0.0933,  0.0447,  ..., -0.0388,  0.0132, -0.0351],\n",
       "        [-0.0051, -0.0956,  0.0231,  ..., -0.0080, -0.0022,  0.0210],\n",
       "        [ 0.0709, -0.1023,  0.0860,  ..., -0.0767, -0.0267, -0.0515],\n",
       "        ...,\n",
       "        [ 0.1012, -0.1536,  0.0316,  ..., -0.1368,  0.0007, -0.0606],\n",
       "        [ 0.1283, -0.0041,  0.0400,  ..., -0.1228,  0.0025, -0.0185],\n",
       "        [ 0.0448, -0.0978, -0.0368,  ...,  0.0091,  0.0891, -0.0386]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-mongolia",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "suited-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the RecSysTrainer, which manages training and evaluation\n",
    "from transformers4rec.recsys_trainer import RecSysTrainer, DatasetType\n",
    "trainer = RecSysTrainer(\n",
    "    model=meta_model,\n",
    "    args=TrainingArguments,\n",
    "    model_args=ModelArguments,\n",
    "    data_args=DataArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "beginning-florida",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "************* Training (time indices:1-3) *************\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Evaluation *************\n",
      "\n",
      "\n",
      "Evaluating on train set (time index:[1, 2, 3])....\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 01:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all data has been set. Are you sure you passed all values?\n",
      "Not all data has been set. Are you sure you passed all values?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train results (time index): [1, 2, 3])*****\n",
      "\n",
      "  epoch = 1.0\n",
      "  eval_mem_cpu_alloc_delta = 4022272\n",
      "  eval_mem_cpu_peaked_delta = 85684224\n",
      "  eval_mem_gpu_alloc_delta = 0\n",
      "  eval_mem_gpu_peaked_delta = 3772212224\n",
      "  train_avg_precision@10 = 0.00013894917153973237\n",
      "  train_avg_precision@1000 = 0.0012684272175344329\n",
      "  train_avg_precision@20 = 0.0007196772235652639\n",
      "  train_loss = -0.054798177546925016\n",
      "  train_ndcg@10 = 0.0002502151894279652\n",
      "  train_ndcg@1000 = 0.012319817362974087\n",
      "  train_ndcg@20 = 0.0025499717772213947\n",
      "  train_precision@10 = 6.103515766476953e-05\n",
      "  train_precision@1000 = 8.676486565188195e-05\n",
      "  train_precision@20 = 0.0005059136318676691\n",
      "  train_recall@10 = 0.0006103515625\n",
      "  train_recall@1000 = 0.0867648654513889\n",
      "  train_recall@20 = 0.010118272569444444\n",
      "  train_runtime = 14.5612\n",
      "  train_samples_per_second = 5625.901\n",
      "\n",
      "Evaluating on test set (time index:4)....\n",
      "\n",
      "***** eval results (time index): 4)*****\n",
      "\n",
      "\t  epoch = 1.0\n",
      "\t  eval_avg_precision@10 = 0.00013894917153973237\n",
      "\t  eval_avg_precision@1000 = 0.0012684272175344329\n",
      "\t  eval_avg_precision@20 = 0.0007196772235652639\n",
      "\t  eval_loss = -0.054798177546925016\n",
      "\t  eval_mem_cpu_alloc_delta = 2195456\n",
      "\t  eval_mem_cpu_peaked_delta = 82452480\n",
      "\t  eval_mem_gpu_alloc_delta = 0\n",
      "\t  eval_mem_gpu_peaked_delta = 3772212224\n",
      "\t  eval_ndcg@10 = 0.0002502151894279652\n",
      "\t  eval_ndcg@1000 = 0.012319817362974087\n",
      "\t  eval_ndcg@20 = 0.0025499717772213947\n",
      "\t  eval_precision@10 = 6.103515766476953e-05\n",
      "\t  eval_precision@1000 = 8.676486565188195e-05\n",
      "\t  eval_precision@20 = 0.0005059136318676691\n",
      "\t  eval_recall@10 = 0.0006103515625\n",
      "\t  eval_recall@1000 = 0.0867648654513889\n",
      "\t  eval_recall@20 = 0.010118272569444444\n",
      "\t  eval_runtime = 14.4318\n",
      "\t  eval_samples_per_second = 5108.735\n"
     ]
    }
   ],
   "source": [
    "from training import fit\n",
    "fit(trainer, start_time_window_index=1, final_time_window_index=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-negative",
   "metadata": {},
   "source": [
    "## 9. Incremental training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "anonymous-virgin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "************* Training (time indices:1-1) *************\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Evaluation *************\n",
      "\n",
      "\n",
      "Evaluating on train set (time index:[1])....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all data has been set. Are you sure you passed all values?\n",
      "Not all data has been set. Are you sure you passed all values?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train results (time index): [1])*****\n",
      "\n",
      "  epoch = 1.0\n",
      "  eval_mem_cpu_alloc_delta = 909312\n",
      "  eval_mem_cpu_peaked_delta = 83886080\n",
      "  eval_mem_gpu_alloc_delta = 0\n",
      "  eval_mem_gpu_peaked_delta = 3763692544\n",
      "  train_avg_precision@10 = 0.019070037454366685\n",
      "  train_avg_precision@1000 = 0.02038167156279087\n",
      "  train_avg_precision@20 = 0.019387409649789335\n",
      "  train_loss = -0.13771139979362487\n",
      "  train_ndcg@10 = 0.01982517819851637\n",
      "  train_ndcg@1000 = 0.04873221516609192\n",
      "  train_ndcg@20 = 0.02114473097026348\n",
      "  train_precision@10 = 0.002250976557843387\n",
      "  train_precision@1000 = 0.00025708007742650805\n",
      "  train_precision@20 = 0.0014038086286745966\n",
      "  train_recall@10 = 0.022509765625\n",
      "  train_recall@1000 = 0.257080078125\n",
      "  train_recall@20 = 0.028076171875\n",
      "  train_runtime = 3.4692\n",
      "  train_samples_per_second = 23613.311\n",
      "\n",
      "Evaluating on test set (time index:2)....\n",
      "\n",
      "***** eval results (time index): 2)*****\n",
      "\n",
      "\t  epoch = 1.0\n",
      "\t  eval_avg_precision@10 = 0.019070037454366685\n",
      "\t  eval_avg_precision@1000 = 0.02038167156279087\n",
      "\t  eval_avg_precision@20 = 0.019387409649789335\n",
      "\t  eval_loss = -0.13771139979362487\n",
      "\t  eval_mem_cpu_alloc_delta = -1863680\n",
      "\t  eval_mem_cpu_peaked_delta = 86224896\n",
      "\t  eval_mem_gpu_alloc_delta = 0\n",
      "\t  eval_mem_gpu_peaked_delta = 3763692544\n",
      "\t  eval_ndcg@10 = 0.01982517819851637\n",
      "\t  eval_ndcg@1000 = 0.04873221516609192\n",
      "\t  eval_ndcg@20 = 0.02114473097026348\n",
      "\t  eval_precision@10 = 0.002250976557843387\n",
      "\t  eval_precision@1000 = 0.00025708007742650805\n",
      "\t  eval_precision@20 = 0.0014038086286745966\n",
      "\t  eval_recall@10 = 0.022509765625\n",
      "\t  eval_recall@1000 = 0.257080078125\n",
      "\t  eval_recall@20 = 0.028076171875\n",
      "\t  eval_runtime = 3.62\n",
      "\t  eval_samples_per_second = 5657.452\n",
      "\n",
      "\n",
      "************* Training (time indices:2-2) *************\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Evaluation *************\n",
      "\n",
      "\n",
      "Evaluating on train set (time index:[2])....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all data has been set. Are you sure you passed all values?\n",
      "Not all data has been set. Are you sure you passed all values?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train results (time index): [2])*****\n",
      "\n",
      "  epoch = 1.0\n",
      "  eval_mem_cpu_alloc_delta = -3121152\n",
      "  eval_mem_cpu_peaked_delta = 89497600\n",
      "  eval_mem_gpu_alloc_delta = 0\n",
      "  eval_mem_gpu_peaked_delta = 3761792000\n",
      "  train_avg_precision@10 = 0.022967674769461155\n",
      "  train_avg_precision@1000 = 0.025625487323850393\n",
      "  train_avg_precision@20 = 0.023957342840731144\n",
      "  train_loss = -0.23869337514042854\n",
      "  train_ndcg@10 = 0.024389258585870266\n",
      "  train_ndcg@1000 = 0.06621946208178997\n",
      "  train_ndcg@20 = 0.027674706652760506\n",
      "  train_precision@10 = 0.0029541016556322575\n",
      "  train_precision@1000 = 0.0003478393628029153\n",
      "  train_precision@20 = 0.002093505929224193\n",
      "  train_recall@10 = 0.029541015625\n",
      "  train_recall@1000 = 0.34783935546875\n",
      "  train_recall@20 = 0.0418701171875\n",
      "  train_runtime = 2.8294\n",
      "  train_samples_per_second = 28953.247\n",
      "\n",
      "Evaluating on test set (time index:3)....\n",
      "\n",
      "***** eval results (time index): 3)*****\n",
      "\n",
      "\t  epoch = 1.0\n",
      "\t  eval_avg_precision@10 = 0.022967674769461155\n",
      "\t  eval_avg_precision@1000 = 0.025625487323850393\n",
      "\t  eval_avg_precision@20 = 0.023957342840731144\n",
      "\t  eval_loss = -0.23869337514042854\n",
      "\t  eval_mem_cpu_alloc_delta = 7688192\n",
      "\t  eval_mem_cpu_peaked_delta = 78712832\n",
      "\t  eval_mem_gpu_alloc_delta = 0\n",
      "\t  eval_mem_gpu_peaked_delta = 3761792000\n",
      "\t  eval_ndcg@10 = 0.024389258585870266\n",
      "\t  eval_ndcg@1000 = 0.06621946208178997\n",
      "\t  eval_ndcg@20 = 0.027674706652760506\n",
      "\t  eval_precision@10 = 0.0029541016556322575\n",
      "\t  eval_precision@1000 = 0.0003478393628029153\n",
      "\t  eval_precision@20 = 0.002093505929224193\n",
      "\t  eval_recall@10 = 0.029541015625\n",
      "\t  eval_recall@1000 = 0.34783935546875\n",
      "\t  eval_recall@20 = 0.0418701171875\n",
      "\t  eval_runtime = 3.031\n",
      "\t  eval_samples_per_second = 5405.442\n",
      "\n",
      "\n",
      "************* Training (time indices:3-3) *************\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Evaluation *************\n",
      "\n",
      "\n",
      "Evaluating on train set (time index:[3])....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all data has been set. Are you sure you passed all values?\n",
      "Not all data has been set. Are you sure you passed all values?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train results (time index): [3])*****\n",
      "\n",
      "  epoch = 1.0\n",
      "  eval_mem_cpu_alloc_delta = 1458176\n",
      "  eval_mem_cpu_peaked_delta = 85053440\n",
      "  eval_mem_gpu_alloc_delta = 0\n",
      "  eval_mem_gpu_peaked_delta = 3765396480\n",
      "  train_avg_precision@10 = 0.0015704996019069637\n",
      "  train_avg_precision@1000 = 0.0033315447425203665\n",
      "  train_avg_precision@20 = 0.0019161895948595234\n",
      "  train_loss = -0.306142053433827\n",
      "  train_ndcg@10 = 0.003178828828302877\n",
      "  train_ndcg@1000 = 0.03575688228011131\n",
      "  train_ndcg@20 = 0.004456988868436643\n",
      "  train_precision@10 = 0.0008893694529043776\n",
      "  train_precision@1000 = 0.0002638113801367581\n",
      "  train_precision@20 = 0.0006992885194319699\n",
      "  train_recall@10 = 0.008893694196428572\n",
      "  train_recall@1000 = 0.26381138392857145\n",
      "  train_recall@20 = 0.013985770089285714\n",
      "  train_runtime = 4.856\n",
      "  train_samples_per_second = 16869.803\n",
      "\n",
      "Evaluating on test set (time index:4)....\n",
      "\n",
      "***** eval results (time index): 4)*****\n",
      "\n",
      "\t  epoch = 1.0\n",
      "\t  eval_avg_precision@10 = 0.0015704996019069637\n",
      "\t  eval_avg_precision@1000 = 0.0033315447425203665\n",
      "\t  eval_avg_precision@20 = 0.0019161895948595234\n",
      "\t  eval_loss = -0.306142053433827\n",
      "\t  eval_mem_cpu_alloc_delta = -3489792\n",
      "\t  eval_mem_cpu_peaked_delta = 87867392\n",
      "\t  eval_mem_gpu_alloc_delta = 0\n",
      "\t  eval_mem_gpu_peaked_delta = 3765396480\n",
      "\t  eval_ndcg@10 = 0.003178828828302877\n",
      "\t  eval_ndcg@1000 = 0.03575688228011131\n",
      "\t  eval_ndcg@20 = 0.004456988868436643\n",
      "\t  eval_precision@10 = 0.0008893694529043776\n",
      "\t  eval_precision@1000 = 0.0002638113801367581\n",
      "\t  eval_precision@20 = 0.0006992885194319699\n",
      "\t  eval_recall@10 = 0.008893694196428572\n",
      "\t  eval_recall@1000 = 0.26381138392857145\n",
      "\t  eval_recall@20 = 0.013985770089285714\n",
      "\t  eval_runtime = 4.9377\n",
      "\t  eval_samples_per_second = 5806.72\n"
     ]
    }
   ],
   "source": [
    "from training import incremental_fit\n",
    "incremental_fit(trainer, start_time_window_index=1, final_time_window_index=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
