<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Model Architectures &mdash; Transformers4Rec  documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />
    <link rel="canonical" href="https://nvidia-merlin.github.io/Transformers4Rec/main/model_definition.html" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Training and Evaluation" href="training_eval.html" />
    <link rel="prev" title="Why Transformers4Rec?" href="why_transformers4rec.html" /> 
</head>

<body class="wy-body-for-nav">
  <div class="banner">
    <p class="banner">
      Beginning in January 2023, versions for all NVIDIA Merlin projects
      will change from semantic versioning like <code>4.0</code>
      to calendar versioning like <code>23.01</code>.</p>
  </div>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Transformers4Rec
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="README.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="why_transformers4rec.html">Why Transformers4Rec?</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Model Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="training_eval.html">Training and Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">End-to-End Pipeline with Hugging Face Transformers and NVIDIA Merlin</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_gpu_train.html">Multi-GPU data-parallel training using the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/modules.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="resources.html">Additional Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Transformers4Rec</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Model Architectures</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="tex2jax_ignore mathjax_ignore section" id="model-architectures">
<h1>Model Architectures<a class="headerlink" href="#model-architectures" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#modular-building-block-design" id="id1">Modular Building-Block Design</a></p></li>
<li><p><a class="reference internal" href="#feature-aggregation-input-block" id="id2">Feature Aggregation (Input Block)</a></p></li>
<li><p><a class="reference internal" href="#sequence-masking" id="id3">Sequence Masking</a></p></li>
<li><p><a class="reference internal" href="#sequence-processing-transformer-rnn-block" id="id4">Sequence Processing (Transformer/RNN Block)</a></p></li>
<li><p><a class="reference internal" href="#prediction-head-output-block" id="id5">Prediction Head (Output Block)</a></p>
<ul>
<li><p><a class="reference internal" href="#tying-embeddings" id="id6">Tying Embeddings</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#regularization" id="id7">Regularization</a></p></li>
</ul>
</div>
<div class="section" id="modular-building-block-design">
<h2>Modular Building-Block Design<a class="headerlink" href="#modular-building-block-design" title="Permalink to this headline"></a></h2>
<p>Transformers4Rec provides modularized building blocks that you can combine with standard PyTorch modules.
This provides a great flexibility in the model definition so that you can use these blocks to build custom architectures with multiple towers, multiple heads, and losses (multi-task).
For more information about the available options for each building block, refer to our <a class="reference internal" href="api/modules.html"><span class="doc std std-doc">API Documentation</span></a>.</p>
<p>The following figure shows a reference architecture for next-item prediction with Transformers.
The model can be used for both sequential and session-based recommendation.
This architecture can be divided into four conceptual layers:</p>
<ul class="simple">
<li><p>Feature aggregation (Input Block)</p></li>
<li><p>Sequence masking</p></li>
<li><p>Sequence processing (Transformer/RNN Block)</p></li>
<li><p>Prediction head (Output Block)</p></li>
</ul>
<p><img alt="Transformers4Rec meta-architecture" src="_images/transformers4rec_metaarchitecture.png" /><br></p>
<div style="text-align: center; margin: 20pt">
<figcaption style="font-style: italic;">Transformers4Rec meta-architecture</figcaption>
</div>
</div>
<div class="section" id="feature-aggregation-input-block">
<h2>Feature Aggregation (Input Block)<a class="headerlink" href="#feature-aggregation-input-block" title="Permalink to this headline"></a></h2>
<p>To read the sequences of input features like user IDs, user metadata, item IDs, and item metadata into a Transformer block, the sequences must be aggregated into a single vector representation per element in the sequence that we call the <strong>interaction embedding</strong>.</p>
<p>The following list identifies the aggregation methods:</p>
<ul class="simple">
<li><p><strong>Concat</strong>: Concatenation of the features.</p></li>
<li><p><strong>Element-wise sum</strong>: Features are summed in which all features must have the same dimension.
For example, categorical embeddings must have the same dimension and continuous features are projected to that dimension.</p></li>
<li><p><strong>Element-wise sum with item multiplication</strong>: Similar to the <em>Element-wise sum</em> aggregation in which all features are summed except for the item ID embedding because it is multiplied by the other features’ sum.
The aggregation formula is available in our <a class="reference external" href="https://dl.acm.org/doi/10.1145/3460231.3474255">Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation</a> paper.</p></li>
</ul>
<p>Categorical features are represented by embeddings.
Numerical features can be represented as a scalar and projected by a fully-connected (FC) layer to multiple dimensions or represented as a weighted average of embeddings by using the soft one-hot encoding technique.
For more information, refer to the online <a class="reference external" href="https://github.com/NVIDIA-Merlin/publications/blob/main/2021_acm_recsys_transformers4rec/Appendices/Appendix_A-Techniques_used_in_Transformers4Rec_Meta-Architecture.md">appendix</a> to the preceding paper.
Categorical input features are optionally normalized (with layer normalization) before aggregation.
Continuous features should be normalized during feature engineering of the dataset.</p>
<p><a class="reference internal" href="api/transformers4rec.torch.features.html#transformers4rec.torch.features.sequence.TabularSequenceFeatures" title="transformers4rec.torch.features.sequence.TabularSequenceFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">TabularSequenceFeatures</span></code></a> is the core class of this module.
This class processes and aggregates all features and outputs a sequence of <em>interaction embeddings</em> to be fed into transformer blocks.
You can create an instance of <code class="docutils literal notranslate"><span class="pre">TabularSequenceFeatures</span></code> automatically from a dataset schema that is generated from NVTabular by using the <a class="reference internal" href="api/transformers4rec.torch.features.html#transformers4rec.torch.features.sequence.TabularSequenceFeatures.from_schema" title="transformers4rec.torch.features.sequence.TabularSequenceFeatures.from_schema"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_schema()</span></code></a> method.
This method creates the layers that are required to represent the categorical and continuous features in the dataset.
In addition, you can specify the <code class="docutils literal notranslate"><span class="pre">aggregation</span></code> option of this method to aggregate the sequential features and to prepare masked labels according to the specified sequence masking approach.</p>
<p>The following code block shows one way the <code class="docutils literal notranslate"><span class="pre">TabularSequenceFeatures.from_schema()</span></code> method can create the interaction embeddings that are ready for use with Transformer blocks:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers4rec.torch</span> <span class="kn">import</span> <span class="n">TabularSequenceFeatures</span>
<span class="n">tabular_inputs</span> <span class="o">=</span> <span class="n">TabularSequenceFeatures</span><span class="o">.</span><span class="n">from_schema</span><span class="p">(</span>
        <span class="n">schema</span><span class="p">,</span>
        <span class="n">embedding_dim_default</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
        <span class="n">max_sequence_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">d_output</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;concat&quot;</span><span class="p">,</span>
        <span class="n">masking</span><span class="o">=</span><span class="s2">&quot;clm&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>The <code class="docutils literal notranslate"><span class="pre">embedding_dim_default</span></code> argument sets a fixed dimension for all categorical input features.
For more information, see the <a class="reference internal" href="api/transformers4rec.torch.features.html#transformers4rec.torch.features.sequence.TabularSequenceFeatures" title="transformers4rec.torch.features.sequence.TabularSequenceFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">TabularSequenceFeatures</span></code></a> class documentation.</p>
</div></blockquote>
</div>
<div class="section" id="sequence-masking">
<h2>Sequence Masking<a class="headerlink" href="#sequence-masking" title="Permalink to this headline"></a></h2>
<p>You can train Transformer architectures in different ways.
Depending on the training method, there is a specific masking schema.
The masking schema sets the items to be predicted–labels–and masks some positions of the sequence that cannot be used by the Transformer layers for prediction.</p>
<p>Transformers4Rec supports the following training approaches that are inspired by NLP:</p>
<ul class="simple">
<li><p><strong>Causal Language Modeling (<code class="docutils literal notranslate"><span class="pre">masking=&quot;clm&quot;</span></code>)</strong>: Predicts the next item based on past positions of the sequence.
Future positions are masked.</p></li>
<li><p><strong>Masked Language Modeling (<code class="docutils literal notranslate"><span class="pre">masking=&quot;mlm&quot;</span></code>)</strong>: Randomly selects some positions of the sequence to predict, which are masked.
The Transformer layer is allowed to use positions on the right–future information–during training.
During inference, all past items are visible for the Transformer layer as it tries to predict the next item.</p></li>
<li><p><strong>Permutation Language Modeling (<code class="docutils literal notranslate"><span class="pre">masking=&quot;plm&quot;</span></code>)</strong>: Uses a permutation factorization at the level of the self-attention layer to define the accessible bi-directional context.</p></li>
</ul>
<p><strong>NOTE</strong>: Not all transformer architectures support all of these training approaches.
Transformers4Rec raises an exception if you attempt to use an invalid combination and provides suggestions for using the appropriate masking techniques for that architecture.</p>
</div>
<div class="section" id="sequence-processing-transformer-rnn-block">
<h2>Sequence Processing (Transformer/RNN Block)<a class="headerlink" href="#sequence-processing-transformer-rnn-block" title="Permalink to this headline"></a></h2>
<p>The Transformer block processes the input sequences of <em>interaction embeddings</em> created by the input block using Transformer architectures like XLNet, GPT-2, and so on–or RNN architectures like LSTM or GRU.
The created block is a standard Torch block and is compatible with and substitutable by other Torch blocks that support the input of a sequence.</p>
<p>In the following example, a <a class="reference internal" href="api/transformers4rec.torch.block.html#transformers4rec.torch.block.base.SequentialBlock" title="transformers4rec.torch.block.base.SequentialBlock"><code class="xref py py-class docutils literal notranslate"><span class="pre">SequentialBlock</span></code></a> module is used to build the model body.
The model contains a <code class="docutils literal notranslate"><span class="pre">TabularSequenceFeatures</span></code> object (<code class="docutils literal notranslate"><span class="pre">tabular_inputs</span></code> defined in the previous code snippet), followed by an MLP projection layer to 64 dim (to match the Transformer <code class="docutils literal notranslate"><span class="pre">d_model</span></code>), and then is followed by an XLNet transformer block with two layers (four heads each).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers4rec.config</span> <span class="kn">import</span> <span class="n">transformer</span>
<span class="kn">from</span> <span class="nn">transformers4rec.torch</span> <span class="kn">import</span> <span class="n">MLPBlock</span><span class="p">,</span> <span class="n">SequentialBlock</span><span class="p">,</span> <span class="n">TransformerBlock</span>

<span class="c1"># Configures the XLNet Transformer architecture.</span>
<span class="n">transformer_config</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">XLNetConfig</span><span class="o">.</span><span class="n">build</span><span class="p">(</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_layer</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">total_seq_length</span><span class="o">=</span><span class="mi">20</span>
<span class="p">)</span>

<span class="c1"># Defines the model body including: inputs, masking, projection and transformer block.</span>
<span class="n">model_body</span> <span class="o">=</span> <span class="n">SequentialBlock</span><span class="p">(</span>
    <span class="n">tabular_inputs</span><span class="p">,</span>
    <span class="n">torch4rec</span><span class="o">.</span><span class="n">MLPBlock</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span>
    <span class="n">torch4rec</span><span class="o">.</span><span class="n">TransformerBlock</span><span class="p">(</span><span class="n">transformer_config</span><span class="p">,</span> <span class="n">masking</span><span class="o">=</span><span class="n">tabular_inputs</span><span class="o">.</span><span class="n">masking</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="prediction-head-output-block">
<h2>Prediction Head (Output Block)<a class="headerlink" href="#prediction-head-output-block" title="Permalink to this headline"></a></h2>
<p>Following the input and transformer blocks, the model outputs its predictions.
Transformers4Rec supports the following prediction heads, which can have multiple losses and can be combined for multi-task learning and multiple metrics:</p>
<ul class="simple">
<li><p><strong>Next Item Prediction</strong>: Predicts next items for a given sequence of interactions.
During training, the prediction can be the next item or randomly selected items depending on the masking scheme.
For inference, the intended purpose is to always predict the next interacted item.
Cross-entropy and pairwise losses are supported.</p></li>
<li><p><strong>Binary Classification</strong>: Predicts a binary feature using the whole sequence.
In the context of recommendation, you can use classification to predict the user’s next action such as whether the user will abandon a product in their cart or proceed with the purchase.</p></li>
<li><p><strong>Regression</strong>: Predicts a continuous feature using the whole sequence, such as the elapsed time until the user returns to a service.</p></li>
</ul>
<p>In the following example, a head is instantiated with the predefined <code class="docutils literal notranslate"><span class="pre">model_body</span></code> for the <a class="reference internal" href="api/transformers4rec.torch.html#transformers4rec.torch.NextItemPredictionTask" title="transformers4rec.torch.NextItemPredictionTask"><code class="xref py py-class docutils literal notranslate"><span class="pre">NextItemPredictionTask</span></code></a>.
This head enables the <code class="docutils literal notranslate"><span class="pre">weight_tying</span></code> option.
Decoupling the model bodies and heads provides a flexible architecture that enables you to define a model with features like multiple towers and multiple heads.
Lastly, the <a class="reference internal" href="api/transformers4rec.torch.html#transformers4rec.torch.Model" title="transformers4rec.torch.Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></a> class combines the heads and wraps the whole model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers4rec.torch</span> <span class="kn">import</span> <span class="n">Head</span><span class="p">,</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">transformers4rec.torch.model.head</span> <span class="kn">import</span> <span class="n">NextItemPredictionTask</span>

<span class="c1"># Defines the head related to next item prediction task</span>
<span class="n">head</span> <span class="o">=</span> <span class="n">Head</span><span class="p">(</span>
    <span class="n">model_body</span><span class="p">,</span>
    <span class="n">NextItemPredictionTask</span><span class="p">(</span><span class="n">weight_tying</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Get the end-to-end Model class</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">head</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="tying-embeddings">
<h3>Tying Embeddings<a class="headerlink" href="#tying-embeddings" title="Permalink to this headline"></a></h3>
<p>For the <a class="reference internal" href="api/transformers4rec.torch.html#transformers4rec.torch.NextItemPredictionTask" title="transformers4rec.torch.NextItemPredictionTask"><code class="xref py py-class docutils literal notranslate"><span class="pre">NextItemPredictionTask</span></code></a> class, we recommend <em>tying embeddings</em>.
The tying embeddings concept was initially proposed by the NLP community to tie the weights of the input (item ID) embedding matrix with the output projection layer.
Not only do tied embeddings reduce the memory requirements significantly, but our own experimentation during <a class="reference external" href="https://resources.nvidia.com/en-us-merlin/recommendation-syste?lx=97GH0Q">recent competitions</a> and empirical analysis detailed in our <a class="reference external" href="https://dl.acm.org/doi/10.1145/3460231.3474255">Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation</a> paper and online <a class="reference external" href="https://github.com/NVIDIA-Merlin/publications/blob/main/2021_acm_recsys_transformers4rec/Appendices/Appendix_A-Techniques_used_in_Transformers4Rec_Meta-Architecture.md">appendix</a> demonstrate that this method is very effective.
Tying embeddings is enabled by default, but can be disabled by setting <code class="docutils literal notranslate"><span class="pre">weight_tying</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</div>
</div>
<div class="section" id="regularization">
<h2>Regularization<a class="headerlink" href="#regularization" title="Permalink to this headline"></a></h2>
<p>Transformers4Rec supports a number of regularization techniques such as dropout, weight decay, softmax temperature scaling, stochastic shared embeddings, and label smoothing.
In our extensive experimentation, we hypertuned all regularization techniques for different datasets and found out that label smoothing was particularly useful at improving both training and validation accuracy and better at calibrating the predictions.</p>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="why_transformers4rec.html" class="btn btn-neutral float-left" title="Why Transformers4Rec?" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="training_eval.html" class="btn btn-neutral float-right" title="Training and Evaluation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NVJ1Y1YJHK"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NVJ1Y1YJHK', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>