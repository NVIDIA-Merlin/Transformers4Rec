<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Multi-GPU data-parallel training using the Trainer class &mdash; Transformers4Rec  documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/mystnb.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Transformers4Rec
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="README.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="why_transformers4rec.html">Why Transformers4Rec?</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_definition.html">Model Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="training_eval.html">Training and Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">End-to-end pipeline with NVIDIA Merlin</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/modules.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="resources.html">Additional Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Transformers4Rec</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Multi-GPU data-parallel training using the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="multi-gpu-data-parallel-training-using-the-trainer-class">
<h1>Multi-GPU data-parallel training using the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class<a class="headerlink" href="#multi-gpu-data-parallel-training-using-the-trainer-class" title="Permalink to this headline"></a></h1>
<p>To train models faster, users can use Data-Parallel training when using <code class="docutils literal notranslate"><span class="pre">transformers4rec.Trainer</span></code> class for training. When using the data-parallel approach for multi-GPU training, the training data will be distributed between the GPUs to speedup training and support larger batch sizes within each step of training.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class supports both <code class="docutils literal notranslate"><span class="pre">DataParallel</span></code> and <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> built-in features of PyTorch. in this documentation we will explain how each of these built-in features work and how they can be used.</p>
<div class="section" id="dataparallel">
<h2>DataParallel<a class="headerlink" href="#dataparallel" title="Permalink to this headline"></a></h2>
<p>When the DataParallel mode is used for training on each training step:</p>
<ul class="simple">
<li><p>GPU0 reads the full batch then evenly distributes it between available GPUs</p></li>
<li><p>The latest model will be copied to all GPUs</p></li>
<li><p>A Python thread will be assigned to running <code class="docutils literal notranslate"><span class="pre">forward()</span></code> on each GPU and thepartial loss will be sent to GPU0 to compute total loss</p></li>
<li><p>Computed total loss will be scattered between GPUs and threads will run <code class="docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
<li><p>Gradients from each GPU will be sent to GPU0 and their average will be computed</p></li>
</ul>
<p>As we see, parallelism in <code class="docutils literal notranslate"><span class="pre">DataParallel</span></code> mode is implemented through Python threads which will be blocked by GIL (Global Interepreter Lock) so <code class="docutils literal notranslate"><span class="pre">DataParallel</span></code> is not the preferred method. Users are advised to use <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> instead as it uses multi-processing instead of multi-threading and is better maintained. Also, some models types such as <code class="docutils literal notranslate"><span class="pre">transfoxl</span></code> can not be used with <code class="docutils literal notranslate"><span class="pre">DataParallel</span></code>. To learn more about <code class="docutils literal notranslate"><span class="pre">DataParalle</span></code> refer to <a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html">PyTorch documentations</a>.</p>
<p>To use the <code class="docutils literal notranslate"><span class="pre">DataParallel</span></code> mode training user just needs to make sure <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> is set. For example when 2 GPUs are available:</p>
<ul class="simple">
<li><p>Add <code class="docutils literal notranslate"><span class="pre">os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;0,1&quot;</span></code> to the script</p></li>
</ul>
<p>or</p>
<ul class="simple">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">CUDA_VISIBLE_DEVICES=0,1</span></code> in the terminal</p></li>
</ul>
<p>Do not try wrapping the model with <code class="docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code> yourself because that will break automatic wrapping done by <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>.
<b>Note:</b> When using <code class="docutils literal notranslate"><span class="pre">DataParallel</span></code> the dataloader generates one batch on each train step then the batch will be divided between GPUs so the <code class="docutils literal notranslate"><span class="pre">per_device_train_batch_size</span></code> argument represents the total batch size in this mode, not size of the batch each GPU receives.</p>
</div>
<div class="section" id="distributeddataparallel">
<h2>DistributedDataParallel<a class="headerlink" href="#distributeddataparallel" title="Permalink to this headline"></a></h2>
<p>This is the suggested and more efficient method. When a model is trained using the <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> mode:</p>
<ul class="simple">
<li><p>A eparate process will be assigned to each GPU in the beginning and GPU0 will replicate the model on each GPU</p></li>
<li><p>On each step each GPU will consume the separate mini-batch produced for it by the dataloader</p></li>
<li><p>On the backward pass the gradient from GPUs will be averaged for accumulation</p></li>
</ul>
<p>To learn more about <code class="docutils literal notranslate"><span class="pre">DistibutedDataParallel</span></code> see the <a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">PyTorch Documentation</a>.</p>
<p>To train using the <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> mode user should use PyTorch distributed launcher to run the script:</p>
<p><code class="docutils literal notranslate"> <span class="pre">python</span> <span class="pre">-m</span> <span class="pre">torch.distributed.launch</span> <span class="pre">--nproc_per_node</span> <span class="pre">N_GPU</span> <span class="pre">your_script.py</span> <span class="pre">--your_arguments</span></code></p>
<p>To have one process per GPU replace <code class="docutils literal notranslate"><span class="pre">N_GPU</span></code> with the number of GPUs you want to use and make sure <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> is set accordingly.</p>
<p><b>Note:</b> When using <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> number of partitions of the dataset must be equal or larger than number of processes. If a parquet file with small number of partitions is used, try repartitioning it using cudf or pandas before training. The dataloader checks <code class="docutils literal notranslate"><span class="pre">dataloader.dataset.npartitions</span></code> and will repartition if needed but we advise users to repartition the dataset and save it for better efficiency. Use pandas or cudf for repartitioning. Example of repartitioning a parquet file with cudf:</p>
<p><code class="docutils literal notranslate"><span class="pre">df.to_parquet(&quot;filename.parquet&quot;,</span> <span class="pre">row_group_size=10000,</span> <span class="pre">engine=&quot;pyarrow&quot;</span></code></p>
<p>Choose <code class="docutils literal notranslate"><span class="pre">row_group_size</span></code> such that <code class="docutils literal notranslate"><span class="pre">nr_rows/row_group_size&gt;=n_proc</span></code> because <code class="docutils literal notranslate"><span class="pre">n_rows=npartition*row_group_size</span></code>.</p>
</div>
<div class="section" id="performance-comparison">
<h2>Performance Comparison<a class="headerlink" href="#performance-comparison" title="Permalink to this headline"></a></h2>
<p>We trained and evaluated a number of models using single GPU, <code class="docutils literal notranslate"><span class="pre">DataParallel</span></code> and <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> training mode and the results are shown in the table below. To reproduce, use the models included in <a class="reference external" href="https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/ci/test_integration.sh">ci/test_integration.sh</a>.</p>
<p><img alt="Performance comparison of diffrerent training modes" src="_images/DP_DDP_perf.png" /><br></p>
<p>These experiments used a node with 2 <i>Tesla V100-SXM2-32GB-LS</i> GPUs.</p>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>