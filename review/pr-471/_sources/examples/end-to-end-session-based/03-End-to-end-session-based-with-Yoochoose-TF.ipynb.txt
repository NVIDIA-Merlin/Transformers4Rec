{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd57412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86956fb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# End-to-end session-based recommendation with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1564e6a",
   "metadata": {},
   "source": [
    "In recent years, several deep learning-based algorithms have been proposed for recommendation systems while its adoption in industry deployments have been steeply growing. In particular, NLP inspired approaches have been successfully adapted for sequential and session-based recommendation problems, which are important for many domains like e-commerce, news and streaming media. Session-Based Recommender Systems (SBRS) have been proposed to model the sequence of interactions within the current user session, where a session is a short sequence of user interactions typically bounded by user inactivity. They have recently gained popularity due to their ability to capture short-term or contextual user preferences towards items. \n",
    "\n",
    "The field of NLP has evolved significantly within the last decade, particularly due to the increased usage of deep learning. As a result, state of the art NLP approaches have inspired RecSys practitioners and researchers to adapt those architectures, especially for sequential and session-based recommendation problems. Here, we leverage one of the state-of-the-art Transformer-based architecture, [XLNet](https://arxiv.org/abs/1906.08237) with `Causal Language Modeling (CLM)` training technique. Causal LM is the task of predicting the token following a sequence of tokens, where the model only attends to the left context, i.e. models the probability of a token given the previous tokens in a sentence (Lample and Conneau, 2019).\n",
    "\n",
    "In this end-to-end-session-based recommnender model example, we use `Transformers4Rec` library, which leverages the popular [HuggingFaceâ€™s Transformers](https://github.com/huggingface/transformers) NLP library and make it possible to experiment with cutting-edge implementation of such architectures for sequential and session-based recommendation problems. For detailed explanations of the building blocks of Transformers4Rec meta-architecture visit [getting-started-session-based](https://github.com/NVIDIA-Merlin/Transformers4Rec/tree/main/examples/getting-started-session-based) and [tutorial](https://github.com/NVIDIA-Merlin/Transformers4Rec/tree/main/examples/tutorial) example notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97476ac8",
   "metadata": {},
   "source": [
    "## 1. Model definition using Transformers4Rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2016a0b",
   "metadata": {},
   "source": [
    "In the previous notebook, we have created sequential features and saved our processed data frames as parquet files, and now we use these processed parquet files to train a session-based recommendation model with XLNet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e7ce88",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56df859f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 20:25:03.688895: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 20:25:04.778884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cudf\n",
    "import numpy as np\n",
    "\n",
    "from nvtabular.loader.tensorflow import KerasSequenceLoader\n",
    "\n",
    "from transformers4rec import tf as tr\n",
    "from transformers4rec.tf.ranking_metric import NDCGAt, RecallAt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa080006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de336ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid numba warnings\n",
    "from numba import config\n",
    "config.CUDA_LOW_OCCUPANCY_WARNINGS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be73bd18",
   "metadata": {},
   "source": [
    "### 1.2 Get the schema "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c78f8",
   "metadata": {},
   "source": [
    "The library uses a schema format to configure the input features and automatically creates the necessary layers. This *protobuf* text file contains the description of each input feature by defining: the name, the type, the number of elements of a list column,  the cardinality of a categorical feature and the min and max values of each feature. In addition, the annotation field contains the tags such as specifying `continuous` and `categorical` features, the `target` column or the `item_id` feature, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8e2334",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"session_id\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"session_id\"\n",
      "    min: 1\n",
      "    max: 9249733 \n",
      "    is_categorical: false\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"groupby_col\"\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"item_id-list_seq\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 185\n",
      "  }\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"item_id/list\"\n",
      "    min: 1\n",
      "    max: 52742\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"item_id\"\n",
      "    tag: \"list\"\n",
      "    tag: \"categorical\"\n",
      "    tag: \"item\"\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"category-list_seq\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 185\n",
      "  }\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"category-list_seq\"\n",
      "    min: 1\n",
      "    max: 337\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"list\"\n",
      "    tag: \"categorical\"\n",
      "    tag: \"item\"\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"product_recency_days_log_norm-list_seq\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 185\n",
      "  }\n",
      "  type: FLOAT\n",
      "  float_domain {\n",
      "    name: \"product_recency_days_log_norm-list_seq\"\n",
      "    min: -2.9177291\n",
      "    max: 1.5231701\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"continuous\"\n",
      "    tag: \"list\"\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"et_dayofweek_sin-list_seq\"\n",
      "  value_count {\n",
      "    min: 2\n",
      "    max: 185\n",
      "  }\n",
      "  type: FLOAT\n",
      "  float_domain {\n",
      "    name: \"et_dayofweek_sin-list_seq\"\n",
      "    min: 0.7421683\n",
      "    max: 0.9995285\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"continuous\"\n",
      "    tag: \"time\"\n",
      "    tag: \"list\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from merlin_standard_lib import Schema\n",
    "SCHEMA_PATH = \"schema_demo.pb\"\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "!cat $SCHEMA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f872a657",
   "metadata": {},
   "source": [
    "We can select the subset of features we want to use for training the model by their tags or their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "531d21c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema.select_by_name(\n",
    "    ['item_id-list_seq', 'category-list_seq', 'product_recency_days_log_norm-list_seq', 'et_dayofweek_sin-list_seq']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96ddee",
   "metadata": {},
   "source": [
    "### 3.2 Define the end-to-end Session-based Transformer-based recommendation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf235cdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "For session-based recommendation model definition, the end-to-end model definition requires four steps:\n",
    "\n",
    "1. Instantiate [TabularSequenceFeatures](https://nvidia-merlin.github.io/Transformers4Rec/main/api/transformers4rec.tf.features.html?highlight=tabularsequence#transformers4rec.tf.features.sequence.TabularSequenceFeatures) input-module from schema to prepare the embedding tables of categorical variables and project continuous features, if specified. In addition, the module provides different aggregation methods (e.g. 'concat', 'elementwise-sum') to merge input features and generate the sequence of interactions embeddings. The module also supports language modeling tasks to prepare masked labels for training and evaluation (e.g: 'clm' for causal language modeling).\n",
    "\n",
    "2. Next, we need to define one or multiple prediction tasks. For this demo, we are going to use [NextItemPredictionTask](https://nvidia-merlin.github.io/Transformers4Rec/main/api/transformers4rec.tf.model.html?highlight=nextitem#transformers4rec.tf.model.prediction_task.NextItemPredictionTask) with `Causal Language modeling (CLM)`.\n",
    "\n",
    "3. Then we construct a `transformer_config` based on the architectures provided by [Hugging Face Transformers](https://github.com/huggingface/transformers) framework. </a>\n",
    "\n",
    "4. Finally we link the transformer-body to the inputs and the prediction tasks to get the final Tensorflow `Model` class.\n",
    "    \n",
    "For more details about the features supported by each sub-module, please check out the library [documentation](https://nvidia-merlin.github.io/Transformers4Rec/main/index.html) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25eecd93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_sequence_length, d_model = 20, 320\n",
    "\n",
    "# Define the evaluation top-N metrics and the cut-offs\n",
    "metrics = [\n",
    "    NDCGAt(top_ks=[10, 20], labels_onehot=True), \n",
    "    RecallAt(top_ks=[10, 20], labels_onehot=True)\n",
    "]\n",
    "\n",
    "# Define input module to process tabular input-features and to prepare masked inputs\n",
    "input_module = tr.TabularSequenceFeatures.from_schema(\n",
    "    schema,\n",
    "    max_sequence_length=max_sequence_length,\n",
    "    continuous_projection=64,\n",
    "    aggregation=\"concat\",\n",
    "    d_output=d_model,\n",
    "    masking=\"clm\",\n",
    ")\n",
    "\n",
    "# Define Next item prediction-task \n",
    "prediction_task = tr.NextItemPredictionTask(weight_tying=True, metrics=metrics)\n",
    "\n",
    "# Define the config of the XLNet Transformer architecture\n",
    "transformer_config = tr.XLNetConfig.build(\n",
    "    d_model=d_model, n_head=8, n_layer=2, total_seq_length=max_sequence_length\n",
    ")\n",
    "\n",
    "# Get the end-to-end model\n",
    "model = transformer_config.to_tf_model(input_module, prediction_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a973cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb4c72",
   "metadata": {},
   "source": [
    "### 3.3. Daily Fine-Tuning: Training over a time windowÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e976be9",
   "metadata": {},
   "source": [
    "Now that the model is defined, we are now going to launch training. In this example, we will conduct a time-based finetuning by iteratively training and evaluating using a sliding time window: At each iteration, we use training data of a specific time index $t$ to train the model then we evaluate on the validation data of next index $t + 1$. Particularly, we set start time to 178 and end time to 180. Note that, we are using tf.keras' `model.fit()` and `model.evaluate()` methods, where we train the model with model.fit(), and evaluate it with model.evaluate()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a532a",
   "metadata": {},
   "source": [
    "#### Sets DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cf59aa",
   "metadata": {},
   "source": [
    "We use the NVTabular `KerasSequenceLoader` Dataloader for optimized loading of multiple features from input parquet files. In our experiments, we see a speed-up by 9x of the same training workflow with NVTabular dataloader. You can learn more about this data loader [here](https://nvidia-merlin.github.io/NVTabular/main/training/tensorflow.html) and [here](https://medium.com/nvidia-merlin/training-deep-learning-based-recommender-systems-9x-faster-with-tensorflow-cc5a2572ea49)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc95962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and continuous columns\n",
    "x_cat_names = ['item_id-list_seq', 'category-list_seq']\n",
    "x_cont_names = ['product_recency_days_log_norm-list_seq', 'et_dayofweek_sin-list_seq']\n",
    "\n",
    "# dictionary representing max sequence length for each column\n",
    "sparse_features_max = {\n",
    "    fname: 20\n",
    "    for fname in x_cat_names + x_cont_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dda067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(paths_or_dataset, batch_size=384):\n",
    "    dataloader = KerasSequenceLoader(\n",
    "        paths_or_dataset,\n",
    "        batch_size=batch_size,\n",
    "        label_names=None,\n",
    "        cat_names=x_cat_names,\n",
    "        cont_names=x_cont_names,\n",
    "        sparse_names=list(sparse_features_max.keys()),\n",
    "        sparse_max=sparse_features_max,\n",
    "        sparse_as_dense=True,\n",
    "    )\n",
    "    return dataloader.map(lambda X, y: (X, []))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f5ecb",
   "metadata": {},
   "source": [
    "The reason we set the targets to [] in the data-loader because the true item labels are computed internally by the `MaskSequence` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92cf595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "# set it to True if to run the model eagerly\n",
    "model.compile(optimizer=opt, run_eagerly=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07941837",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", './preproc_sessions_by_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a45e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training for day 178 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 20:25:15.775103: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8204\n",
      "2021-12-06 20:25:15.920777: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "75/75 [==============================] - 28s 231ms/step - train_ndcg@10: 0.0650 - train_ndcg@20: 0.0712 - train_recall@10: 0.0931 - train_recall@20: 0.1178 - loss: 9.2902 - regularization_loss: 0.0000e+00 - total_loss: 9.2902\n",
      "Epoch 2/5\n",
      "75/75 [==============================] - 20s 229ms/step - train_ndcg@10: 0.5058 - train_ndcg@20: 0.5179 - train_recall@10: 0.5796 - train_recall@20: 0.6275 - loss: 3.9481 - regularization_loss: 0.0000e+00 - total_loss: 3.9481\n",
      "Epoch 3/5\n",
      "75/75 [==============================] - 20s 229ms/step - train_ndcg@10: 0.6742 - train_ndcg@20: 0.6805 - train_recall@10: 0.7266 - train_recall@20: 0.7520 - loss: 2.6460 - regularization_loss: 0.0000e+00 - total_loss: 2.6460\n",
      "Epoch 4/5\n",
      "75/75 [==============================] - 20s 226ms/step - train_ndcg@10: 0.7045 - train_ndcg@20: 0.7107 - train_recall@10: 0.7525 - train_recall@20: 0.7773 - loss: 2.3314 - regularization_loss: 0.0000e+00 - total_loss: 2.3314\n",
      "Epoch 5/5\n",
      "75/75 [==============================] - 20s 226ms/step - train_ndcg@10: 0.7167 - train_ndcg@20: 0.7237 - train_recall@10: 0.7655 - train_recall@20: 0.7930 - loss: 2.1691 - regularization_loss: 0.0000e+00 - total_loss: 2.1691\n",
      "6/6 [==============================] - 4s 276ms/step - eval_ndcg@10: 0.5293 - eval_ndcg@20: 0.5330 - eval_recall@10: 0.5957 - eval_recall@20: 0.6103 - loss: 3.9674 - regularization_loss: 0.0000e+00 - total_loss: 3.9674\n",
      "********************\n",
      "Eval results for day 179 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_ndcg@10 = 0.16843447089195251\n",
      " eval_ndcg@20 = 0.16843447089195251\n",
      " eval_recall@10 = 0.3142857253551483\n",
      " eval_recall@20 = 0.3142857253551483\n",
      " loss = 6.932893753051758\n",
      " regularization_loss = 0\n",
      " total_loss = 6.932893753051758\n",
      "********************\n",
      "Launch training for day 179 are:\n",
      "********************\n",
      "\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 15s 234ms/step - train_ndcg@10: 0.6677 - train_ndcg@20: 0.6739 - train_recall@10: 0.7088 - train_recall@20: 0.7331 - loss: 2.8084 - regularization_loss: 0.0000e+00 - total_loss: 2.8084\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 15s 235ms/step - train_ndcg@10: 0.7027 - train_ndcg@20: 0.7093 - train_recall@10: 0.7493 - train_recall@20: 0.7751 - loss: 2.4409 - regularization_loss: 0.0000e+00 - total_loss: 2.4409\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 15s 234ms/step - train_ndcg@10: 0.7271 - train_ndcg@20: 0.7339 - train_recall@10: 0.7733 - train_recall@20: 0.8000 - loss: 2.2137 - regularization_loss: 0.0000e+00 - total_loss: 2.2137\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 15s 231ms/step - train_ndcg@10: 0.7432 - train_ndcg@20: 0.7493 - train_recall@10: 0.7904 - train_recall@20: 0.8145 - loss: 2.0358 - regularization_loss: 0.0000e+00 - total_loss: 2.0358\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 15s 240ms/step - train_ndcg@10: 0.7558 - train_ndcg@20: 0.7619 - train_recall@10: 0.8051 - train_recall@20: 0.8291 - loss: 1.8771 - regularization_loss: 0.0000e+00 - total_loss: 1.8771\n",
      "5/5 [==============================] - 2s 303ms/step - eval_ndcg@10: 0.5249 - eval_ndcg@20: 0.5308 - eval_recall@10: 0.5637 - eval_recall@20: 0.5873 - loss: 4.5000 - regularization_loss: 0.0000e+00 - total_loss: 4.5000\n",
      "********************\n",
      "Eval results for day 180 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_ndcg@10 = 0.12385287880897522\n",
      " eval_ndcg@20 = 0.13401004672050476\n",
      " eval_recall@10 = 0.1855670064687729\n",
      " eval_recall@20 = 0.22680412232875824\n",
      " loss = 8.349146842956543\n",
      " regularization_loss = 0\n",
      " total_loss = 8.349146842956543\n"
     ]
    }
   ],
   "source": [
    "start_time_window_index = 178\n",
    "final_time_window_index = 180\n",
    "# Iterating over days of one week\n",
    "for time_index in range(start_time_window_index, final_time_window_index):\n",
    "    # Set data\n",
    "    time_index_train = time_index\n",
    "    time_index_eval = time_index + 1\n",
    "    train_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_train}/train.parquet\"))\n",
    "    eval_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_eval}/valid.parquet\"))\n",
    "\n",
    "    # Train on day related to time_index \n",
    "    print('*' * 20)\n",
    "    print(\"Launch training for day %s are:\" %time_index)\n",
    "    print('*' * 20 + '\\n')\n",
    "    train_loader = get_dataloader(train_paths, batch_size=384)\n",
    "    losses = model.fit(train_loader, epochs=5)\n",
    "    model.reset_metrics()\n",
    "    # Evaluate on the following day\n",
    "    eval_loader = get_dataloader(eval_paths, batch_size=512)\n",
    "    eval_metrics = model.evaluate(eval_loader, return_dict=True)\n",
    "    print('*'*20)\n",
    "    print(\"Eval results for day %s are:\\t\" %time_index_eval)\n",
    "    print('\\n' + '*' * 20 + '\\n')\n",
    "    for key in sorted(eval_metrics.keys()):\n",
    "        print(\" %s = %s\" % (key, str(eval_metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f39137",
   "metadata": {},
   "source": [
    "#### Exports the preprocessing workflow and model in the format required by Triton server:** \n",
    "\n",
    "NVTabularâ€™s `export_tensorflow_ensemble()` function enables us to create model files and config files to be served to Triton Inference Server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cece87c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvtabular as nvt\n",
    "workflow = nvt.Workflow.load('workflow_etl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35eee6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvtabular/nvtabular/inference/triton/ensemble.py:80: UserWarning: TF model expects int64 for column category-list_seq, but workflow  is producing type list. Overriding dtype in NVTabular workflow.\n",
      "  warnings.warn(\n",
      "/nvtabular/nvtabular/inference/triton/ensemble.py:80: UserWarning: TF model expects float32 for column et_dayofweek_sin-list_seq, but workflow  is producing type list. Overriding dtype in NVTabular workflow.\n",
      "  warnings.warn(\n",
      "/nvtabular/nvtabular/inference/triton/ensemble.py:80: UserWarning: TF model expects int64 for column item_id-list_seq, but workflow  is producing type list. Overriding dtype in NVTabular workflow.\n",
      "  warnings.warn(\n",
      "/nvtabular/nvtabular/inference/triton/ensemble.py:80: UserWarning: TF model expects float32 for column product_recency_days_log_norm-list_seq, but workflow  is producing type list. Overriding dtype in NVTabular workflow.\n",
      "  warnings.warn(\n",
      "/nvtabular/nvtabular/inference/triton/ensemble.py:279: UserWarning: Column session_id is being generated by NVTabular workflow  but is unused in t4r_tf_tf model\n",
      "  warnings.warn(\n",
      "/nvtabular/nvtabular/inference/triton/ensemble.py:279: UserWarning: Column item_id-count is being generated by NVTabular workflow  but is unused in t4r_tf_tf model\n",
      "  warnings.warn(\n",
      "/nvtabular/nvtabular/inference/triton/ensemble.py:279: UserWarning: Column day_index is being generated by NVTabular workflow  but is unused in t4r_tf_tf model\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from nvtabular.inference.triton import export_tensorflow_ensemble\n",
    "export_tensorflow_ensemble(\n",
    "    model,\n",
    "    workflow,\n",
    "    name=\"t4r_tf\",\n",
    "    model_path='/workspace/TF4Rec/models/tf/',\n",
    "    label_columns=[],\n",
    "    sparse_max=sparse_features_max\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748fb2f",
   "metadata": {},
   "source": [
    "## 4. Serving Ensemble Model to the Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de119ce2",
   "metadata": {},
   "source": [
    "NVIDIA [Triton Inference Server (TIS)](https://github.com/triton-inference-server/server) simplifies the deployment of AI models at scale in production. TIS provides a cloud and edge inferencing solution optimized for both CPUs and GPUs. It supports a number of different machine learning frameworks such as TensorFlow and PyTorch.\n",
    "\n",
    "The last step of machine learning (ML)/deep learning (DL) pipeline is to deploy the ETL workflow and saved model to production. In the production setting, we want to transform the input data as done during training (ETL). We need to apply the same mean/std for continuous features and use the same categorical mapping to convert the categories to continuous integer before we use the DL model for a prediction. Therefore, we deploy the NVTabular workflow with the Tensorflow model as an ensemble model to Triton Inference. The ensemble model guarantees that the same transformation is applied to the raw inputs.\n",
    "\n",
    "\n",
    "In this section, you will learn how to\n",
    "- to deploy saved NVTabular and Tensorflow models to Triton Inference Server \n",
    "- send requests for predictions and get responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7d4c56",
   "metadata": {},
   "source": [
    "### 4.1. Pull and Start Inference Container\n",
    "\n",
    "At this point, we launch the Triton Server, and we will load the ensemble `t4r_tf` to the inference server below. \n",
    "\n",
    "**Start triton server**<br>\n",
    "You can start triton server with the command below. You need to provide correct path of the models folder.\n",
    "\n",
    "```\n",
    "tritonserver --model-repository=<path_to_models> --backend-config=tensorflow,version=2 --model-control-mode=explicit\n",
    "```\n",
    "Note: The model-repository path for our example is `/workspace/TF4Rec/models/tf/`. The models haven't been loaded, yet. We will request the Triton server to load the saved ensemble model, below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731181e1",
   "metadata": {},
   "source": [
    "### Connect to the Triton Inference Server and check if the server is alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63466ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n",
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tritonhttpclient/__init__.py:31: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tritonhttpclient\n",
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))\n",
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9244c98e",
   "metadata": {},
   "source": [
    "### Load raw data for inference\n",
    "We select the last 50 interactions and filter out sessions with less than 2 interactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f097fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_merged_df = cudf.read_parquet('/workspace/data/interactions_merged_df.parquet')\n",
    "interactions_merged_df = interactions_merged_df.sort_values('timestamp')\n",
    "batch = interactions_merged_df[-50:]\n",
    "sessions_to_use = batch.session_id.value_counts()\n",
    "# ignore sessions with less than 2 interactions\n",
    "filtered_batch = batch[batch.session_id.isin(sessions_to_use[sessions_to_use.values > 1].index.values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad3a3b4",
   "metadata": {},
   "source": [
    "### Send the request to triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d318e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '62'}>\n",
      "bytearray(b'[{\"name\":\"t4r_tf\"},{\"name\":\"t4r_tf_nvt\"},{\"name\":\"t4r_tf_tf\"}]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 't4r_tf'}, {'name': 't4r_tf_nvt'}, {'name': 't4r_tf_tf'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef341a3c",
   "metadata": {},
   "source": [
    "### Load the ensemble model to triton\n",
    "If all models are loaded successfully, you should be seeing `successfully loaded` status next to each model name on your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2bbc0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST /v2/repository/models/t4r_tf/load, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 't4r_tf'\n"
     ]
    }
   ],
   "source": [
    "triton_client.load_model(model_name=\"t4r_tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5d45bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_1 :\n",
      " [[-23.45367  -20.654154 -12.082266 ... -19.95935  -17.551102 -21.46679 ]\n",
      " [-15.886233 -16.107304  -5.136529 ... -15.80361  -15.616753 -14.409036]\n",
      " [-20.026276 -16.286356  -9.805498 ... -17.55959  -15.639177 -17.18394 ]\n",
      " ...\n",
      " [-20.121122 -16.542429  -5.615509 ... -15.828735 -16.362495 -19.196321]\n",
      " [-19.131699 -16.101086  -6.773807 ... -16.170242 -15.505514 -15.895823]\n",
      " [-16.898193 -15.042047  -7.174122 ... -14.768758 -16.249992 -15.368987]]\n"
     ]
    }
   ],
   "source": [
    "import nvtabular.inference.triton as nvt_triton\n",
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "inputs = nvt_triton.convert_df_to_triton_input(filtered_batch.columns, filtered_batch, grpcclient.InferInput)\n",
    "\n",
    "output_names = [\"output_1\"]\n",
    "\n",
    "outputs = []\n",
    "for col in output_names:\n",
    "    outputs.append(grpcclient.InferRequestedOutput(col))\n",
    "\n",
    "MODEL_NAME_NVT = \"t4r_tf\"\n",
    "\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(MODEL_NAME_NVT, inputs)\n",
    "    print(col, ':\\n', response.as_numpy(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44ef3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_response(batch, response, top_k, session_col=\"session_id\"):\n",
    "    \"\"\"\n",
    "    Util function to extract top-k encoded item-ids from logits\n",
    "    Parameters\n",
    "    \"\"\"\n",
    "    sessions = batch[session_col].drop_duplicates().values\n",
    "    predictions = response.as_numpy(\"output_1\")\n",
    "    top_preds = np.argpartition(predictions, -top_k, axis=1)[:, -top_k:]\n",
    "    for session, next_items in zip(sessions, top_preds):\n",
    "        print(\n",
    "            \"- Top-%s predictions for session `%s`: %s\\n\"\n",
    "            % (top_k, session, \" || \".join([str(e) for e in next_items]))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7944bff0",
   "metadata": {},
   "source": [
    "- Visualise top-k predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2b68b72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Top-5 predictions for session `11257991`: 10790 || 202 || 5168 || 9334 || 4111\n",
      "\n",
      "- Top-5 predictions for session `11270119`: 1697 || 18021 || 101 || 2 || 5856\n",
      "\n",
      "- Top-5 predictions for session `11311424`: 6988 || 8622 || 5064 || 1297 || 6603\n",
      "\n",
      "- Top-5 predictions for session `11336059`: 28411 || 6607 || 19414 || 1435 || 2259\n",
      "\n",
      "- Top-5 predictions for session `11394056`: 6510 || 8769 || 3590 || 18021 || 5856\n",
      "\n",
      "- Top-5 predictions for session `11399751`: 1470 || 2336 || 7613 || 1853 || 1657\n",
      "\n",
      "- Top-5 predictions for session `11401481`: 28 || 1436 || 5892 || 1229 || 2196\n",
      "\n",
      "- Top-5 predictions for session `11421333`: 5168 || 9334 || 3233 || 2336 || 3632\n",
      "\n",
      "- Top-5 predictions for session `11425751`: 2336 || 4541 || 18021 || 74 || 7613\n",
      "\n",
      "- Top-5 predictions for session `11445777`: 2888 || 2016 || 1342 || 184 || 664\n",
      "\n",
      "- Top-5 predictions for session `11457123`: 1853 || 7613 || 13292 || 1470 || 25106\n",
      "\n",
      "- Top-5 predictions for session `11467406`: 5168 || 745 || 2844 || 8769 || 2375\n",
      "\n",
      "- Top-5 predictions for session `11493827`: 366 || 1294 || 18021 || 2067 || 2336\n",
      "\n",
      "- Top-5 predictions for session `11528554`: 540 || 4067 || 2034 || 7613 || 500\n",
      "\n",
      "- Top-5 predictions for session `11561822`: 771 || 6607 || 1306 || 5958 || 18021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualize_response(filtered_batch, response, top_k=5, session_col='session_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2633720",
   "metadata": {},
   "source": [
    "As you noticed, we first got prediction results (logits) from the trained model head, and then by using a handy util function `visualize_response` we extracted top-k encoded item-ids from logits. Basically, we generated recommended items for a given session.\n",
    "\n",
    "This is the end of these example notebooks. You successfully\n",
    "\n",
    "- performed feature engineering with NVTabular\n",
    "- trained transformer architecture based session-based recommendation models with Transformers4Rec\n",
    "- deployed a trained model to Triton Inference Server, sent request and got responses from the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859877c",
   "metadata": {},
   "source": [
    "**Unload models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c61b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client.unload_model(model_name=\"t4r_tf\")\n",
    "triton_client.unload_model(model_name=\"t4r_tf_nvt\")\n",
    "triton_client.unload_model(model_name=\"t4r_tf_tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb9dc23",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c5274",
   "metadata": {},
   "source": [
    "- Merlin Transformers4rec: https://github.com/NVIDIA-Merlin/Transformers4Rec\n",
    "- Merlin NVTabular: https://github.com/NVIDIA-Merlin/NVTabular/tree/main/nvtabular\n",
    "- Triton inference server: https://github.com/triton-inference-server\n",
    "- Guillaume Lample, and Alexis Conneau. \"Cross-lingual language model pretraining.\" arXiv preprint arXiv:1901.07291"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b543a88d374ac88bf8df97911b380f671b13649694a5b49eb21e60fd27eb479"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
