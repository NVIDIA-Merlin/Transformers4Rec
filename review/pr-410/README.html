<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Transformers4Rec &mdash; Transformers4Rec  documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/mystnb.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Why Transformers4Rec?" href="why_transformers4rec.html" />
    <link rel="prev" title="Merlin Transformers4Rec" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Transformers4Rec
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#highlights">Highlights</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quick-tour">Quick tour</a></li>
<li class="toctree-l2"><a class="reference internal" href="#use-cases">Use cases</a></li>
<li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="why_transformers4rec.html">Why Transformers4Rec?</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_definition.html">Model Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="training_eval.html">Training and Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">End-to-end pipeline with NVIDIA Merlin</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/modules.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="resources.html">Additional Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Transformers4Rec</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Transformers4Rec</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="tex2jax_ignore mathjax_ignore section" id="transformers4rec">
<h1><a class="reference external" href="https://github.com/NVIDIA-Merlin/Transformers4Rec/">Transformers4Rec</a><a class="headerlink" href="#transformers4rec" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://pypi.python.org/pypi/Transformers4Rec"><img alt="PyPI" src="https://img.shields.io/pypi/v/Transformers4Rec?color=orange&amp;label=version" /></a>
<a class="reference external" href="https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/LICENSE"><img alt="LICENSE" src="https://img.shields.io/github/license/NVIDIA-Merlin/Transformers4Rec" /></a>
<a class="reference external" href="https://nvidia-merlin.github.io/Transformers4Rec/main/README.html"><img alt="Documentation" src="https://img.shields.io/badge/documentation-blue.svg" /></a></p>
<p>Transformers4Rec is a flexible and efficient library for sequential and session-based recommendation, available for both PyTorch and Tensorflow.</p>
<p>It works as a bridge between NLP and recommender systems by integrating with one the most popular NLP frameworks <a class="reference external" href="https://github.com/huggingface/transformers">HuggingFace Transformers</a>, making state-of-the-art Transformer architectures available for RecSys researchers and industry practitioners.</p>
<div style="text-align: center; margin: 20pt"><img src="_images/sequential_rec.png" alt="Sequential and Session-based recommendation with Transformers4Rec" style="width:800px;"/><br><figcaption style="font-style: italic;">Sequential and Session-based recommendation with Transformers4Rec</figcaption></div>
<p>Transformers4Rec supports multiple input features and provides configurable building blocks that can be easily combined for custom architectures.</p>
<p>You can build a fully GPU-accelerated pipeline for sequential and session-based recommendation with Transformers4Rec and its smooth integration with other components of <a class="reference external" href="https://developer.nvidia.com/nvidia-merlin">NVIDIA Merlin</a>:  <a class="reference external" href="https://github.com/NVIDIA-Merlin/NVTabular">NVTabular</a> for preprocessing and <a class="reference external" href="https://github.com/triton-inference-server/server">Triton Inference Server</a>.</p>
<div class="section" id="highlights">
<h2>Highlights<a class="headerlink" href="#highlights" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>Winning and SOTA solution</strong>: We have leveraged and evolved the Transformers4Rec library to win two recent session-based recommendation competitions: the <a class="reference external" href="https://developer.nvidia.com/blog/how-to-build-a-winning-deep-learning-powered-recommender-system-part-3/">WSDM WebTour Workshop Challenge 2021, organized by Booking.com</a>, and the <a class="reference external" href="https://medium.com/nvidia-merlin/winning-the-sigir-ecommerce-challenge-on-session-based-recommendation-with-transformers-v2-793f6fac2994">SIGIR eCommerce Workshop Data Challenge 2021, organized by Coveo</a>. Furthermore, we have also done extensive empirical evaluation on the usage of Transformers4Rec for session-based recommendation, which was able to provide higher accuracy than baselines algorithms, as published in our <a class="reference external" href="https://dl.acm.org/doi/10.1145/3460231.3474255">ACM RecSys’21 paper</a>.</p></li>
<li><p><strong>Flexibility</strong>: The building blocks are modularized and are compatible with vanilla PyTorch modules and TF Keras layers. You can create custom architectures, e.g. with multiple towers, multiple heads/tasks and losses.</p></li>
<li><p><strong>Production-ready</strong>: Exports trained models to serve with Triton Inference Server in a single pipeline that includes online features preprocessing and model inference.</p></li>
<li><p><strong>Leverages cutting-edge NLP research</strong>: With the integration with <a class="reference external" href="https://github.com/huggingface/transformers">HuggingFace Transformers</a>, you have available more than 64 different Transformer architectures (and counting) to evaluate for your sequential and session-based recommendation task.</p></li>
<li><p><strong>Support for multiple input features</strong>: HF Transformers supports only sequence of token id as input, as it was originally designed for NLP. Due to the rich features available in RecSys datasets, transformers4Rec enables the usage of HF Transformers with any type of sequential tabular data. The library uses a schema format to configure the input features, and automatically creates the necessary layers (e.g. embedding tables, projection layers, output layers based on the target) without requiring code changes to include new features. Interaction and sequence-level input features can be normalized and combined in configurable ways.</p></li>
<li><p><strong>Seamless preprocessing and feature engineering</strong>: The integration with NVTabular has common preprocessing ops for session-based recommendation and exports a dataset schema compatible with Transformers4Rec, so that input features can be configured automatically.</p></li>
</ul>
<div style="text-align: center; margin: 20pt"><img src="_images/pipeline.png" alt="GPU-accelerated Sequential and Session-based recommendation" style="width:600px;"/><br><figcaption style="font-style: italic;">GPU-accelerated pipeline for Sequential and Session-based recommendation using NVIDIA Merlin components</figcaption></div>
</div>
<div class="section" id="quick-tour">
<h2>Quick tour<a class="headerlink" href="#quick-tour" title="Permalink to this headline"></a></h2>
<p>To train a model on a dataset, the first step is to provide the <a class="reference external" href="https://nvidia-merlin.github.io/Transformers4Rec/main/api/merlin_standard_lib.schema.html#merlin_standard_lib.schema.schema.Schema">schema</a> and use this to construct an input-module.
For session-based recommendation problems you typically want to use <a class="reference external" href="https://nvidia-merlin.github.io/Transformers4Rec/main/api/transformers4rec.torch.features.html#transformers4rec.torch.features.sequence.TabularSequenceFeatures">TabularSequenceFeatures</a>, which
merges context features with sequential features. Next, we need to provide the prediction-task(s)
(the tasks we provide out of the box can be found <a class="reference external" href="https://nvidia-merlin.github.io/Transformers4Rec/main/api/transformers4rec.torch.model.html#module-transformers4rec.torch.model.prediction_task">here</a>).
Then all that’s left is to construct a transformer-body and convert this to a model.</p>
<p>Here is the PyTorch version:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers4rec</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">tr</span>

<span class="n">schema</span><span class="p">:</span> <span class="n">tr</span><span class="o">.</span><span class="n">Schema</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">tabular_sequence_testing_data</span><span class="o">.</span><span class="n">schema</span>
<span class="c1"># Or read schema from disk: tr.Schema().from_json(SCHEMA_PATH)</span>
<span class="n">max_sequence_length</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">64</span>

<span class="c1"># Define input module to process tabular input-features</span>
<span class="n">input_module</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">TabularSequenceFeatures</span><span class="o">.</span><span class="n">from_schema</span><span class="p">(</span>
    <span class="n">schema</span><span class="p">,</span>
    <span class="n">max_sequence_length</span><span class="o">=</span><span class="n">max_sequence_length</span><span class="p">,</span>
    <span class="n">continuous_projection</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
    <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;concat&quot;</span><span class="p">,</span>
    <span class="n">masking</span><span class="o">=</span><span class="s2">&quot;causal&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Define one or multiple prediction-tasks</span>
<span class="n">prediction_tasks</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">NextItemPredictionTask</span><span class="p">()</span>

<span class="c1"># Define a transformer-config, like the XLNet architecture</span>
<span class="n">transformer_config</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">XLNetConfig</span><span class="o">.</span><span class="n">build</span><span class="p">(</span>
    <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_layer</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">total_seq_length</span><span class="o">=</span><span class="n">max_sequence_length</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">:</span> <span class="n">tr</span><span class="o">.</span><span class="n">Model</span> <span class="o">=</span> <span class="n">transformer_config</span><span class="o">.</span><span class="n">to_torch_model</span><span class="p">(</span><span class="n">input_module</span><span class="p">,</span> <span class="n">prediction_tasks</span><span class="p">)</span>
</pre></div>
</div>
<p>And here is the equivalent code for TensorFlow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers4rec</span> <span class="kn">import</span> <span class="n">tf</span> <span class="k">as</span> <span class="n">tr</span>

<span class="n">schema</span><span class="p">:</span> <span class="n">tr</span><span class="o">.</span><span class="n">Schema</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">tabular_sequence_testing_data</span><span class="o">.</span><span class="n">schema</span>
<span class="c1"># Or read schema from disk: tr.Schema().from_json(SCHEMA_PATH)</span>
<span class="n">max_sequence_length</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">64</span>

<span class="c1"># Define input module to process tabular input-features</span>
<span class="n">input_module</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">TabularSequenceFeatures</span><span class="o">.</span><span class="n">from_schema</span><span class="p">(</span>
    <span class="n">schema</span><span class="p">,</span>
    <span class="n">max_sequence_length</span><span class="o">=</span><span class="n">max_sequence_length</span><span class="p">,</span>
    <span class="n">continuous_projection</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
    <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;concat&quot;</span><span class="p">,</span>
    <span class="n">masking</span><span class="o">=</span><span class="s2">&quot;causal&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Define one or multiple prediction-tasks</span>
<span class="n">prediction_tasks</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">NextItemPredictionTask</span><span class="p">()</span>

<span class="c1"># Define a transformer-config, like the XLNet architecture</span>
<span class="n">transformer_config</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">XLNetConfig</span><span class="o">.</span><span class="n">build</span><span class="p">(</span>
    <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_layer</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">total_seq_length</span><span class="o">=</span><span class="n">max_sequence_length</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">:</span> <span class="n">tr</span><span class="o">.</span><span class="n">Model</span> <span class="o">=</span> <span class="n">transformer_config</span><span class="o">.</span><span class="n">to_tf_model</span><span class="p">(</span><span class="n">input_module</span><span class="p">,</span> <span class="n">prediction_tasks</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="use-cases">
<h2>Use cases<a class="headerlink" href="#use-cases" title="Permalink to this headline"></a></h2>
<div class="section" id="sequential-and-session-based-recommendation">
<h3>Sequential and Session-based recommendation<a class="headerlink" href="#sequential-and-session-based-recommendation" title="Permalink to this headline"></a></h3>
<p>Traditional recommendation algorithms usually ignore the temporal dynamics and the sequence of interactions when trying to model user behaviour. Generally, the next user interaction is related to the sequence of the user’s previous choices. In some cases, it might be even a repeated purchase or song play. User interests might also suffer from the interest drift, as preferences might change over time. Those challenges are addressed by the <strong>sequential recommendation</strong> task.
A special case of sequential-recommendation is the <strong>session-based recommendation</strong> task, where you have only access to the short sequence of interactions within the current session. This is very common in online services like e-commerce, news and media portals where the user might choose to browse anonymously (and due to GDPR compliance no cookies are collected), or because it is a new user. This task is also relevant for scenarios where users’ interests change a lot over time depending on the user context or intent, so leveraging the current session interactions is more promising than old interactions to provide relevant recommendations.</p>
<p>To deal with sequential and session-based recommendation, many sequence learning algorithms previously applied in machine learning and NLP research have been explored for RecSys, based on k-Nearest Neighbors, Frequent Pattern Mining, Hidden Markov Models, Recurrent Neural Networks, and more recently neural architectures using the Self-Attention Mechanism and the Transformer architectures.</p>
<p>Differently from Transformers4Rec, existing frameworks for such tasks are generally focused for research, accept only sequence of item ids as input and do not provide a modularized and scalable implementation for production usage.</p>
</div>
</div>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline"></a></h2>
<div class="section" id="installing-with-pip">
<h3>Installing with pip<a class="headerlink" href="#installing-with-pip" title="Permalink to this headline"></a></h3>
<p>Transformers4Rec comes in two flavors: PyTorch and Tensorflow. It can optionally use the GPU-accelerated NVTabular dataloader, which is highly recommended.
Those components can be installed as optional args for the pip install package. Note that installation NVTabular with <code class="docutils literal notranslate"><span class="pre">pip</span></code> supports only CPU version of NVTabular for now.</p>
<ul class="simple">
<li><p>All
<code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">transformers4rec[all]</span></code></p></li>
<li><p>PyTorch
<code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">transformers4rec[pytorch,nvtabular]</span></code></p></li>
<li><p>Tensorflow:
<code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">transformers4rec[tensorflow,nvtabular]</span></code></p></li>
</ul>
</div>
<div class="section" id="installing-with-conda">
<h3>Installing with conda<a class="headerlink" href="#installing-with-conda" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">install</span> <span class="pre">-c</span> <span class="pre">nvidia</span> <span class="pre">transformers4rec</span></code></p>
</div>
<div class="section" id="installing-with-docker">
<h3>Installing with Docker<a class="headerlink" href="#installing-with-docker" title="Permalink to this headline"></a></h3>
<p>Transformers4Rec library is pre-installed in the NVIDIA Merlin Docker containers, that are available in the <a class="reference external" href="https://ngc.nvidia.com/catalog/containers/nvidia:merlin">NVIDIA container repository</a> in three different containers:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Container Name</th>
<th>Container Location</th>
<th>Functionality</th>
</tr>
</thead>
<tbody>
<tr>
<td>merlin-tensorflow-training</td>
<td><a href="https://ngc.nvidia.com/catalog/containers/nvidia:merlin:merlin-tensorflow-training">https://ngc.nvidia.com/catalog/containers/nvidia:merlin:merlin-tensorflow-training</a></td>
<td>Transformers4Rec, NVTabular, TensorFlow, and HugeCTR Tensorflow Embedding plugin</td>
</tr>
<tr>
<td>merlin-pytorch-training</td>
<td><a href="https://ngc.nvidia.com/catalog/containers/nvidia:merlin:merlin-pytorch-training">https://ngc.nvidia.com/catalog/containers/nvidia:merlin:merlin-pytorch-training</a></td>
<td>Transformers4Rec, NVTabular and PyTorch</td>
</tr>
<tr>
<td>merlin-inference</td>
<td><a href="https://ngc.nvidia.com/catalog/containers/nvidia:merlin:merlin-inference">https://ngc.nvidia.com/catalog/containers/nvidia:merlin:merlin-inference</a></td>
<td>Transformers4Rec, NVTabular, PyTorch, and Triton Inference</td>
</tr>
</tbody>
</table>
<p>To use these Docker containers, you’ll first need to install the <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker">NVIDIA Container Toolkit</a> to provide GPU support for Docker. You can use the NGC links referenced in the table above to obtain more information about how to launch and run these containers.</p>
</div>
<div class="section" id="feedback-and-support">
<h3>Feedback and Support<a class="headerlink" href="#feedback-and-support" title="Permalink to this headline"></a></h3>
<p>If you’d like to contribute to the library directly, see the <a class="reference internal" href="CONTRIBUTING.html"><span class="doc std std-doc">CONTRIBUTING.md</span></a>. We’re particularly interested in contributions or feature requests for our feature engineering and preprocessing operations. To further advance our Merlin Roadmap, we encourage you to share all the details regarding your recommender system pipeline in this <a class="reference external" href="https://developer.nvidia.com/merlin-devzone-survey">survey</a>.</p>
<p>If you’re interested in learning more about how NVTabular works, see
<a class="reference external" href="https://nvidia-merlin.github.io/Transformers4Rec/main/getting_started.html">Transformers4Rec documentation</a>. We also have the <a class="reference external" href="https://nvidia-merlin.github.io/Transformers4Rec/main/api/modules.html">API documentation</a> that outlines  specifics of the available modules and classes within the Transformers4Rec library.</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Merlin Transformers4Rec" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="why_transformers4rec.html" class="btn btn-neutral float-right" title="Why Transformers4Rec?" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>