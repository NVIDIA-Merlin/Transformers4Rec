# Model Architectures

Transformers4Rec provides modularized building blocks that can be combined with plain PyTorch modules and Keras layers, offering model definition flexibility so that you can use these blocks to build custom architectures with multiple towers, multiple heads, and losses (multi-task). For more information about the available options for each building block, refer to our [API Documentation](https://nvidia-merlin.github.io/Transformers4Rec/main/api/modules.html).

Fig. 1 exhibits an architecture that was built using next-item prediction and transformers, which can be used for both sequential and session-based recommendation. This architecture can be divided into four conceptual layers:

- Feature aggregation (Input Block)
- Sequence masking
- Sequence processing (Transformer/RNN Block)
- Prediction head (Output Block)

<div align=center><img src="/_images/transformers4rec_metaarchitecture.png" alt="Transformers4Rec meta-architecture" style="width:600px;"/><br>
<figcaption font-style: italic; align: center>Fig. 1: Transformers4Rec Meta-Architecture</figcaption></div>


## Feature Aggregation (Input Block)
In order to be fed into a transformer block, the sequences of input features (user_ids, user metadata, item_ids, and item metadata) must be aggregated into a single vector representation per element in the sequence that we call the *interaction embedding*.

Possible feature aggregation options include:

- **Concat**: Concatenation of the features.
- **Element-wise sum**: Features are summed in which all features must have the same dimension. For example, categorical embeddings must have the same dim and continuous features are projected to that dim.
- **Element-wise sum and item multiplication**: Similar to the *Element-wise sum* in which all features are summed except for the item id embedding since it's multiplied by the other features sum. The aggregation formula is available in our [Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation paper](https://dl.acm.org/doi/10.1145/3460231.3474255).

Categorical features are represented by embeddings. Numerical features can be represented as a scalar and projected by a fully-connected (FC) layer to multiple dimensions, or represented as a weighted average of embeddings using the technique Soft One-Hot embeddings. For more information, refer to our [online paper appendix](https://github.com/NVIDIA-Merlin/publications/blob/main/2021_acm_recsys_transformers4rec/Appendices/Appendix_A-Techniques_used_in_Transformers4Rec_Meta-Architecture.md). Categorical input features are optionally normalized with the normalization layer prior to aggregation. Continuous features should be normalized during feature engineering of the dataset.

`TabularSequenceFeatures` is the core class of this module. It processes and aggregates all features and outputs a sequence of *interaction embeddings* to be fed into transformer blocks. It can be instantiated automatically from a dataset schema (`from_schema()`) that is generated by [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular), which directly creates all the necessary layers to represent the categorical and continuous features in the dataset. In addition, it has the ability to aggregate the sequential features, and to prepare masked labels depending on the chosen sequence masking approach.

For example:

```python
from transformers4rec.torch import TabularSequenceFeatures
tabular_inputs = TabularSequenceFeatures.from_schema(
        schema,
        max_sequence_length=20,
        d_output=100,
        aggregation="concat",
        masking="clm"
    )
```

## Sequence Masking
Transformer architectures can be trained in different ways. Depending on the training method, there is a specific masking schema. The masking schema sets the items to be predicted (labels) and masks some positions of the sequence that cannot be used by the Transformer layers for prediction. Transformers4Rec currently supports the following training approaches that have been inspired by NLP:

- **Causal Language Modeling (`masking="clm"`)**: Predicts the next item based on past positions of the sequence. Future positions are masked.
- **Masked Language Modeling (`masking="mlm"`)**: Randomly selects some positions of the sequence to be predicted, which are masked. The Transformer layer is allowed to use positions on the right (future information) during training. During inference, all past items are visible for the Transformer layer, which tries to predict the next item.
- **Permutation Language Modeling (`masking="plm"`)**: Uses a permutation factorization at the level of the self-attention layer to define the accessible bi-directional context.
- **Replacement Token Detection (`masking="rtd"`)**: Uses MLM to randomly select some items, but replaces them by random tokens. A discriminator model, which can share the weights with the generator, is then requested to identify whether the item at each position belongs to the original sequence. The generator-discriminator architecture is jointly trained using Masked LM and RTD tasks.

**NOTE**: Not all transformer architectures support all of these training approaches. Transformers4Rec will raise an exception when you attempt to use an invalid combination and will provide suggestions using the appropriate masking techniques for that architecture.

## Sequence Processing (Transformer/RNN Block)
The Transformer block processes the input sequences of *interaction embeddings* created by the input block using Transformer architectures such as XLNet and GPT-2, or RNN architectures such as LSTM or GRU. The block is a standard keras layer or torch block depending on the underlying framework and is compatible with and substitutable by other blocks of the same type that support the input of a sequence.

In the following example, a `SequentialBlock` module is used to build the model body, which contains a `TabularSequenceFeatures` object (`tabular_inputs` defined in the previous code snippet), followed by an MLP projection layer to 64 dim (to match the Transformer `d_model`), and then followed by an XLNet transformer block with two layers (four heads each).

```python
from transformers4rec.config import transformer
from transformers4rec.torch import MLPBlock, SequentialBlock, TransformerBlock

# Configures the XLNet Transformer architecture
transformer_config = transformer.XLNetConfig.build(
    d_model=64, n_head=4, n_layer=2, total_seq_length=20
)

# Defines the model body including: inputs, masking, projection and transformer block.
model_body = SequentialBlock(
    tabular_inputs,
    torch4rec.MLPBlock([64]),
    torch4rec.TransformerBlock(transformer_config, masking=tabular_inputs.masking)
)
```

## Prediction Head (Output Block)
Following the input and transformer blocks, the model outputs its predictions. Transformers4Rec supports the following prediction heads, which can have multiple losses and can be combined for multi-task learning and multiple metrics:

- **Next Item Prediction**: Predicts next items for a given sequence of interactions. During training, it can be the next item or randomly selected items depending on the masking scheme. For inference, it's intended purposes is to always predict the next interacted item. Currently, cross-entropy and pairwise losses are supported.
- **Classification**: Predicts a categorical feature using the whole sequence. In the context of recommendation, which can be used to predict the user's next action such as whether the user will abandon a product in their cart or proceed with the purchase.
- **Regression**: Predicts a continuous feature using the whole sequence, such as the elapsed time until the user returns to a service.

In the following example, a head is instantiated with the pre-defined `model_body` for the `NextItemPredictionTask`. This head enables the `weight_tying` option. Decoupling model bodies and heads allow for a flexible model architecture definition since it allows for multiple towers and/or heads. Lastly, the `Model` class combines the heads and wraps the whole model.

```python
from transformers4rec.torch import Head, Model
from transformers4rec.torch.model.head import NextItemPredictionTask

# Defines the head related to next item prediction task
head = Head(
    model_body,
    NextItemPredictionTask(weight_tying=True, hf_format=True),
    inputs=inputs,
)

# Get the end-to-end Model class
model = Model(head)
```

### Tying Embeddings
For `NextItemPredictionTask`, we recommend **Tying Embeddings**, which was initially proposed by the NLP community to tie the weights of the input (item id) embedding matrix with the output projection layer. Not only do tied embeddings reduce the memory requirements significantly, but our own experimentation during [recent competitions](https://resources.nvidia.com/en-us-merlin/recommendation-syste?lx=97GH0Q) and empirical analysis detailed in our [Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation paper](https://dl.acm.org/doi/10.1145/3460231.3474255) and [online paper appendix](https://github.com/NVIDIA-Merlin/publications/blob/main/2021_acm_recsys_transformers4rec/Appendices/Appendix_A-Techniques_used_in_Transformers4Rec_Meta-Architecture.md) demonstrate that this method is very effective. It is enabled by default, but can be disabled by setting `weight_tying` to `False`.

## Regularization
Transformers4Rec supports a number of regularization techniques such as Dropout, Weight Decay, Softmax Temperature Scaling, Stochastic Shared Embeddings, and Label Smoothing. In our extensive experimentation, we hypertuned all regularization techniques for different datasets and found out that the Label Smoothing was particularly useful at improving both train and validation accuracy and better at calibrating the predictions.
