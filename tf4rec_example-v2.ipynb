{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3643614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import numpy\n",
    "import pandas as pd \n",
    "import cudf\n",
    "import cupy\n",
    "import nvtabular as nvt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41769047",
   "metadata": {},
   "source": [
    "### Create random input similar to pre-processed Yoochoose dataset structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "551db053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>day</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category</th>\n",
       "      <th>timestamp/age_days</th>\n",
       "      <th>timestamp/weekday/sin</th>\n",
       "      <th>purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75584</td>\n",
       "      <td>1</td>\n",
       "      <td>13959</td>\n",
       "      <td>228</td>\n",
       "      <td>0.188646</td>\n",
       "      <td>0.251946</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70373</td>\n",
       "      <td>5</td>\n",
       "      <td>22671</td>\n",
       "      <td>112</td>\n",
       "      <td>0.958216</td>\n",
       "      <td>0.398291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75911</td>\n",
       "      <td>8</td>\n",
       "      <td>7628</td>\n",
       "      <td>100</td>\n",
       "      <td>0.620943</td>\n",
       "      <td>0.566578</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77271</td>\n",
       "      <td>2</td>\n",
       "      <td>11962</td>\n",
       "      <td>181</td>\n",
       "      <td>0.835652</td>\n",
       "      <td>0.360247</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77767</td>\n",
       "      <td>1</td>\n",
       "      <td>32419</td>\n",
       "      <td>130</td>\n",
       "      <td>0.583954</td>\n",
       "      <td>0.020455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>75128</td>\n",
       "      <td>3</td>\n",
       "      <td>46371</td>\n",
       "      <td>217</td>\n",
       "      <td>0.608423</td>\n",
       "      <td>0.059013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>71278</td>\n",
       "      <td>4</td>\n",
       "      <td>34712</td>\n",
       "      <td>135</td>\n",
       "      <td>0.811443</td>\n",
       "      <td>0.112605</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>74048</td>\n",
       "      <td>7</td>\n",
       "      <td>1811</td>\n",
       "      <td>118</td>\n",
       "      <td>0.202478</td>\n",
       "      <td>0.415030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>75250</td>\n",
       "      <td>7</td>\n",
       "      <td>1093</td>\n",
       "      <td>161</td>\n",
       "      <td>0.884268</td>\n",
       "      <td>0.878341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>77614</td>\n",
       "      <td>6</td>\n",
       "      <td>45375</td>\n",
       "      <td>302</td>\n",
       "      <td>0.548861</td>\n",
       "      <td>0.612832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_id  day  item_id  category  timestamp/age_days  \\\n",
       "0          75584    1    13959       228            0.188646   \n",
       "1          70373    5    22671       112            0.958216   \n",
       "2          75911    8     7628       100            0.620943   \n",
       "3          77271    2    11962       181            0.835652   \n",
       "4          77767    1    32419       130            0.583954   \n",
       "...          ...  ...      ...       ...                 ...   \n",
       "9995       75128    3    46371       217            0.608423   \n",
       "9996       71278    4    34712       135            0.811443   \n",
       "9997       74048    7     1811       118            0.202478   \n",
       "9998       75250    7     1093       161            0.884268   \n",
       "9999       77614    6    45375       302            0.548861   \n",
       "\n",
       "      timestamp/weekday/sin  purchase  \n",
       "0                  0.251946         0  \n",
       "1                  0.398291         0  \n",
       "2                  0.566578         0  \n",
       "3                  0.360247         1  \n",
       "4                  0.020455         0  \n",
       "...                     ...       ...  \n",
       "9995               0.059013         1  \n",
       "9996               0.112605         1  \n",
       "9997               0.415030         0  \n",
       "9998               0.878341         1  \n",
       "9999               0.612832         0  \n",
       "\n",
       "[10000 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_ROWS = 10000\n",
    "session_length = 20\n",
    "batch_size = 100\n",
    "inputs = {\n",
    "    'session_id': numpy.random.randint(70000, 80000, NUM_ROWS),\n",
    "    'day': numpy.random.randint(1, 10, NUM_ROWS),\n",
    "    'item_id': numpy.random.randint(1, 51996, NUM_ROWS),\n",
    "    'category': numpy.random.randint(0, 332, NUM_ROWS),\n",
    "    'timestamp/age_days': numpy.random.uniform(0, 1, NUM_ROWS),\n",
    "    'timestamp/weekday/sin' : numpy.random.uniform(0, 1, NUM_ROWS),\n",
    "    'purchase': numpy.random.randint(0, 2, NUM_ROWS)\n",
    "    }\n",
    "random_data = cudf.DataFrame(inputs)\n",
    "random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1827df",
   "metadata": {},
   "source": [
    "### NVTabular workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f83f8f",
   "metadata": {},
   "source": [
    "- #TODO : Change the workflow using tagging API once it is finalized  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed85e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.8/site-packages/nvtabular-0.6.0+16.gd36ebd2-py3.8-linux-x86_64.egg/nvtabular/workflow/node.py:43: FutureWarning: The `[\"a\", \"b\", \"c\"] >> ops.Operator` syntax for creating a `ColumnGroup` has been deprecated in NVTabular 21.09 and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Groupby Workflow\n",
    "groupby_features = list(inputs.keys()) >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"session_id\"], \n",
    "    aggs={\n",
    "        \"item_id\": [\"list\"],\n",
    "        \"category\": [\"list\"],     \n",
    "        \"day\": [\"first\"],\n",
    "        \"purchase\": [\"first\"],\n",
    "        \"timestamp/age_days\": [\"list\"],\n",
    "        'timestamp/weekday/sin': [\"list\"],\n",
    "        },\n",
    "    name_sep=\"-\")\n",
    "# Trim sessions to first 20 items \n",
    "groupby_features_nonlist = [x for x in groupby_features.selector if '-list' not in x]\n",
    "groupby_features_nonlist\n",
    "groupby_features_trim = ((groupby_features - groupby_features_nonlist)) >> nvt.ops.ListSlice(0,20) >> nvt.ops.Rename(postfix = '_trim')\n",
    "\n",
    "workflow = nvt.Workflow(groupby_features + groupby_features_trim )\n",
    "dataset = nvt.Dataset(random_data, cpu=False)\n",
    "workflow.fit(dataset)\n",
    "sessions_gdf = workflow.transform(dataset).to_ddf().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb01c7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>day-first</th>\n",
       "      <th>category-list</th>\n",
       "      <th>purchase-first</th>\n",
       "      <th>timestamp/weekday/sin-list</th>\n",
       "      <th>timestamp/age_days-list</th>\n",
       "      <th>item_id-list</th>\n",
       "      <th>category-list_trim</th>\n",
       "      <th>timestamp/weekday/sin-list_trim</th>\n",
       "      <th>timestamp/age_days-list_trim</th>\n",
       "      <th>item_id-list_trim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70000</td>\n",
       "      <td>1</td>\n",
       "      <td>[9]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.16624953636898254]</td>\n",
       "      <td>[0.7724655470980988]</td>\n",
       "      <td>[18748]</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[0.16624953636898254]</td>\n",
       "      <td>[0.7724655470980988]</td>\n",
       "      <td>[18748]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70001</td>\n",
       "      <td>8</td>\n",
       "      <td>[135]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.015624112985417216]</td>\n",
       "      <td>[0.032640360108273314]</td>\n",
       "      <td>[3652]</td>\n",
       "      <td>[135]</td>\n",
       "      <td>[0.015624112985417216]</td>\n",
       "      <td>[0.032640360108273314]</td>\n",
       "      <td>[3652]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70002</td>\n",
       "      <td>2</td>\n",
       "      <td>[30, 228]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.8508864494735382, 0.32605929617141827]</td>\n",
       "      <td>[0.11044592975969747, 0.7642976814488108]</td>\n",
       "      <td>[7308, 766]</td>\n",
       "      <td>[30, 228]</td>\n",
       "      <td>[0.8508864494735382, 0.32605929617141827]</td>\n",
       "      <td>[0.11044592975969747, 0.7642976814488108]</td>\n",
       "      <td>[7308, 766]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id  day-first category-list  purchase-first  \\\n",
       "0       70000          1           [9]               0   \n",
       "1       70001          8         [135]               0   \n",
       "2       70002          2     [30, 228]               0   \n",
       "\n",
       "                  timestamp/weekday/sin-list  \\\n",
       "0                      [0.16624953636898254]   \n",
       "1                     [0.015624112985417216]   \n",
       "2  [0.8508864494735382, 0.32605929617141827]   \n",
       "\n",
       "                     timestamp/age_days-list item_id-list category-list_trim  \\\n",
       "0                       [0.7724655470980988]      [18748]                [9]   \n",
       "1                     [0.032640360108273314]       [3652]              [135]   \n",
       "2  [0.11044592975969747, 0.7642976814488108]  [7308, 766]          [30, 228]   \n",
       "\n",
       "             timestamp/weekday/sin-list_trim  \\\n",
       "0                      [0.16624953636898254]   \n",
       "1                     [0.015624112985417216]   \n",
       "2  [0.8508864494735382, 0.32605929617141827]   \n",
       "\n",
       "                timestamp/age_days-list_trim item_id-list_trim  \n",
       "0                       [0.7724655470980988]           [18748]  \n",
       "1                     [0.032640360108273314]            [3652]  \n",
       "2  [0.11044592975969747, 0.7642976814488108]       [7308, 766]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions_gdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "909251f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#workflow.save('workflow_inference_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba34c9",
   "metadata": {},
   "source": [
    "### Export pre-processed data by day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7476bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a Datset and write out hive-partitioned data to disk\n",
    "nvt_output_path_tmp ='./output_nvt_tmp/'\n",
    "PARTITION_COL = 'day-first'\n",
    "nvt.Dataset(sessions_gdf).to_parquet(nvt_output_path_tmp, partition_on=[PARTITION_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e329fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER = \"./preproc_sessions_by_day_ts/\"\n",
    "!mkdir -p $OUTPUT_FOLDER\n",
    "days_folders = [f for f in sorted(os.listdir(nvt_output_path_tmp)) if f.startswith(PARTITION_COL)]\n",
    "for day_folder in days_folders:\n",
    "    df = cudf.read_parquet(os.path.join(nvt_output_path_tmp, day_folder))\n",
    "#     print(len(df))\n",
    "    \n",
    "    out_folder = os.path.join(OUTPUT_FOLDER, day_folder.replace('day-first=', ''))\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    df.to_parquet(os.path.join(out_folder, 'train.parquet'))\n",
    "    \n",
    "    random_values = cupy.random.rand(len(df))\n",
    "    \n",
    "    #Extracts 10% for valid and test set. Those sessions are also in the train set, but as evaluation\n",
    "    #happens only for the subsequent day of training, that is not an issue, and we can keep the train set larger.\n",
    "    valid_set = df[random_values <= 0.10]\n",
    "    valid_set.to_parquet(os.path.join(out_folder, 'valid.parquet'))\n",
    "    \n",
    "    test_set = df[random_values >= 0.90]\n",
    "    test_set.to_parquet(os.path.join(out_folder, 'test.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f76c4",
   "metadata": {},
   "source": [
    "# Transformers4rec model  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e86a186",
   "metadata": {},
   "source": [
    "- Manually set the schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f74589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import transformers4rec.torch as torch4rec\n",
    "from transformers4rec.torch import TabularSequenceFeatures, MLPBlock, SequentialBlock, Head, TransformerBlock\n",
    "\n",
    "from transformers4rec.utils.schema import DatasetSchema\n",
    "\n",
    "from transformers4rec.torch.head import NextItemPredictionTask\n",
    "\n",
    "from transformers4rec.config import transformer\n",
    "from transformers4rec.torch.ranking_metric import NDCGAt, AvgPrecisionAt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d484ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = DatasetSchema.from_schema(\"schema.pb\")\n",
    "\n",
    "# if I do the following it works.\n",
    "# schema = schema.select_by_name(['item_id-list_trim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "516942f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the tabular module that converts the inputs and aggregate them into one single interaction tensor \n",
    "# the current supported aggregations are: sequential_concat and element-wise-sum\n",
    "\n",
    "inputs = TabularSequenceFeatures.from_schema(\n",
    "        schema,\n",
    "        max_sequence_length=20,\n",
    "        d_output=100,\n",
    "        masking=\"causal\",\n",
    "    )\n",
    "\n",
    "inputs.masking.device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79039c7",
   "metadata": {},
   "source": [
    "   * 1. Define the Transformer block :  LM task + HF Transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c1cd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # case-1: Define XLNetConfig class and set default parameters \n",
    "\n",
    "transformer_config = transformer.XLNetConfig.build(\n",
    "    d_model=64, n_head=4, n_layer=2, total_seq_length=20\n",
    ")\n",
    "\n",
    "body = torch4rec.SequentialBlock(\n",
    "    inputs, torch4rec.MLPBlock([64]), torch4rec.TransformerBlock(transformer=transformer_config, masking=inputs.masking)\n",
    ")\n",
    "\n",
    "\n",
    "head = torch4rec.Head(\n",
    "    body,\n",
    "    torch4rec.NextItemPredictionTask(weight_tying=True, hf_format=True),\n",
    "    inputs=inputs,\n",
    ")\n",
    "model = torch4rec.Model(head)\n",
    "\n",
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252fc69",
   "metadata": {},
   "source": [
    "# Train the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d0fe2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data using old NVTabular dataloader  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c0a16c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers4rec.recsys_args import DataArguments, ModelArguments, TrainingArguments\n",
    "\n",
    "TrainingArguments.local_rank = -1\n",
    "TrainingArguments.world_size = 1\n",
    "TrainingArguments.dataloader_drop_last = True\n",
    "TrainingArguments.device = \"cuda\"\n",
    "TrainingArguments.report_to = []\n",
    "TrainingArguments.debug = [\"r\"]\n",
    "TrainingArguments.n_gpu = 1\n",
    "TrainingArguments.gradient_accumulation_steps = 32\n",
    "TrainingArguments.train_batch_size = 512\n",
    "TrainingArguments.per_device_train_batch_size = 512\n",
    "TrainingArguments.per_device_eval_batch_size = 512\n",
    "TrainingArguments.output_dir = \"\"\n",
    "TrainingArguments.world_size = 1\n",
    "\n",
    "\n",
    "DataArguments.data_path = \"./preproc_sessions_by_day_ts/\"\n",
    "DataArguments.data_loader_engine = \"nvtabular\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3b4a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# NVTabular dependencies\n",
    "from nvtabular import Dataset as NVTDataset\n",
    "from nvtabular.loader.torch import DLDataLoader\n",
    "from nvtabular.loader.torch import TorchAsyncItr as NVTDataLoader\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n",
    "\n",
    "# BATCH_SIZE = 16\n",
    "SESSION_LENGTH_MAX = 20\n",
    "\n",
    "# x_cat_names, x_cont_names = ['item_id-list_trim', 'category-list_trim'], ['timestamp/weekday/sin-list_trim','timestamp/age_days-list_trim']\n",
    "\n",
    "x_cat_names, x_cont_names = ['item_id-list_trim'], []\n",
    "\n",
    "sparse_features_max = {\n",
    "    fname: SESSION_LENGTH_MAX\n",
    "    for fname in x_cat_names + x_cont_names\n",
    "}\n",
    "\n",
    "train_data_paths = glob.glob(\"./preproc_sessions_by_day_ts/*/train.parquet\")\n",
    "train_dataset = NVTDataset(\n",
    "    train_data_paths,\n",
    "    engine=\"parquet\",\n",
    ")\n",
    "\n",
    "def dataloader_collate_dict(inputs):\n",
    "    # Gets only the features dict\n",
    "    inputs = inputs[0][0]\n",
    "    return inputs\n",
    "\n",
    "class DLDataLoaderWrapper(DLDataLoader):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        if \"batch_size\" in kwargs:\n",
    "            # Setting the batch size directly to DLDataLoader makes it 3x slower. \n",
    "            # So we set as an alternative attribute and use it within RecSysTrainer during evaluation\n",
    "            self._batch_size = kwargs.pop(\"batch_size\")\n",
    "        super().__init__(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8ba308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = NVTDataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=TrainingArguments.train_batch_size,\n",
    "    shuffle=False,\n",
    "    cats=x_cat_names,\n",
    "    conts=x_cont_names,\n",
    "    device=0,\n",
    "    labels=[],\n",
    "    sparse_names=x_cat_names + x_cont_names,\n",
    "    sparse_max=sparse_features_max,\n",
    "    sparse_as_dense=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "dl_loader = DLDataLoaderWrapper(\n",
    "    loader, collate_fn=dataloader_collate_dict, batch_size=TrainingArguments.train_batch_size\n",
    "    )\n",
    "out = next(iter(dl_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dea0bf0-cf09-4b8c-b349-f62b2388fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = NVTDataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=TrainingArguments.train_batch_size,\n",
    "    shuffle=False,\n",
    "    cats=x_cat_names,\n",
    "    conts=x_cont_names,\n",
    "    device=0,\n",
    "    labels=[],\n",
    "    sparse_names=x_cat_names + x_cont_names,\n",
    "    sparse_max=sparse_features_max,\n",
    "    sparse_as_dense=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "dl_loader = DLDataLoaderWrapper(\n",
    "    loader, collate_fn=dataloader_collate_dict, batch_size=TrainingArguments.train_batch_size\n",
    "    )\n",
    "out = next(iter(dl_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "121bb9c3-bd14-4993-9a14-67bc8f9f14b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 20])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['item_id-list_trim'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "068b2b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_id-list_trim': tensor([[18748,     0,     0,  ...,     0,     0,     0],\n",
       "         [12856,  5574,     0,  ...,     0,     0,     0],\n",
       "         [43739, 37018,     0,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [34889, 44216,     0,  ...,     0,     0,     0],\n",
       "         [45364,     0,     0,  ...,     0,     0,     0],\n",
       "         [  215, 21723, 16622,  ...,     0,     0,     0]], device='cuda:0')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51854b19",
   "metadata": {},
   "source": [
    "- Test the output of the model : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a32cdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFWrapper(torch.nn.Module): \n",
    "    def __init__(self, model): \n",
    "        super().__init__()\n",
    "        self.model = model \n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        inputs = kwargs\n",
    "        return model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d4353db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wp = HFWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3bb295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7380b4b8-ba72-4c22-aa41-da0e16703332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "heads.0.body.0.to_merge.categorical_module.embedding_tables.category-list_trim.weight \t torch.Size([332, 64])\n",
      "heads.0.body.0.to_merge.categorical_module.embedding_tables.item_id-list_trim.weight \t torch.Size([51996, 64])\n",
      "heads.0.body.0.projection_module.0.0.weight \t torch.Size([100, 129])\n",
      "heads.0.body.0.projection_module.0.0.bias \t torch.Size([100])\n",
      "heads.0.body.0.masking.masked_item_embedding \t torch.Size([100])\n",
      "heads.0.body.1.0.0.weight \t torch.Size([64, 100])\n",
      "heads.0.body.1.0.0.bias \t torch.Size([64])\n",
      "heads.0.body.2.transformer.mask_emb \t torch.Size([1, 1, 64])\n",
      "heads.0.body.2.transformer.word_embedding.weight \t torch.Size([1, 64])\n",
      "heads.0.body.2.transformer.layer.0.rel_attn.q \t torch.Size([64, 4, 16])\n",
      "heads.0.body.2.transformer.layer.0.rel_attn.k \t torch.Size([64, 4, 16])\n",
      "heads.0.body.2.transformer.layer.0.rel_attn.v \t torch.Size([64, 4, 16])\n",
      "heads.0.body.2.transformer.layer.0.rel_attn.o \t torch.Size([64, 4, 16])\n",
      "heads.0.body.2.transformer.layer.0.rel_attn.r \t torch.Size([64, 4, 16])\n",
      "heads.0.body.2.transformer.layer.0.rel_attn.r_r_bias \t torch.Size([4, 16])\n",
      "heads.0.body.2.transformer.layer.0.rel_attn.r_s_bias \t torch.Size([4, 16])\n",
      "heads.0.body.2.transformer.layer.0.rel_attn.r_w_bias \t torch.Size([4, 16])\n",
      "heads.0.body.2.transformer.layer.0.rel_attn.seg_embed \t torch.Size([2, 4, 16])\n",
      "heads.0.body.2.transformer.layer.0.rel_attn.layer_norm.weight \t torch.Size([64])\n",
      "heads.0.body.2.transformer.layer.0.rel_attn.layer_norm.bias \t torch.Size([64])\n",
      "heads.0.body.2.transformer.layer.0.ff.layer_norm.weight \t torch.Size([64])\n",
      "heads.0.body.2.transformer.layer.0.ff.layer_norm.bias \t torch.Size([64])\n",
      "heads.0.body.2.transformer.layer.0.ff.layer_1.weight \t torch.Size([256, 64])\n",
      "heads.0.body.2.transformer.layer.0.ff.layer_1.bias \t torch.Size([256])\n",
      "heads.0.body.2.transformer.layer.0.ff.layer_2.weight \t torch.Size([64, 256])\n",
      "heads.0.body.2.transformer.layer.0.ff.layer_2.bias \t torch.Size([64])\n",
      "heads.0.body.2.transformer.layer.1.rel_attn.q \t torch.Size([64, 4, 16])\n",
      "heads.0.body.2.transformer.layer.1.rel_attn.k \t torch.Size([64, 4, 16])\n",
      "heads.0.body.2.transformer.layer.1.rel_attn.v \t torch.Size([64, 4, 16])\n",
      "heads.0.body.2.transformer.layer.1.rel_attn.o \t torch.Size([64, 4, 16])\n",
      "heads.0.body.2.transformer.layer.1.rel_attn.r \t torch.Size([64, 4, 16])\n",
      "heads.0.body.2.transformer.layer.1.rel_attn.r_r_bias \t torch.Size([4, 16])\n",
      "heads.0.body.2.transformer.layer.1.rel_attn.r_s_bias \t torch.Size([4, 16])\n",
      "heads.0.body.2.transformer.layer.1.rel_attn.r_w_bias \t torch.Size([4, 16])\n",
      "heads.0.body.2.transformer.layer.1.rel_attn.seg_embed \t torch.Size([2, 4, 16])\n",
      "heads.0.body.2.transformer.layer.1.rel_attn.layer_norm.weight \t torch.Size([64])\n",
      "heads.0.body.2.transformer.layer.1.rel_attn.layer_norm.bias \t torch.Size([64])\n",
      "heads.0.body.2.transformer.layer.1.ff.layer_norm.weight \t torch.Size([64])\n",
      "heads.0.body.2.transformer.layer.1.ff.layer_norm.bias \t torch.Size([64])\n",
      "heads.0.body.2.transformer.layer.1.ff.layer_1.weight \t torch.Size([256, 64])\n",
      "heads.0.body.2.transformer.layer.1.ff.layer_1.bias \t torch.Size([256])\n",
      "heads.0.body.2.transformer.layer.1.ff.layer_2.weight \t torch.Size([64, 256])\n",
      "heads.0.body.2.transformer.layer.1.ff.layer_2.bias \t torch.Size([64])\n",
      "heads.0.body.2.masking.masked_item_embedding \t torch.Size([100])\n",
      "heads.0.prediction_tasks.0.item_embedding_table.weight \t torch.Size([51996, 64])\n",
      "heads.0.prediction_tasks.0.masking.masked_item_embedding \t torch.Size([100])\n",
      "heads.0.prediction_tasks.0.pre.module.output_layer_bias \t torch.Size([51996])\n",
      "heads.0.prediction_tasks.0.pre.module.item_embedding_table.weight \t torch.Size([51996, 64])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcdfee6",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ef136",
   "metadata": {},
   "source": [
    "- Basic fit  and evaluate to test the model training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5a4cb",
   "metadata": {},
   "source": [
    "- Load arguments:  \n",
    "    N.B: These classes will be updated to keep only what required by recsys_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2020e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the RecSysTrainer, which manages training and evaluation\n",
    "from transformers4rec.recsys_trainer import RecSysTrainer, DatasetType\n",
    "\n",
    "trainer = RecSysTrainer(\n",
    "    model=model_wp,\n",
    "    args=TrainingArguments,\n",
    "    model_args=ModelArguments,\n",
    "    data_args=DataArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a1ada5",
   "metadata": {},
   "source": [
    "- Fit the model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5215c65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ** On entry to SGEMM  parameter number 10 had an illegal value\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2108/4070531899.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_lr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1261\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1781\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2108/3770593878.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/TF4Rec/transformers4rec/torch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_body\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_output_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/TF4Rec/transformers4rec/torch/head.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, body_outputs, call_body, always_output_dict, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcall_body\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0mbody_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_tasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/TF4Rec/transformers4rec/torch/tabular.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, pre, post, merge_with, stack_outputs, concat_outputs, aggregation, augmentation, filter_columns, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/TF4Rec/transformers4rec/torch/tabular.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, pre, post, merge_with, stack_outputs, concat_outputs, aggregation, augmentation, filter_columns, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/TF4Rec/transformers4rec/torch/features/sequence.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/TF4Rec/transformers4rec/torch/tabular.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, pre, post, merge_with, stack_outputs, concat_outputs, aggregation, augmentation, filter_columns, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/TF4Rec/transformers4rec/torch/tabular.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, pre, post, merge_with, stack_outputs, concat_outputs, aggregation, augmentation, filter_columns, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "trainer.set_train_dataloader(dl_loader)\n",
    "trainer.reset_lr_scheduler()\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
