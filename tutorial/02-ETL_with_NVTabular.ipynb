{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will create a preprocessing and feature engineering pipeline with [Rapids cuDF](https://github.com/rapidsai/cudf) and [Merlin NVTabular](https://github.com/NVIDIA/NVTabular) libraries to prepare our dataset for session-based recommendation model training. \n",
    "\n",
    "NVTabular is a feature engineering and preprocessing library for tabular data that is designed to easily manipulate terabyte scale datasets and train deep learning (DL) based recommender systems.\n",
    "\n",
    "Our main goal is to create session-based features. In order to do that, we are going to perform the following:\n",
    "\n",
    "- Categorify categorical features with `Categorify()` op\n",
    "- Create temporal features with a `user-defined custom` op and `Lambda` op\n",
    "- Transform continuous features using `Log` and `Normalize` ops\n",
    "- Group all these features together at the session level sorting the interactions by time with `Groupby`\n",
    "- Finally export the preprocessed datasets to parquet files by hive-partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our hands-on exercise notebooks we are going to use a subset of the publicly available [eCommerce dataset](https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store). The ecommeerce behavior data contains 7 months of data (from October 2019 to April 2020) from a large multi-category online store. Each row in the file represents an event. All events are related to products and users. Each event is like many-to-many relation between products and users.\n",
    "\n",
    "Data collected by Open CDP project and the source of the dataset is [REES46 Marketing Platform](https://rees46.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import cupy as cp\n",
    "import glob\n",
    "\n",
    "import cudf\n",
    "import cupy\n",
    "import nvtabular as nvt\n",
    "from nvtabular import ColumnSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up Input and Output Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some information about where to get our data\n",
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", \"/data/\")\n",
    "OUTPUT_DATA_DIR = os.environ.get(\"OUTPUT_DATA_DIR\", INPUT_DATA_DIR + \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Read the Parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already performed certain preprocessing steps on the first month (Oct-2019) of the raw dataset in the `01-preprocess` notebook: <br>\n",
    "\n",
    "- we created `event_time_ts` column from `event_time` column which shows the time when event happened at (in UTC).\n",
    "- we created `prod_first_event_time_ts` column which indicates the timestamp that an item was seen first time.\n",
    "- we removed the rows where the `user_session` is Null. As a result, 2 rows were removed.\n",
    "- we categorified the `user_session` column, so that it now has only integer values.\n",
    "- we removed consequetively repeated (user, item) interactions. For example, an original session with `[1, 2, 4, 1, 2, 2, 3, 3, 3]` product interactions has become `[1, 2, 4, 1, 2, 3]` after removing the repeated interactions on the same item within the same session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the original dataset contains 7 months data files, we are going to use the first seven days of the `Oct-2019.csv` ecommerce dataset. We use cuDF to read the parquet file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 650 ms, sys: 308 ms, total: 959 ms\n",
      "Wall time: 983 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_session</th>\n",
       "      <th>event_type</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_code</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_time_ts</th>\n",
       "      <th>prod_first_event_time_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197</td>\n",
       "      <td>view</td>\n",
       "      <td>12300705</td>\n",
       "      <td>2053013556311359947</td>\n",
       "      <td>construction.tools.drill</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>79.78</td>\n",
       "      <td>513371687</td>\n",
       "      <td>1570373382</td>\n",
       "      <td>1569905277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>197</td>\n",
       "      <td>view</td>\n",
       "      <td>12300621</td>\n",
       "      <td>2053013556311359947</td>\n",
       "      <td>construction.tools.drill</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>71.82</td>\n",
       "      <td>513371687</td>\n",
       "      <td>1570373453</td>\n",
       "      <td>1569905174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197</td>\n",
       "      <td>view</td>\n",
       "      <td>12300747</td>\n",
       "      <td>2053013556311359947</td>\n",
       "      <td>construction.tools.drill</td>\n",
       "      <td>kolner</td>\n",
       "      <td>65.89</td>\n",
       "      <td>513371687</td>\n",
       "      <td>1570373484</td>\n",
       "      <td>1569903299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>197</td>\n",
       "      <td>view</td>\n",
       "      <td>12301303</td>\n",
       "      <td>2053013556311359947</td>\n",
       "      <td>construction.tools.drill</td>\n",
       "      <td>kolner</td>\n",
       "      <td>96.67</td>\n",
       "      <td>513371687</td>\n",
       "      <td>1570373530</td>\n",
       "      <td>1570030847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>197</td>\n",
       "      <td>view</td>\n",
       "      <td>12300061</td>\n",
       "      <td>2053013556311359947</td>\n",
       "      <td>construction.tools.drill</td>\n",
       "      <td>bosch</td>\n",
       "      <td>82.76</td>\n",
       "      <td>513371687</td>\n",
       "      <td>1570373559</td>\n",
       "      <td>1569898066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_session event_type  product_id          category_id  \\\n",
       "0           197       view    12300705  2053013556311359947   \n",
       "1           197       view    12300621  2053013556311359947   \n",
       "2           197       view    12300747  2053013556311359947   \n",
       "3           197       view    12301303  2053013556311359947   \n",
       "4           197       view    12300061  2053013556311359947   \n",
       "\n",
       "              category_code   brand  price    user_id  event_time_ts  \\\n",
       "0  construction.tools.drill    <NA>  79.78  513371687     1570373382   \n",
       "1  construction.tools.drill    <NA>  71.82  513371687     1570373453   \n",
       "2  construction.tools.drill  kolner  65.89  513371687     1570373484   \n",
       "3  construction.tools.drill  kolner  96.67  513371687     1570373530   \n",
       "4  construction.tools.drill   bosch  82.76  513371687     1570373559   \n",
       "\n",
       "   prod_first_event_time_ts  \n",
       "0                1569905277  \n",
       "1                1569905174  \n",
       "2                1569903299  \n",
       "3                1570030847  \n",
       "4                1569898066  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df = cudf.read_parquet(os.path.join(INPUT_DATA_DIR, 'Oct-2019.parquet')) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6390928, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if there is any column with nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_session                False\n",
       "event_type                  False\n",
       "product_id                  False\n",
       "category_id                 False\n",
       "category_code                True\n",
       "brand                        True\n",
       "price                       False\n",
       "user_id                     False\n",
       "event_time_ts               False\n",
       "prod_first_event_time_ts    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We see that `'category_code', 'brand'` columns have null values, and in the following cell we are going to fill these nulls with via categorify op, and then all categorical columns will be encoded to continuous integers. Note that we also add `1` after we categorify the categorical columns, the reason for that we want the encoded null values to start from `1` instead of `0` because we reserve `0` for padding the seqeunce features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize NVTabular Workflow\n",
    "\n",
    "### 5.1. Categorical Features Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorify features \n",
    "cat_feats = ['user_session', 'category_code', 'brand', 'user_id', 'product_id', 'category_id', 'event_type'] >> nvt.ops.Categorify() >> nvt.ops.LambdaOp(lambda col: col +1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Extract Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time features\n",
    "sessionTs = ['event_time_ts']\n",
    "\n",
    "sessionTime = (\n",
    "    sessionTs >> \n",
    "    nvt.ops.LambdaOp(lambda col: cudf.to_datetime(col, unit='s')) >> \n",
    "    nvt.ops.Rename(name = 'event_time_dt')\n",
    ")\n",
    "\n",
    "sessionTime_weekday = (\n",
    "    sessionTime >> \n",
    "    nvt.ops.LambdaOp(lambda col: col.dt.weekday) >> \n",
    "    nvt.ops.Rename(name ='et_dayofweek')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create cycling features from the `sessionTime_weekday` column. We would like to use the temporal features (hour, day of week, month, etc.) inherently cylical characteristic. We represent the day of week as a cycling feature, so that it can be represented in a continuous space (sine and cosine). That way, the difference between the representation of two different days is the same, in other words, with cyclical features we can convey closeness between data. You can read more about it [here](https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cycled_feature_value_sin(col, max_value):\n",
    "    value_scaled = (col + 0.000001) / max_value\n",
    "    value_sin = np.sin(2*np.pi*value_scaled)\n",
    "    return value_sin\n",
    "\n",
    "def get_cycled_feature_value_cos(col, max_value):\n",
    "    value_scaled = (col + 0.000001) / max_value\n",
    "    value_cos = np.cos(2*np.pi*value_scaled)\n",
    "    return value_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_sin = sessionTime_weekday >> (lambda col: get_cycled_feature_value_sin(col+1, 7)) >> nvt.ops.Rename(name = 'et_dayofweek_sin')\n",
    "weekday_cos= sessionTime_weekday >> (lambda col: get_cycled_feature_value_cos(col+1, 7)) >> nvt.ops.Rename(name = 'et_dayofweek_cos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Add Product Recency feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's define a custom op to calculate product recency in days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular.ops import Operator\n",
    "\n",
    "class ItemRecency(Operator):\n",
    "    def transform(self, columns, gdf):\n",
    "        for column in columns:\n",
    "            col = gdf[column]\n",
    "            item_first_timestamp = gdf['prod_first_event_time_ts']\n",
    "            delta_days = (col - item_first_timestamp) / (60*60*24)\n",
    "            gdf[column + \"_age_days\"] = delta_days * (delta_days >=0)\n",
    "        return gdf\n",
    "            \n",
    "    def output_column_names(self, columns):\n",
    "        return ColumnSelector([column + \"_age_days\" for column in columns])\n",
    "\n",
    "    def dependencies(self):\n",
    "        return [\"prod_first_event_time_ts\"]\n",
    "    \n",
    "    \n",
    "recency_features = ['event_time_ts'] >> ItemRecency() \n",
    "recency_features_norm = recency_features >> nvt.ops.LogOp() >> nvt.ops.Normalize() >> nvt.ops.Rename(name='product_recency_days_log_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = (\n",
    "    sessionTime +\n",
    "    sessionTime_weekday +\n",
    "    weekday_sin +\n",
    "    weekday_cos +\n",
    "    recency_features_norm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize our workflow graph from a feature columngroup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"1257pt\" height=\"548pt\"\n",
       " viewBox=\"0.00 0.00 1256.75 548.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 544)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-544 1252.75,-544 1252.75,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"532.91\" cy=\"-306\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"532.91\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">Rename</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"795.91\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"795.91\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M485.17,-303.76C361.78,-300.05 39.63,-287.08 7.91,-252 -2.82,-240.13 -2.38,-228.25 7.91,-216 105.8,-99.47 616.27,-90.89 758.87,-90.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"758.87,-94.3 768.88,-90.81 758.88,-87.3 758.87,-94.3\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"269.91\" cy=\"-234\" rx=\"252.66\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"269.91\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">(lambda col: get_cycled_feature_value_sin(col+1, 7))</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;4 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>0&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M494.95,-294.9C454.94,-284.25 390.98,-267.22 341.46,-254.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"342.32,-250.65 331.76,-251.46 340.52,-257.42 342.32,-250.65\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>10</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"795.91\" cy=\"-234\" rx=\"254.55\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"795.91\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">(lambda col: get_cycled_feature_value_cos(col+1, 7))</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;10 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>0&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M570.87,-294.9C610.81,-284.27 674.63,-267.28 724.12,-254.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"725.05,-257.48 733.81,-251.53 723.25,-250.72 725.05,-257.48\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>7</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"676.91\" cy=\"-378\" rx=\"230.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"676.91\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\">nvt.ops.LambdaOp(lambda col: col.dt.weekday)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>7&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M642.05,-360.05C620.35,-349.5 592.44,-335.94 570.28,-325.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"571.7,-321.96 561.17,-320.74 568.64,-328.26 571.7,-321.96\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1173.91\" cy=\"-306\" rx=\"40.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1173.91\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">LogOp</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>8</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1170.91\" cy=\"-234\" rx=\"58.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1170.91\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">Normalize</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;8 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>1&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1173.17,-287.7C1172.84,-279.98 1172.44,-270.71 1172.07,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1175.57,-261.95 1171.64,-252.1 1168.58,-262.25 1175.57,-261.95\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1177.91\" cy=\"-378\" rx=\"70.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1177.91\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\">ItemRecency</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1176.92,-359.7C1176.48,-351.98 1175.95,-342.71 1175.46,-334.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1178.95,-333.89 1174.89,-324.1 1171.96,-334.29 1178.95,-333.89\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"795.91\" cy=\"-18\" rx=\"306.55\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"795.91\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">output cols=[event_time_dt, et_dayofweek, et_dayofweek_sin...]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;13 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>2&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M795.91,-71.7C795.91,-63.98 795.91,-54.71 795.91,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"799.41,-46.1 795.91,-36.1 792.41,-46.1 799.41,-46.1\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>11</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"805.91\" cy=\"-450\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"805.91\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\">Rename</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;7 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>11&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M779.81,-434.83C761.72,-425.02 737.3,-411.77 716.79,-400.64\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"718.44,-397.55 707.98,-395.86 715.1,-403.7 718.44,-397.55\"/>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>11&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M852.84,-445.59C930.9,-437.1 1078.91,-407.7 1078.91,-307 1078.91,-307 1078.91,-307 1078.91,-233 1078.91,-124.19 909.72,-98.75 832.98,-92.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"833.01,-89.3 822.79,-92.09 832.52,-96.29 833.01,-89.3\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"475.91\" cy=\"-162\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"475.91\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">Rename</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;2 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>6&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M516.44,-152.13C579.58,-138.32 700.4,-111.89 760.36,-98.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"761.45,-102.12 770.47,-96.57 759.95,-95.28 761.45,-102.12\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>9</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"795.91\" cy=\"-162\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"795.91\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">Rename</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;2 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>9&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M795.91,-143.7C795.91,-135.98 795.91,-126.71 795.91,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"799.41,-118.1 795.91,-108.1 792.41,-118.1 799.41,-118.1\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>12</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1162.91\" cy=\"-162\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1162.91\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">Rename</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;2 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>12&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1124.13,-151.13C1114,-148.68 1103.07,-146.14 1092.91,-144 999.26,-124.29 887.99,-105.72 831.96,-96.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"832.26,-93.2 821.84,-95.07 831.16,-100.11 832.26,-93.2\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"805.91\" cy=\"-522\" rx=\"300.05\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"805.91\" y=\"-518.3\" font-family=\"Times,serif\" font-size=\"14.00\">nvt.ops.LambdaOp(lambda col: cudf.to_datetime(col, unit=&#39;s&#39;))</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;11 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>3&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M805.91,-503.7C805.91,-495.98 805.91,-486.71 805.91,-478.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"809.41,-478.1 805.91,-468.1 802.41,-478.1 809.41,-478.1\"/>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;6 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>4&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M318.99,-216.32C353.18,-204.7 398.4,-189.34 431.52,-178.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"433.04,-181.27 441.38,-174.73 430.79,-174.64 433.04,-181.27\"/>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;12 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>8&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1168.93,-215.7C1168.05,-207.98 1166.99,-198.71 1166.01,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1169.48,-189.64 1164.87,-180.1 1162.52,-190.44 1169.48,-189.64\"/>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;9 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M795.91,-215.7C795.91,-207.98 795.91,-198.71 795.91,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"799.41,-190.1 795.91,-180.1 792.41,-190.1 799.41,-190.1\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fce23bd0d00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_features.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Normalize Continuous Features¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing price long-tailed distribution\n",
    "price_log = ['price'] >> nvt.ops.LogOp() >> nvt.ops.Normalize() >> nvt.ops.Rename(name='price_log_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Price to the average price for the category_id\n",
    "def relative_price_to_avg_categ(col, gdf):\n",
    "    epsilon = 1e-5\n",
    "    col = ((gdf['price'] - col) / (col + epsilon)) * (col > 0).astype(int)\n",
    "    return col\n",
    "    \n",
    "avg_category_id_pr = ['category_id'] >> nvt.ops.JoinGroupby(cont_cols =['price'], stats=[\"mean\"]) >> nvt.ops.Rename(name='avg_category_id_price')\n",
    "relative_price_to_avg_category = avg_category_id_pr >> nvt.ops.LambdaOp(relative_price_to_avg_categ, dependency=['price']) >> nvt.ops.Rename(name=\"relative_price_to_avg_categ_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Grouping interactions into sessions¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate by session id (create sequence as type of array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_feats = ['event_time_ts', 'user_session'] + cat_feats + time_features + price_log + relative_price_to_avg_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Groupby Workflow\n",
    "groupby_features = groupby_feats >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"user_session\"], \n",
    "    sort_cols=[\"event_time_ts\"],\n",
    "    aggs={\n",
    "        'user_id': ['first'],\n",
    "        'product_id': [\"list\", \"count\"],\n",
    "        'category_code': [\"list\"],  \n",
    "        'event_type': [\"list\"], \n",
    "        'brand': [\"list\"], \n",
    "        'category_id': [\"list\"], \n",
    "        'event_time_ts': [\"first\"],\n",
    "        'event_time_dt': [\"first\"],\n",
    "        'et_dayofweek_sin': [\"list\"],\n",
    "        'et_dayofweek_cos': [\"list\"],\n",
    "        'price_log_norm': [\"list\"],\n",
    "        'relative_price_to_avg_categ_id': [\"list\"],\n",
    "        'product_recency_days_log_norm': [\"list\"]\n",
    "        },\n",
    "    name_sep=\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select columns which are not list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['event_time_dt-first',\n",
       " 'user_session',\n",
       " 'event_time_ts-first',\n",
       " 'product_id-count',\n",
       " 'user_id-first']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby_features_nonlist = [x for x in groupby_features.output_columns.names if '-list' not in x]\n",
    "groupby_features_nonlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSIONS_MAX_LENGTH = 20 \n",
    "MINIMUM_SESSION_LENGTH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trim the features in sequence in each session according to sessions_max_length param which is set as 20 in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvtabular/nvtabular/workflow/node.py:45: FutureWarning: The `[\"a\", \"b\", \"c\"] >> ops.Operator` syntax for creating a `ColumnGroup` has been deprecated in NVTabular 21.09 and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "groupby_features_trim = ((groupby_features - groupby_features_nonlist)) >> nvt.ops.ListSlice(0,SESSIONS_MAX_LENGTH) >> nvt.ops.Rename(postfix = '_seq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a `day_index` column in order to partition sessions by day when saving the parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate session day index based on 'timestamp-first' column\n",
    "day_index = ((groupby_features['event_time_dt-first'])  >> \n",
    "    nvt.ops.LambdaOp(lambda col: (col - col.min()).dt.days +1) >> \n",
    "    nvt.ops.Rename(f = lambda col: \"day_index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select certain columns to be used in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = groupby_features[groupby_features_nonlist] + groupby_features_trim + day_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter out the session that have sesion size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols = ['event_time_dt-first', 'event_time_ts-first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sessions = (selected_features - remove_cols) >> nvt.ops.Filter(f=lambda df: df[\"product_id-count\"] >= MINIMUM_SESSION_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize the NVTabular dataset object and workflow graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVTabular's preprocessing and feature engineering workflows are directed graphs of operators. When we initialize an NVTabular Workflow with our pipeline, workflow organizes the input and output columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.8/site-packages/numba-0.54.0rc2-py3.8-linux-x86_64.egg/numba/cuda/compiler.py:865: NumbaPerformanceWarning: \u001b[1mGrid size (1) < 2 * SM count (160) will likely result in GPU under utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/root/.local/lib/python3.8/site-packages/numba-0.54.0rc2-py3.8-linux-x86_64.egg/numba/cuda/compiler.py:865: NumbaPerformanceWarning: \u001b[1mGrid size (1) < 2 * SM count (160) will likely result in GPU under utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "dataset = nvt.Dataset(df)\n",
    "\n",
    "workflow = nvt.Workflow(filtered_sessions)\n",
    "workflow.fit(dataset)\n",
    "sessions_gdf = workflow.transform(dataset).to_ddf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we created an NVTabular Dataset object using our input dataset. Then, we calculate statistics for this workflow on the input dataset, i.e. on our training set, using the workflow.fit() method so that our Workflow can use these stats to transform any given input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the head of our preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_session</th>\n",
       "      <th>product_id-count</th>\n",
       "      <th>user_id-first</th>\n",
       "      <th>price_log_norm-list_seq</th>\n",
       "      <th>product_recency_days_log_norm-list_seq</th>\n",
       "      <th>category_code-list_seq</th>\n",
       "      <th>et_dayofweek_sin-list_seq</th>\n",
       "      <th>event_type-list_seq</th>\n",
       "      <th>product_id-list_seq</th>\n",
       "      <th>category_id-list_seq</th>\n",
       "      <th>relative_price_to_avg_categ_id-list_seq</th>\n",
       "      <th>et_dayofweek_cos-list_seq</th>\n",
       "      <th>brand-list_seq</th>\n",
       "      <th>day_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>779</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.60630524, -0.59222364, -0.5865736, -0.9532...</td>\n",
       "      <td>[-2.2660873, -2.2660873, -2.265767, -2.2660873...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 12, 12, 12, 12, 12, 12, 12, 12...</td>\n",
       "      <td>[0.9749277, 0.9749277, 0.9749277, 0.9749277, 0...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[19064, 52057, 13290, 11446, 15835, 879, 633, ...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 17, 17, 17, 17, 17, 17, 17, 17...</td>\n",
       "      <td>[0.03519272311423613, 0.05391071556495246, 0.0...</td>\n",
       "      <td>[-0.22252177, -0.22252177, -0.22252177, -0.222...</td>\n",
       "      <td>[171, 120, 231, 392, 562, 20, 9, 20, 295, 143,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>59</td>\n",
       "      <td>[0.7637999, 0.40693888, 0.25858778, 0.01305765...</td>\n",
       "      <td>[-0.85815144, -0.9379316, -1.0066851, -0.93632...</td>\n",
       "      <td>[1, 17, 17, 15, 1, 1, 17, 31, 17, 17, 17, 17, ...</td>\n",
       "      <td>[0.43388295, 0.43388295, 0.43388295, 0.4338829...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[252, 2801, 5399, 1074, 252, 355, 327, 319, 34...</td>\n",
       "      <td>[234, 36, 36, 30, 234, 52, 36, 48, 36, 36, 36,...</td>\n",
       "      <td>[0.0006990395471891284, -0.04875344340417949, ...</td>\n",
       "      <td>[-0.90096927, -0.90096927, -0.90096927, -0.900...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 50, 1, 1, 36, 1, 1, 36, 50, 1,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_session  product_id-count  user_id-first  \\\n",
       "0             2               779              4   \n",
       "1             3               316             59   \n",
       "\n",
       "                             price_log_norm-list_seq  \\\n",
       "0  [-0.60630524, -0.59222364, -0.5865736, -0.9532...   \n",
       "1  [0.7637999, 0.40693888, 0.25858778, 0.01305765...   \n",
       "\n",
       "              product_recency_days_log_norm-list_seq  \\\n",
       "0  [-2.2660873, -2.2660873, -2.265767, -2.2660873...   \n",
       "1  [-0.85815144, -0.9379316, -1.0066851, -0.93632...   \n",
       "\n",
       "                              category_code-list_seq  \\\n",
       "0  [1, 1, 1, 1, 1, 12, 12, 12, 12, 12, 12, 12, 12...   \n",
       "1  [1, 17, 17, 15, 1, 1, 17, 31, 17, 17, 17, 17, ...   \n",
       "\n",
       "                           et_dayofweek_sin-list_seq  \\\n",
       "0  [0.9749277, 0.9749277, 0.9749277, 0.9749277, 0...   \n",
       "1  [0.43388295, 0.43388295, 0.43388295, 0.4338829...   \n",
       "\n",
       "                                 event_type-list_seq  \\\n",
       "0  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "1  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                 product_id-list_seq  \\\n",
       "0  [19064, 52057, 13290, 11446, 15835, 879, 633, ...   \n",
       "1  [252, 2801, 5399, 1074, 252, 355, 327, 319, 34...   \n",
       "\n",
       "                                category_id-list_seq  \\\n",
       "0  [3, 3, 3, 3, 3, 17, 17, 17, 17, 17, 17, 17, 17...   \n",
       "1  [234, 36, 36, 30, 234, 52, 36, 48, 36, 36, 36,...   \n",
       "\n",
       "             relative_price_to_avg_categ_id-list_seq  \\\n",
       "0  [0.03519272311423613, 0.05391071556495246, 0.0...   \n",
       "1  [0.0006990395471891284, -0.04875344340417949, ...   \n",
       "\n",
       "                           et_dayofweek_cos-list_seq  \\\n",
       "0  [-0.22252177, -0.22252177, -0.22252177, -0.222...   \n",
       "1  [-0.90096927, -0.90096927, -0.90096927, -0.900...   \n",
       "\n",
       "                                      brand-list_seq  day_index  \n",
       "0  [171, 120, 231, 392, 562, 20, 9, 20, 295, 143,...          1  \n",
       "1  [1, 1, 1, 1, 1, 50, 1, 1, 36, 1, 1, 36, 50, 1,...          2  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions_gdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_session',\n",
       " 'product_id-count',\n",
       " 'user_id-first',\n",
       " 'price_log_norm-list_seq',\n",
       " 'product_recency_days_log_norm-list_seq',\n",
       " 'category_code-list_seq',\n",
       " 'et_dayofweek_sin-list_seq',\n",
       " 'event_type-list_seq',\n",
       " 'product_id-list_seq',\n",
       " 'category_id-list_seq',\n",
       " 'relative_price_to_avg_categ_id-list_seq',\n",
       " 'et_dayofweek_cos-list_seq',\n",
       " 'brand-list_seq',\n",
       " 'day_index']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.output_schema.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save workflow to load later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.save('workflow_ecommerce_etl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we can serialize our nvtabular model, and generate model config files to be able to serve our model to [Triton Inference Server](https://github.com/triton-inference-server/server)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular.inference.triton import generate_nvtabular_model\n",
    "generate_nvtabular_model(\n",
    "    workflow=workflow,\n",
    "    name='model_nvt',\n",
    "    output_path='/workspace/TF4Rec/models/model_nvt/',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Export dataset to parquet, partioned by the session day_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our Workflow transforms our dataset, we also save the results out to parquet files for fast reading at train time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTITION_COL = 'day_index'\n",
    "\n",
    "# Convert to a Dataset and write out hive-partitioned data to disk\n",
    "workflow.transform(dataset).to_parquet(OUTPUT_DATA_DIR, partition_on=[PARTITION_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _metadata     'day_index=2'  'day_index=4'  'day_index=6'\n",
      "'day_index=1'  'day_index=3'  'day_index=5'  'day_index=7'\n"
     ]
    }
   ],
   "source": [
    "!ls $OUTPUT_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Converting to the Transformers4Rec dir structure and splitting dataset¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER = os.path.join(INPUT_DATA_DIR, 'sessions_by_day')\n",
    "!mkdir -p $OUTPUT_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to create a folder structure as shown below. As we explained above, this is just to structure parquet files so that it would be easier to do incremental training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "/output/sessions_by_day/\n",
    "|-- 0001\n",
    "|   |-- train.parquet\n",
    "|   |-- valid.parquet\n",
    "|   |-- test.parquet\n",
    "\n",
    "|-- 0002\n",
    "|   |-- train.parquet\n",
    "|   |-- valid.parquet\n",
    "|   |-- test.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_folders = [f for f in sorted(os.listdir(OUTPUT_DATA_DIR)) if f.startswith(PARTITION_COL)]\n",
    "for day_folder in days_folders:\n",
    "    df = cudf.read_parquet(os.path.join(OUTPUT_DATA_DIR, day_folder))\n",
    "    out_folder = os.path.join(OUTPUT_FOLDER, day_folder.replace('day_index=', ''))\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    cp.random.seed(1)\n",
    "    random_values = cp.random.rand(len(df))\n",
    "    # Extracts 80% , 10%  and 10% for train, valid and test set, respectively. \n",
    "    train_set = df[random_values <= 0.80]\n",
    "    train_set.to_parquet(os.path.join(out_folder, 'train.parquet'))\n",
    "    \n",
    "    valid_set = df[(random_values > 0.80) &  (random_values < 0.90)]\n",
    "    valid_set.to_parquet(os.path.join(out_folder, 'valid.parquet'))\n",
    "    \n",
    "    test_set = df[random_values >= 0.90]\n",
    "    test_set.to_parquet(os.path.join(out_folder, 'test.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wrap Up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! we finished our first task. We reprocessed our dataset and created new features to train a session-based recommendation model. Please run the cell below to shut down the kernel before moving on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
