<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>transformers4rec.torch package &mdash; Transformers4Rec  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="canonical" href="https://nvidia-merlin.github.io/Transformers4Rec/main/api/transformers4rec.torch.html" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="transformers4rec.torch.block package" href="transformers4rec.torch.block.html" />
    <link rel="prev" title="transformers4rec.tf.utils package" href="transformers4rec.tf.utils.html" /> 
</head>

<body class="wy-body-for-nav">
  <div class="banner">
    <p class="banner">
      Beginning in January 2023, versions for all NVIDIA Merlin projects
      will change from semantic versioning like <code>4.0</code>
      to calendar versioning like <code>23.01</code>.</p>
  </div>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Transformers4Rec
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../README.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../why_transformers4rec.html">Why Transformers4Rec?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_definition.html">Model Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training_eval.html">Training and Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pipeline.html">End-to-end pipeline with NVIDIA Merlin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Example Notebooks</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">API Documentation</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="transformers4rec.html">transformers4rec package</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="transformers4rec.config.html">transformers4rec.config package</a></li>
<li class="toctree-l3"><a class="reference internal" href="transformers4rec.tf.html">transformers4rec.tf package</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">transformers4rec.torch package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformers4rec.torch.block.html">transformers4rec.torch.block package</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformers4rec.torch.features.html">transformers4rec.torch.features package</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformers4rec.torch.model.html">transformers4rec.torch.model package</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformers4rec.torch.tabular.html">transformers4rec.torch.tabular package</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformers4rec.torch.utils.html">transformers4rec.torch.utils package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="transformers4rec.utils.html">transformers4rec.utils package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="merlin_standard_lib.html">merlin_standard_lib package</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Additional Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Transformers4Rec</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="modules.html">API Documentation</a> &raquo;</li>
          <li><a href="transformers4rec.html">transformers4rec package</a> &raquo;</li>
      <li>transformers4rec.torch package</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="transformers4rec-torch-package">
<h1>transformers4rec.torch package<a class="headerlink" href="#transformers4rec-torch-package" title="Permalink to this headline"></a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline"></a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="transformers4rec.torch.block.html">transformers4rec.torch.block package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.block.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.block.html#module-transformers4rec.torch.block.base">transformers4rec.torch.block.base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.block.html#module-transformers4rec.torch.block.mlp">transformers4rec.torch.block.mlp module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.block.html#module-transformers4rec.torch.block.transformer">transformers4rec.torch.block.transformer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.block.html#module-transformers4rec.torch.block">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="transformers4rec.torch.features.html">transformers4rec.torch.features package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.features.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.features.html#module-transformers4rec.torch.features.base">transformers4rec.torch.features.base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.features.html#module-transformers4rec.torch.features.continuous">transformers4rec.torch.features.continuous module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.features.html#module-transformers4rec.torch.features.embedding">transformers4rec.torch.features.embedding module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.features.html#module-transformers4rec.torch.features.sequence">transformers4rec.torch.features.sequence module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.features.html#module-transformers4rec.torch.features.tabular">transformers4rec.torch.features.tabular module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.features.html#module-transformers4rec.torch.features.text">transformers4rec.torch.features.text module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.features.html#module-transformers4rec.torch.features">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="transformers4rec.torch.model.html">transformers4rec.torch.model package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.model.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.model.html#transformers4rec-torch-model-head-module">transformers4rec.torch.model.head module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.model.html#transformers4rec-torch-model-model-module">transformers4rec.torch.model.model module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.model.html#transformers4rec-torch-model-prediction-task-module">transformers4rec.torch.model.prediction_task module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.model.html#module-transformers4rec.torch.model">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="transformers4rec.torch.tabular.html">transformers4rec.torch.tabular package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.tabular.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.tabular.html#module-transformers4rec.torch.tabular.aggregation">transformers4rec.torch.tabular.aggregation module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.tabular.html#transformers4rec-torch-tabular-tabular-module">transformers4rec.torch.tabular.tabular module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.tabular.html#module-transformers4rec.torch.tabular.transformations">transformers4rec.torch.tabular.transformations module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.tabular.html#module-transformers4rec.torch.tabular">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="transformers4rec.torch.utils.html">transformers4rec.torch.utils package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.utils.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.utils.html#module-transformers4rec.torch.utils.data_utils">transformers4rec.torch.utils.data_utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.utils.html#module-transformers4rec.torch.utils.examples_utils">transformers4rec.torch.utils.examples_utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.utils.html#module-transformers4rec.torch.utils.schema_utils">transformers4rec.torch.utils.schema_utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.utils.html#module-transformers4rec.torch.utils.torch_utils">transformers4rec.torch.utils.torch_utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers4rec.torch.utils.html#module-transformers4rec.torch.utils">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="module-transformers4rec.torch.masking">
<span id="transformers4rec-torch-masking-module"></span><h2>transformers4rec.torch.masking module<a class="headerlink" href="#module-transformers4rec.torch.masking" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt id="transformers4rec.torch.masking.MaskingInfo">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">transformers4rec.torch.masking.</span></code><code class="sig-name descname"><span class="pre">MaskingInfo</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">schema</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#MaskingInfo"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.MaskingInfo" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.11)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<dl class="py attribute">
<dt id="transformers4rec.torch.masking.MaskingInfo.schema">
<code class="sig-name descname"><span class="pre">schema</span></code><em class="property"><span class="pre">:</span> <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></em><a class="headerlink" href="#transformers4rec.torch.masking.MaskingInfo.schema" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="transformers4rec.torch.masking.MaskingInfo.targets">
<code class="sig-name descname"><span class="pre">targets</span></code><em class="property"><span class="pre">:</span> <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></em><a class="headerlink" href="#transformers4rec.torch.masking.MaskingInfo.targets" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="transformers4rec.torch.masking.MaskSequence">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">transformers4rec.torch.masking.</span></code><code class="sig-name descname"><span class="pre">MaskSequence</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><span class="pre">int</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_on_last_item_seq_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><span class="pre">bool</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#MaskSequence"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.MaskSequence" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="transformers4rec.torch.utils.html#transformers4rec.torch.utils.torch_utils.OutputSizeMixin" title="transformers4rec.torch.utils.torch_utils.OutputSizeMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers4rec.torch.utils.torch_utils.OutputSizeMixin</span></code></a>, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v1.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></a></p>
<p>Base class to prepare masked items inputs/labels for language modeling tasks.</p>
<p>Transformer architectures can be trained in different ways. Depending of the training method,
there is a specific masking schema. The masking schema sets the items to be predicted (labels)
and mask (hide) their positions in the sequence so that they are not used by the Transformer
layers for prediction.</p>
<dl class="simple">
<dt>We currently provide 4 different masking schemes out of the box:</dt><dd><ul class="simple">
<li><p>Causal LM (clm)</p></li>
<li><p>Masked LM (mlm)</p></li>
<li><p>Permutation LM (plm)</p></li>
<li><p>Replacement Token Detection (rtd)</p></li>
</ul>
</dd>
</dl>
<p>This class can be extended to add different a masking scheme.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> – The hidden dimension of input tensors, needed to initialize trainable vector of
masked positions.</p></li>
<li><p><strong>pad_token</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a><em>, </em><em>default = 0</em>) – Index of the padding token used for getting batch of sequences with the same length</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers4rec.torch.masking.MaskSequence.compute_masked_targets">
<code class="sig-name descname"><span class="pre">compute_masked_targets</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item_ids</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#transformers4rec.torch.masking.MaskingInfo" title="transformers4rec.torch.masking.MaskingInfo"><span class="pre">transformers4rec.torch.masking.MaskingInfo</span></a><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#MaskSequence.compute_masked_targets"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.MaskSequence.compute_masked_targets" title="Permalink to this definition"></a></dt>
<dd><p>Method to prepare masked labels based on the sequence of item ids.
It returns The true labels of masked positions and the related boolean mask.
And the attributes of the class <cite>mask_schema</cite> and <cite>masked_targets</cite>
are updated to be re-used in other modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>item_ids</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><em>torch.Tensor</em></a>) – The sequence of input item ids used for deriving labels of
next item prediction task.</p></li>
<li><p><strong>training</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – Flag to indicate whether we are in <cite>Training</cite> mode or not.
During training, the labels can be any items within the sequence
based on the selected masking task.
During evaluation, we are predicting the last item in the sequence.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tuple[MaskingSchema, MaskedTargets]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers4rec.torch.masking.MaskSequence.apply_mask_to_inputs">
<code class="sig-name descname"><span class="pre">apply_mask_to_inputs</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">schema</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#MaskSequence.apply_mask_to_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.MaskSequence.apply_mask_to_inputs" title="Permalink to this definition"></a></dt>
<dd><p>Control the masked positions in the inputs by replacing the true interaction
by a learnable masked embedding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><em>torch.Tensor</em></a>) – The 3-D tensor of interaction embeddings resulting from the ops:
TabularFeatures + aggregation + projection(optional)</p></li>
<li><p><strong>schema</strong> (<em>MaskingSchema</em>) – The boolean mask indicating masked positions.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers4rec.torch.masking.MaskSequence.predict_all">
<code class="sig-name descname"><span class="pre">predict_all</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item_ids</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#transformers4rec.torch.masking.MaskingInfo" title="transformers4rec.torch.masking.MaskingInfo"><span class="pre">transformers4rec.torch.masking.MaskingInfo</span></a><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#MaskSequence.predict_all"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.MaskSequence.predict_all" title="Permalink to this definition"></a></dt>
<dd><p>Prepare labels for all next item predictions instead of
last-item predictions in a user’s sequence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>item_ids</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><em>torch.Tensor</em></a>) – The sequence of input item ids used for deriving labels of
next item prediction task.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tuple[MaskingSchema, MaskedTargets]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers4rec.torch.masking.MaskSequence.forward">
<code class="sig-name descname"><span class="pre">forward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_ids</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#MaskSequence.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.MaskSequence.forward" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="transformers4rec.torch.masking.MaskSequence.forward_output_size">
<code class="sig-name descname"><span class="pre">forward_output_size</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#MaskSequence.forward_output_size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.MaskSequence.forward_output_size" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="transformers4rec.torch.masking.MaskSequence.transformer_required_arguments">
<code class="sig-name descname"><span class="pre">transformer_required_arguments</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#MaskSequence.transformer_required_arguments"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.MaskSequence.transformer_required_arguments" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="transformers4rec.torch.masking.MaskSequence.transformer_optional_arguments">
<code class="sig-name descname"><span class="pre">transformer_optional_arguments</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#MaskSequence.transformer_optional_arguments"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.MaskSequence.transformer_optional_arguments" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="transformers4rec.torch.masking.MaskSequence.transformer_arguments">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">transformer_arguments</span></code><a class="headerlink" href="#transformers4rec.torch.masking.MaskSequence.transformer_arguments" title="Permalink to this definition"></a></dt>
<dd><p>Prepare additional arguments to pass to the Transformer forward methods.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="transformers4rec.torch.masking.CausalLanguageModeling">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">transformers4rec.torch.masking.</span></code><code class="sig-name descname"><span class="pre">CausalLanguageModeling</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><span class="pre">int</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_on_last_item_seq_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><span class="pre">bool</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_on_last_item_seq_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><span class="pre">bool</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#CausalLanguageModeling"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.CausalLanguageModeling" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#transformers4rec.torch.masking.MaskSequence" title="transformers4rec.torch.masking.MaskSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers4rec.torch.masking.MaskSequence</span></code></a></p>
<p>In Causal Language Modeling (clm) you predict the next item based on past positions of the
sequence. Future positions are masked.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – The hidden dimension of input tensors, needed to initialize trainable vector of masked
positions.</p></li>
<li><p><strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a><em>, </em><em>default = 0</em>) – Index of padding item used for getting batch of sequences with the same length</p></li>
<li><p><strong>eval_on_last_item_seq_only</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>, </em><em>default = True</em>) – Predict only last item during evaluation</p></li>
<li><p><strong>train_on_last_item_seq_only</strong> (<em>predict only last item during training</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers4rec.torch.masking.CausalLanguageModeling.apply_mask_to_inputs">
<code class="sig-name descname"><span class="pre">apply_mask_to_inputs</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_schema</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#CausalLanguageModeling.apply_mask_to_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.CausalLanguageModeling.apply_mask_to_inputs" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="transformers4rec.torch.masking.MaskedLanguageModeling">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">transformers4rec.torch.masking.</span></code><code class="sig-name descname"><span class="pre">MaskedLanguageModeling</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><span class="pre">int</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_on_last_item_seq_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><span class="pre">bool</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlm_probability</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.11)"><span class="pre">float</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.15</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#MaskedLanguageModeling"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.MaskedLanguageModeling" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#transformers4rec.torch.masking.MaskSequence" title="transformers4rec.torch.masking.MaskSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers4rec.torch.masking.MaskSequence</span></code></a></p>
<p>In Masked Language Modeling (mlm) you randomly select some positions of the sequence to be
predicted, which are masked.
During training, the Transformer layer is allowed to use positions on the right (future info).
During inference, all past items are visible for the Transformer layer, which tries to predict
the next item.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – The hidden dimension of input tensors, needed to initialize trainable vector of masked
positions.</p></li>
<li><p><strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a><em>, </em><em>default = 0</em>) – Index of padding item used for getting batch of sequences with the same length</p></li>
<li><p><strong>eval_on_last_item_seq_only</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>, </em><em>default = True</em>) – Predict only last item during evaluation</p></li>
<li><p><strong>mlm_probability</strong> (<em>Optional</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.11)"><em>float</em></a><em>]</em><em>, </em><em>default = 0.15</em>) – Probability of an item to be selected (masked) as a label of the given sequence.
p.s. We enforce that at least one item is masked for each sequence, so that the network can
learn something with it.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="transformers4rec.torch.masking.PermutationLanguageModeling">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">transformers4rec.torch.masking.</span></code><code class="sig-name descname"><span class="pre">PermutationLanguageModeling</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><span class="pre">int</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_on_last_item_seq_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><span class="pre">bool</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plm_probability</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.11)"><span class="pre">float</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.16666666666666666</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_span_length</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><span class="pre">int</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">permute_all</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><span class="pre">bool</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#PermutationLanguageModeling"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.PermutationLanguageModeling" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#transformers4rec.torch.masking.MaskSequence" title="transformers4rec.torch.masking.MaskSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers4rec.torch.masking.MaskSequence</span></code></a></p>
<p>In Permutation Language Modeling (plm) you use a permutation factorization at the level of the
self-attention layer to define the accessible bidirectional context.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – The hidden dimension of input tensors, needed to initialize trainable vector of masked
positions.</p></li>
<li><p><strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a><em>, </em><em>default = 0</em>) – Index of padding item used for getting batch of sequences with the same length</p></li>
<li><p><strong>eval_on_last_item_seq_only</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>, </em><em>default = True</em>) – Predict only last item during evaluation</p></li>
<li><p><strong>max_span_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – maximum length of a span of masked items</p></li>
<li><p><strong>plm_probability</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.11)"><em>float</em></a>) – The ratio of surrounding items to unmask to define the context of the span-based
prediction segment of items</p></li>
<li><p><strong>permute_all</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – Compute partial span-based prediction (=False) or not.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers4rec.torch.masking.PermutationLanguageModeling.compute_masked_targets">
<code class="sig-name descname"><span class="pre">compute_masked_targets</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item_ids</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#transformers4rec.torch.masking.MaskingInfo" title="transformers4rec.torch.masking.MaskingInfo"><span class="pre">transformers4rec.torch.masking.MaskingInfo</span></a><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#PermutationLanguageModeling.compute_masked_targets"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.PermutationLanguageModeling.compute_masked_targets" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="transformers4rec.torch.masking.PermutationLanguageModeling.transformer_required_arguments">
<code class="sig-name descname"><span class="pre">transformer_required_arguments</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#PermutationLanguageModeling.transformer_required_arguments"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.PermutationLanguageModeling.transformer_required_arguments" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="transformers4rec.torch.masking.ReplacementLanguageModeling">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">transformers4rec.torch.masking.</span></code><code class="sig-name descname"><span class="pre">ReplacementLanguageModeling</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><span class="pre">int</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_on_last_item_seq_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><span class="pre">bool</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_from_batch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><span class="pre">bool</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#ReplacementLanguageModeling"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.ReplacementLanguageModeling" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#transformers4rec.torch.masking.MaskedLanguageModeling" title="transformers4rec.torch.masking.MaskedLanguageModeling"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers4rec.torch.masking.MaskedLanguageModeling</span></code></a></p>
<p>Replacement Language Modeling (rtd) you use MLM to randomly select some items, but replace
them by random tokens.
Then, a discriminator model (that can share the weights with the generator or not), is asked
to classify whether the item at each position belongs or not to the original sequence.
The generator-discriminator architecture was jointly trained using Masked LM and RTD tasks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – The hidden dimension of input tensors, needed to initialize trainable vector of masked
positions.</p></li>
<li><p><strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a><em>, </em><em>default = 0</em>) – Index of padding item used for getting batch of sequences with the same length</p></li>
<li><p><strong>eval_on_last_item_seq_only</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>, </em><em>default = True</em>) – Predict only last item during evaluation</p></li>
<li><p><strong>sample_from_batch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – Whether to sample replacement item ids from the same batch or not</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers4rec.torch.masking.ReplacementLanguageModeling.get_fake_tokens">
<code class="sig-name descname"><span class="pre">get_fake_tokens</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">itemid_seq</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_flat</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#ReplacementLanguageModeling.get_fake_tokens"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.ReplacementLanguageModeling.get_fake_tokens" title="Permalink to this definition"></a></dt>
<dd><p>Second task of RTD is binary classification to train the discriminator.
The task consists of generating fake data by replacing [MASK] positions with random items,
ELECTRA discriminator learns to detect fake replacements.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>itemid_seq</strong> (<em>torch.Tensor of shape</em><em> (</em><em>bs</em><em>, </em><em>max_seq_len</em><em>)</em>) – input sequence of item ids</p></li>
<li><p><strong>target_flat</strong> (<em>torch.Tensor of shape</em><em> (</em><em>bs*max_seq_len</em><em>)</em>) – flattened masked label sequences</p></li>
<li><p><strong>logits</strong> (<em>torch.Tensor of shape</em><em> (</em><em>#pos_item</em><em>, </em><em>vocab_size</em><em> or </em><em>#pos_item</em><em>)</em><em>,</em>) – mlm probabilities of positive items computed by the generator model.
The logits are over the whole corpus if sample_from_batch = False,
over the positive items (masked) of the current batch otherwise</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>corrupted_inputs</strong> (<em>torch.Tensor of shape (bs, max_seq_len)</em>) – input sequence of item ids with fake replacement</p></li>
<li><p><strong>discriminator_labels</strong> (<em>torch.Tensor of shape (bs, max_seq_len)</em>) – binary labels to distinguish between original and replaced items</p></li>
<li><p><strong>batch_updates</strong> (<em>torch.Tensor of shape (#pos_item)</em>) – the indices of replacement item within the current batch if sample_from_batch is enabled</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers4rec.torch.masking.ReplacementLanguageModeling.sample_from_softmax">
<code class="sig-name descname"><span class="pre">sample_from_softmax</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a><a class="reference internal" href="../_modules/transformers4rec/torch/masking.html#ReplacementLanguageModeling.sample_from_softmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.masking.ReplacementLanguageModeling.sample_from_softmax" title="Permalink to this definition"></a></dt>
<dd><p>Sampling method for replacement token modeling (ELECTRA)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>logits</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><em>torch.Tensor</em></a><em>(</em><em>pos_item</em><em>, </em><em>vocab_size</em><em>)</em>) – scores of probability of masked positions returned  by the generator model</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>samples</strong> – ids of replacements items.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)">torch.Tensor</a>(#pos_item)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-transformers4rec.torch.ranking_metric">
<span id="transformers4rec-torch-ranking-metric-module"></span><h2>transformers4rec.torch.ranking_metric module<a class="headerlink" href="#module-transformers4rec.torch.ranking_metric" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt id="transformers4rec.torch.ranking_metric.RankingMetric">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">transformers4rec.torch.ranking_metric.</span></code><code class="sig-name descname"><span class="pre">RankingMetric</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">top_ks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_onehot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/ranking_metric.html#RankingMetric"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.ranking_metric.RankingMetric" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torchmetrics.metric.Metric</span></code></p>
<p>Metric wrapper for computing ranking metrics&#64;K for session-based task.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>top_ks</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.11)"><em>list</em></a><em>, </em><em>default</em><em> [</em><em>2</em><em>, </em><em>5</em><em>]</em><em>)</em>) – list of cutoffs</p></li>
<li><p><strong>labels_onehot</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – Enable transform the labels to one-hot representation</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers4rec.torch.ranking_metric.RankingMetric.update">
<code class="sig-name descname"><span class="pre">update</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">preds</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.13)"><span class="pre">torch.Tensor</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/ranking_metric.html#RankingMetric.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.ranking_metric.RankingMetric.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="transformers4rec.torch.ranking_metric.RankingMetric.compute">
<code class="sig-name descname"><span class="pre">compute</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/ranking_metric.html#RankingMetric.compute"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.ranking_metric.RankingMetric.compute" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="transformers4rec.torch.ranking_metric.PrecisionAt">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">transformers4rec.torch.ranking_metric.</span></code><code class="sig-name descname"><span class="pre">PrecisionAt</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">top_ks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_onehot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/ranking_metric.html#PrecisionAt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.ranking_metric.PrecisionAt" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#transformers4rec.torch.ranking_metric.RankingMetric" title="transformers4rec.torch.ranking_metric.RankingMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers4rec.torch.ranking_metric.RankingMetric</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt id="transformers4rec.torch.ranking_metric.RecallAt">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">transformers4rec.torch.ranking_metric.</span></code><code class="sig-name descname"><span class="pre">RecallAt</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">top_ks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_onehot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/ranking_metric.html#RecallAt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.ranking_metric.RecallAt" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#transformers4rec.torch.ranking_metric.RankingMetric" title="transformers4rec.torch.ranking_metric.RankingMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers4rec.torch.ranking_metric.RankingMetric</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt id="transformers4rec.torch.ranking_metric.AvgPrecisionAt">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">transformers4rec.torch.ranking_metric.</span></code><code class="sig-name descname"><span class="pre">AvgPrecisionAt</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">top_ks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_onehot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/ranking_metric.html#AvgPrecisionAt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.ranking_metric.AvgPrecisionAt" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#transformers4rec.torch.ranking_metric.RankingMetric" title="transformers4rec.torch.ranking_metric.RankingMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers4rec.torch.ranking_metric.RankingMetric</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt id="transformers4rec.torch.ranking_metric.DCGAt">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">transformers4rec.torch.ranking_metric.</span></code><code class="sig-name descname"><span class="pre">DCGAt</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">top_ks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_onehot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/ranking_metric.html#DCGAt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.ranking_metric.DCGAt" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#transformers4rec.torch.ranking_metric.RankingMetric" title="transformers4rec.torch.ranking_metric.RankingMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers4rec.torch.ranking_metric.RankingMetric</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt id="transformers4rec.torch.ranking_metric.NDCGAt">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">transformers4rec.torch.ranking_metric.</span></code><code class="sig-name descname"><span class="pre">NDCGAt</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">top_ks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_onehot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/ranking_metric.html#NDCGAt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.ranking_metric.NDCGAt" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#transformers4rec.torch.ranking_metric.RankingMetric" title="transformers4rec.torch.ranking_metric.RankingMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers4rec.torch.ranking_metric.RankingMetric</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt id="transformers4rec.torch.ranking_metric.MeanRecipricolRankAt">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">transformers4rec.torch.ranking_metric.</span></code><code class="sig-name descname"><span class="pre">MeanRecipricolRankAt</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">top_ks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_onehot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers4rec/torch/ranking_metric.html#MeanRecipricolRankAt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#transformers4rec.torch.ranking_metric.MeanRecipricolRankAt" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#transformers4rec.torch.ranking_metric.RankingMetric" title="transformers4rec.torch.ranking_metric.RankingMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers4rec.torch.ranking_metric.RankingMetric</span></code></a></p>
</dd></dl>

</div>
<div class="section" id="transformers4rec-torch-trainer-module">
<h2>transformers4rec.torch.trainer module<a class="headerlink" href="#transformers4rec-torch-trainer-module" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="module-transformers4rec.torch.typing">
<span id="transformers4rec-torch-typing-module"></span><h2>transformers4rec.torch.typing module<a class="headerlink" href="#module-transformers4rec.torch.typing" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="module-contents">
<h2>Module contents<a class="headerlink" href="#module-contents" title="Permalink to this headline"></a></h2>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="transformers4rec.tf.utils.html" class="btn btn-neutral float-left" title="transformers4rec.tf.utils package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="transformers4rec.torch.block.html" class="btn btn-neutral float-right" title="transformers4rec.torch.block package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v0.1.11
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../v0.1.10/api/transformers4rec.torch.html">v0.1.10</a></dd>
      <dd><a href="transformers4rec.torch.html">v0.1.11</a></dd>
      <dd><a href="../../v0.1.12/api/transformers4rec.torch.html">v0.1.12</a></dd>
      <dd><a href="../../v0.1.13/api/transformers4rec.torch.html">v0.1.13</a></dd>
      <dd><a href="../../v0.1.14/api/transformers4rec.torch.html">v0.1.14</a></dd>
      <dd><a href="../../v0.1.15/api/transformers4rec.torch.html">v0.1.15</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../../main/api/transformers4rec.torch.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NVJ1Y1YJHK"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NVJ1Y1YJHK', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>