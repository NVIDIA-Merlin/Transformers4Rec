{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e128361",
   "metadata": {},
   "source": [
    "In this notebook, we pick up a model trained using `train_and_save_models_for_benchmarking.ipynb` and stored on google drive to perform inference and benchmark performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1d7155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (0.13.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (1.3.5)\n",
      "Requirement already satisfied: nvidia-pyindex in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (1.0.9)\n",
      "Requirement already satisfied: dllogger from git+https://github.com/NVIDIA/dllogger#egg=dllogger in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 1)) (0.1.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 1)) (2.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 1)) (1.0.11)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 1)) (5.9.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0; python_version < \"3.9\" and sys_platform == \"linux\" in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 1)) (3.20.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 1)) (2.28.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 1)) (3.1.30)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb->-r requirements.txt (line 1)) (45.2.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 1)) (8.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 2)) (1.22.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 2)) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from promise<3,>=2.0->wandb->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (2019.11.28)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb->-r requirements.txt (line 1)) (4.0.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->-r requirements.txt (line 1)) (5.0.0)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown) (1.14.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.28.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.26.13)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1NCFZ5ya3zyxPsrmupEoc9UEm4sslAddV\n",
      "To: /workspace/examples/t4rec_paper_experiments/t4r_paper_repro/rees46_ecom_dataset_small_for_ci.zip\n",
      "100%|██████████| 43.4M/43.4M [00:06<00:00, 6.42MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Hit:3 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "unzip is already the newest version (6.0-25ubuntu1.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 74 not upgraded.\n",
      "Archive:  rees46_ecom_dataset_small_for_ci.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "replace /transformers4rec/examples/t4rec_paper_experiments/t4r_paper_repro/0001/valid.parquet? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [# gdown h]\n",
      "replace /transformers4rec/examples/t4rec_paper_experiments/t4r_paper_repro/0001/valid.parquet? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [ttps://dr]\n",
      "replace /transformers4rec/examples/t4rec_paper_experiments/t4r_paper_repro/0001/valid.parquet? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [ive.googl]\n",
      "replace /transformers4rec/examples/t4rec_paper_experiments/t4r_paper_repro/0001/valid.parquet? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [e.com/uc?]\n",
      "replace /transformers4rec/examples/t4rec_paper_experiments/t4r_paper_repro/0001/valid.parquet? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [id=18Ella]\n",
      "replace /transformers4rec/examples/t4rec_paper_experiments/t4r_paper_repro/0001/valid.parquet? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [Kaodqaesr]\n",
      "replace /transformers4rec/examples/t4rec_paper_experiments/t4r_paper_repro/0001/valid.parquet? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "#### Install requirements\n",
    "cd examples/t4rec_paper_experiments\n",
    "pip install -r requirements.txt\n",
    "\n",
    "### Get data\n",
    "cd t4r_paper_repro\n",
    "\n",
    "FEATURE_SCHEMA_PATH=../datasets_configs/ecom_rees46/rees46_schema.pbtxt\n",
    "pip install gdown\n",
    "gdown https://drive.google.com/uc?id=1NCFZ5ya3zyxPsrmupEoc9UEm4sslAddV\n",
    "apt-get update -y\n",
    "apt-get install unzip -y\n",
    "DATA_PATH=/transformers4rec/examples/t4rec_paper_experiments/t4r_paper_repro/\n",
    "unzip -d $DATA_PATH \"rees46_ecom_dataset_small_for_ci.zip\"\n",
    "# gdown https://drive.google.com/uc?id=18EllaKaodqaesrNJ3YGEmv0YUD3NX0vK\n",
    "# mkdir -p /transformers4rec/TF4Rec/models/\n",
    "# MODEL_PATH=/transformers4rec/TF4Rec/models/\n",
    "# unzip -d $MODEL_PATH \"model.zip\"\n",
    "exit 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513f52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import numpy as np\n",
    "import nvtabular.inference.triton as nvt_triton\n",
    "import tritonclient.grpc as grpcclient\n",
    "import subprocess\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abc674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /workspace/examples/t4rec_paper_experiments/t4r_paper_repro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "163eef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/workspace/examples/t4rec_paper_experiments/t4r_paper_repro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4071799",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = os.path.join(\n",
    "    '/transformers4rec/examples/t4rec_paper_experiments/t4r_paper_repro/',\n",
    "    str(2,).zfill(4),\n",
    "    \"valid.parquet\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2775430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x7f7b8b8666a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0220 02:43:34.847979 18298 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f03d6000000' with size 268435456\n",
      "I0220 02:43:34.848302 18298 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0220 02:43:34.850169 18298 model_lifecycle.cc:459] loading: t4r_pytorch_pt:1\n",
      "I0220 02:43:38.522804 18298 python_be.cc:1856] TRITONBACKEND_ModelInstanceInitialize: t4r_pytorch_pt (GPU device 0)\n"
     ]
    }
   ],
   "source": [
    "# load model trained locally using train_and_save_models_for_benchmarking.ipynb\n",
    "\n",
    "my_env = os.environ.copy()\n",
    "\n",
    "# # run on the CPU\n",
    "# my_env[\"CUDA_VISIBLE_DEVICES\"] = ''\n",
    "# my_env[\"HAS_GPU\"] = '0'\n",
    "\n",
    "# run on the GPU\n",
    "my_env[\"HAS_GPU\"] = '1'\n",
    "\n",
    "subprocess.Popen(['tritonserver', '--model-repository=/workspace/models_for_benchmarking/'], env=my_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39dfa60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load model downloaded from google drive\n",
    "# subprocess.Popen(['tritonserver',  '--model-repository=/transformers4rec/TF4Rec/models/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b8f3a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0220 02:43:42.878213 18298 model_lifecycle.cc:694] successfully loaded 't4r_pytorch_pt' version 1\n",
      "I0220 02:43:42.878340 18298 server.cc:563] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0220 02:43:42.878405 18298 server.cc:590] \n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Backend | Path                                                  | Config                                                                                                                                                        |\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| python  | /opt/tritonserver/backends/python/libtriton_python.so | {\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}} |\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "I0220 02:43:42.878442 18298 server.cc:633] \n",
      "+----------------+---------+--------+\n",
      "| Model          | Version | Status |\n",
      "+----------------+---------+--------+\n",
      "| t4r_pytorch_pt | 1       | READY  |\n",
      "+----------------+---------+--------+\n",
      "\n",
      "I0220 02:43:42.903695 18298 metrics.cc:864] Collecting metrics for GPU 0: Quadro RTX 8000\n",
      "I0220 02:43:42.903932 18298 metrics.cc:757] Collecting CPU metrics\n",
      "I0220 02:43:42.904063 18298 tritonserver.cc:2264] \n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Option                           | Value                                                                                                                                                                                                |\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| server_id                        | triton                                                                                                                                                                                               |\n",
      "| server_version                   | 2.28.0                                                                                                                                                                                               |\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace logging |\n",
      "| model_repository_path[0]         | /workspace/models_for_benchmarking/                                                                                                                                                                  |\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                                            |\n",
      "| strict_model_config              | 0                                                                                                                                                                                                    |\n",
      "| rate_limit                       | OFF                                                                                                                                                                                                  |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                            |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                             |\n",
      "| response_cache_byte_size         | 0                                                                                                                                                                                                    |\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                                  |\n",
      "| strict_readiness                 | 1                                                                                                                                                                                                    |\n",
      "| exit_timeout                     | 30                                                                                                                                                                                                   |\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "I0220 02:43:42.904966 18298 grpc_server.cc:4819] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0220 02:43:42.905121 18298 http_server.cc:3477] Started HTTPService at 0.0.0.0:8000\n",
      "I0220 02:43:42.945837 18298 http_server.cc:184] Started Metrics Service at 0.0.0.0:8002\n"
     ]
    }
   ],
   "source": [
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2413171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n",
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tritonhttpclient/__init__.py:31: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tritonhttpclient\n",
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))\n",
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1726cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = cudf.read_parquet(eval_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a21bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['sess_pid_seq']\n",
    "inputs = nvt_triton.convert_df_to_triton_input(col_names, prediction_data.loc[6, col_names], grpcclient.InferInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cd5d6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058879852294921875"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "MODEL_NAME_PT = \"t4r_pytorch_pt\"\n",
    "\n",
    "N_TRIALS = 1000\n",
    "\n",
    "# WarmUp\n",
    "for _ in range(N_TRIALS):\n",
    "    payload = cudf.DataFrame(data={'sess_pid_seq': np.random.randint(0, 390001, 20), 'id': 0}).groupby('id').agg({'sess_pid_seq': list})\n",
    "    with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "        col_names = ['sess_pid_seq']\n",
    "        inputs = nvt_triton.convert_df_to_triton_input(col_names, payload, grpcclient.InferInput)\n",
    "        response = client.infer(MODEL_NAME_PT, inputs)\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "# Collecting\n",
    "\n",
    "out = []\n",
    "for _ in range(N_TRIALS):\n",
    "    payload = cudf.DataFrame(data={'sess_pid_seq': np.random.randint(0, 390001, 20), 'id': 0}).groupby('id').agg({'sess_pid_seq': list})\n",
    "    start_time = time.time()\n",
    "    with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "        col_names = ['sess_pid_seq']\n",
    "        inputs = nvt_triton.convert_df_to_triton_input(col_names, payload, grpcclient.InferInput)\n",
    "        response = client.infer(MODEL_NAME_PT, inputs)\n",
    "    end_time = time.time()\n",
    "    out.append(end_time-start_time)\n",
    "    \n",
    "# P95\n",
    "np.sort(out)[int(0.95 * N_TRIALS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "876aaf7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008340835571289062"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "MODEL_NAME_PT = \"t4r_pytorch_pt\"\n",
    "\n",
    "N_TRIALS = 1000\n",
    "\n",
    "# WarmUp\n",
    "for _ in range(N_TRIALS):\n",
    "    payload = cudf.DataFrame(data={'sess_pid_seq': np.random.randint(0, 390001, 20), 'id': 0}).groupby('id').agg({'sess_pid_seq': list})\n",
    "    with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "        col_names = ['sess_pid_seq']\n",
    "        inputs = nvt_triton.convert_df_to_triton_input(col_names, payload, grpcclient.InferInput)\n",
    "        response = client.infer(MODEL_NAME_PT, inputs)\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "# Collecting\n",
    "\n",
    "out = []\n",
    "for _ in range(N_TRIALS):\n",
    "    payload = cudf.DataFrame(data={'sess_pid_seq': np.random.randint(0, 390001, 20), 'id': 0}).groupby('id').agg({'sess_pid_seq': list})\n",
    "    start_time = time.time()\n",
    "    with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "        col_names = ['sess_pid_seq']\n",
    "        inputs = nvt_triton.convert_df_to_triton_input(col_names, payload, grpcclient.InferInput)\n",
    "        response = client.infer(MODEL_NAME_PT, inputs)\n",
    "    end_time = time.time()\n",
    "    out.append(end_time-start_time)\n",
    "    \n",
    "# P95\n",
    "np.sort(out)[int(0.95 * N_TRIALS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b537ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212fd93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "# output_names = [\"output\"]\n",
    "\n",
    "# outputs = []\n",
    "# for col in output_names:\n",
    "#     outputs.append(grpcclient.InferRequestedOutput(col))\n",
    "    \n",
    "# MODEL_NAME_PT = \"t4r_pytorch_pt\"\n",
    "# payload = cudf.DataFrame(data={'sess_pid_seq': np.random.randint(0, 390001, 20), 'id': 0}).groupby('id').agg({'sess_pid_seq': list})\n",
    "\n",
    "# with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "#     col_names = ['sess_pid_seq']\n",
    "#     inputs = nvt_triton.convert_df_to_triton_input(col_names, payload, grpcclient.InferInput)\n",
    "#     response = client.infer(MODEL_NAME_PT, inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
